%% This document created by Scientific Word (R) Version 2.5
%% Starting shell: mathart1


\documentclass[12pt,thmsb,titlepage,final,oneside,letterpaper]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amssymb}
\usepackage{amsmath}
% \usepackage{sw20elba}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{caption}

\setcounter{MaxMatrixCols}{10}
% \TCIDATA{TCIstyle=article/art1.lat,elba,article}

% %TCIDATA{OutputFilter=LATEX.DLL}
% %TCIDATA{Version=5.00.0.2606}
% %TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
% %TCIDATA{BibliographyScheme=Manual}
% %TCIDATA{Created=Sun Oct 14 23:49:17 2007}
% %TCIDATA{LastRevised=Tuesday, July 24, 2012 15:26:47}
% %TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
% %TCIDATA{Language=American English}

\onehalfspacing
\addtolength{\oddsidemargin}{-.10in}
\addtolength{\evensidemargin}{-.10in}
\addtolength{\textwidth}{0.2in}
\renewcommand{\theequation}{\thesection.\arabic{equation}}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{acknowledgement}{Acknowledgement}
\newtheorem{algorithm}{Algorithm}
\newtheorem{axiom}{Assumption}
\newtheorem{case}{Case}
\newtheorem{claim}{Claim}
\newtheorem{conclusion}{Conclusion}
\newtheorem{condition}{Condition}
\newtheorem{conjecture}{Conjecture}
\newtheorem{corollary}{Corollary}[section]
\newtheorem{criterion}{Criterion}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem{exercise}{Exercise}
\newtheorem{lemma}{Lemma}[section]
\newtheorem{notation}{Notation}
\newtheorem{problem}{Problem}
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Comments}
\newtheorem{solution}{Solution}
\newtheorem{summary}{Summary}
\newenvironment{proof}[1][Proof]{\noindent\textbf{#1.} }{\ \rule{0.5em}{0.5em}}
\captionsetup{labelformat=empty, labelsep= none, justification= justified,width=.90\textwidth,aboveskip=3pt}
\captionsetup{justification=raggedright, singlelinecheck=false}
\captionsetup{font={small,sf}}
% \input{tcilatex}

\begin{document}

\author{$%
\begin{array}{c}
\text{Donald W. K.\ Andrews}^{\ast } \\ 
\text{Cowles Foundation} \\ 
\text{Yale University} \\ 
\vspace{0.2in} \\ 
\text{Xu Cheng} \\ 
\text{Department of Economics} \\ 
\text{University of Pennsylvania}%
\end{array}%
$}
\title{\textbf{GMM Estimation and}\\
\textbf{Uniform Subvector Inference}\\
\textbf{with Possible Identification Failure}}
\date{First Draft: August, 2007\\
Revised: \today\bigskip \\
$^{\ast }$Andrews gratefully acknowledges the research support of the
National Science Foundation via grant numbers SES-0751517 and SES-1058376.
The authors thank a co-editor, two referees, Tim Armstrong, Xiaohong Chen,
Sukjin Han, Yuichi Kitamura, Peter Phillips, Eric Renault, Frank
Schorfheide, and Ed Vytlacil for helpful comments.}
\maketitle

% %TCIMACRO{\TeXButton{empty page}{\thispagestyle{empty}}}%
% %BeginExpansion
% \thispagestyle{empty}%
% %EndExpansion
% $\vspace*{0.5cm}$

\begin{center}
\textbf{Abstract}$\vspace{0.25cm}$
\end{center}

This paper determines the properties of standard generalized method of
moments (GMM) estimators, tests, and confidence sets (CS's) in moment
condition models in which some parameters are unidentified or weakly
identified in part of the parameter space. The asymptotic distributions of
GMM estimators are established under a full range of drifting sequences of
true parameters and distributions. The asymptotic sizes (in a uniform sense)
of standard GMM tests and CS's are established.

The paper also establishes the correct asymptotic sizes of \textquotedblleft
robust\textquotedblright\ GMM-based Wald, $t,$ and quasi-likelihood ratio
tests and CS's whose critical values are designed to yield robustness to
identification problems.

The results of the paper are applied to a nonlinear regression model with
endogeneity and a probit model with endogeneity and possibly weak
instrumental variables.$\vspace{2in}$

\noindent \emph{Keywords}\textbf{:} Asymptotic size, confidence set,
generalized method of moments, GMM estimator, identification, nonlinear
models, test, Wald test, weak identification.\newline
\newline

\noindent \emph{JEL Classification Numbers}: C12, C15.\newpage

%TCIMACRO{\TeXButton{pageno}{\setcounter{page}{1}}}%
%BeginExpansion
\setcounter{page}{1}%
%EndExpansion

\section{ \hspace{-0.34in}\textbf{.}\hspace{0.2in}Introduction\label{Intro
Sec}}

\setcounter{equation}{0}\hspace{0.25in}This paper gives a set of GMM
regularity conditions that are akin to the classic conditions in Hansen
(1982) and Pakes and Pollard (1989). But, they allow for singularity of the
GMM estimator's variance matrix due to the lack of identification of some
parameters in part of the parameter space.\footnote{%
Throughout the paper, we use the term identification/lack of identification
in the sense of identification by a GMM or minimum distance criterion
function $Q_{n}(\theta ).$ Lack of identification by $Q_{n}(\theta )$ means
that $Q_{n}(\theta )$ is flat in some directions in part of the parameter
space. See Assumption GMM1(i) below for a precise definition. Lack of
identification by the criterion function $Q_{n}(\theta )$ is not the same as
lack of identification in the usual or strict sense of the term, although
there is often a close relationship.} This paper is a sequel to Andrews and
Cheng (2012a) (AC1). The latter paper provides results for general extremum
estimators, $t$ tests, and QLR tests in the presence of possible weak
identification under high-level assumptions. Here we provide more primitive
conditions for GMM-based statistics by verifying the high-level assumptions
of AC1. This paper provides results for Wald tests and CS's that apply not
only to GMM estimators, but also to other extremum estimators covered by
AC1. This paper also provides some results for minimum distance (MD)
estimators, tests, and CS's. Lastly, the paper analyzes two specific models
that are not considered in AC1.

Under the conditions given, the asymptotic distributions of GMM estimators
and Wald and quasi-likelihood ratio (QLR) test statistics are established.
The asymptotic sizes of standard GMM tests and confidence sets (CS's) are
established. In many cases, their asymptotic sizes are not correct. We show
that Wald and QLR statistics combined with \textquotedblleft identification
robust\textquotedblright\ critical values have correct asymptotic sizes (in
a uniform sense).

In contrast to standard GMM results in the literature, the results given
here cover a full range of drifting sequences of true parameters and
distributions. Such results are needed to establish the (uniform) asymptotic
size properties of tests and CS's and to give good approximations to the
finite-sample properties of estimators, tests, and CS's under weak
identification. Non-smooth sample moment conditions are allowed, as in Pakes
and Pollard (1989) and Andrews (2002).

We consider moment condition models where the parameter $\theta $ is of the
form $\theta =(\beta ,\zeta ,\pi ),$ where $\pi $ is identified if and only
if $\beta \neq 0,$ $\zeta $ is not related to the identification of $\pi ,$
and $\psi =(\beta ,\zeta )$ is always identified. The parameters $\beta ,$ $%
\zeta ,$ and $\pi $ may be scalars or vectors. For example, this framework
applies to the nonlinear regression model $Y_{i}=\beta \cdot h\left(
X_{1,i},\pi \right) +X_{2,i}^{\prime }\zeta +U_{i}$ with endogenous
variables $X_{1,i}$ or $X_{2,i}$ and instruments (IV's) $Z_{i}.$ Here lack
of identification of $\pi $ when $\beta =0$ occurs because of nonlinearity.
This framework also applies to the probit model with endogeneity: $%
y_{i}^{\ast }=Y_{i}\pi +X_{i}^{\prime }\zeta _{1}^{\ast }+U_{i}^{\ast },$
where one observes $y_{i}=1(y_{i}^{\ast }>0),$ the endogenous variable $%
Y_{i},$ and the exogenous regressor vector $X_{i}$ and the reduced form for $%
Y_{i}$ is $Y_{i}=Z_{i}^{\prime }\beta +X_{i}^{\prime }\zeta _{2}+V_{i}.$ In
this case, lack of identification of $\pi $ occurs when $\beta =0$ because
the IV's are irrelevant.

We determine the asymptotic properties of GMM estimators and tests under
drifting sequences of true parameters $\theta _{n}=(\beta _{n},\zeta
_{n},\pi _{n})$ for $n\geq 1,$ where $n$ indexes the sample size. The
behavior of GMM estimators and tests depends on the magnitude of $||\beta
_{n}||.$ The asymptotic behavior of these statistics varies across three
categories of sequences $\{\beta _{n}:n\geq 1\}:$ Category I(a) $\beta
_{n}=0 $ $\forall n\geq 1,$ $\pi $ is unidentified; Category I(b) $\beta
_{n}\neq 0$ and $n^{1/2}\beta _{n}\rightarrow b\in R^{d_{\beta }},$ $\pi $
is weakly identified; Category II $\beta _{n}\rightarrow 0$ and $%
n^{1/2}||\beta _{n}||\rightarrow \infty ,$ $\pi $ is semi-strongly
identified; and Category III $\beta _{n}\rightarrow \beta _{0}\neq 0,$ $\pi $
is strongly identified.

For Category I sequences, GMM estimators, tests, and CS's are shown to have
non-standard asymptotic properties. For Category II and III\ sequences, they
are shown to have standard asymptotic properties such as normal and
chi-squared distributions. However, for Category II sequences, the rates of
convergence of estimators of $\pi $ are slower than $n^{1/2}$ and tests
concerning $\pi $ do not have power against $n^{-1/2}$-local alternatives.

Numerical results for the nonlinear regression model with endogeneity show
that the GMM estimators of both $\beta $ and $\pi $ have highly non-normal
asymptotic and finite-sample ($n=500$) distributions when $\pi $ is
unidentified or weakly identified. The asymptotics provide excellent
approximations to the finite-sample distributions. Nominal $95\%$ standard $%
t $ confidence intervals (CI's) for $\beta $ are found to have asymptotic
size equal to $68\%$ and finite-sample size of $72\%.$ In contrast, nominal $%
95\%$ standard QLR CI's for $\beta $ have asymptotic and finite-sample size
of $93\%.$ There are no asymptotic size distortions for the standard $t$ and
QLR CI's for $\pi $ and the finite-sample sizes are close to the asymptotic
values. However, the CI's for $\pi $ are far from being similar
asymptotically or in finite samples. The robust CI's for $\beta $ have
correct asymptotic size. Their finite-sample sizes are $91.5\%$ for $t$ CI's
and $95\%$ for QLR CI's for nominal $95\%$ CI's.

To conclude, the numerical results show that (i) weak identification can
have substantial effects on the properties of estimators and standard tests
and CS's; (ii) the asymptotic results of the paper provide useful
approximations to the finite-sample distributions of estimators and test
statistics under weak identification and identification failure; and (iii)
the robust tests and CS's improve the size properties of tests and CS's in
finite-samples noticeably compared to standard tests and CS's.

Like the results in Hansen (1982), Pakes and Pollard (1989), and Andrews
(2002), the present paper applies when the GMM criterion function has a
stochastic quadratic approximation as a function of $\theta .$ This rules
out a number of models of interest in which identification failure may
appear, including regime switching models, mixture models, abrupt transition
structural change models, and abrupt transition threshold autoregressive
models.\footnote{%
For references concerning results for these models, see AC1.}

Now, we discuss the literature related to this paper. The following papers
are companions to this one: AC1, Andrews and Cheng (2012b) (AC1-SM), and
Andrews and Cheng (2011a) (AC2). These papers provide related, complementary
results to the present paper. AC1 provides results under high-level
conditions and analyzes the ARMA(1, 1) model in detail. AC1-SM provides
proofs for AC1 and related results. AC2 provides primitive conditions and
results for estimators and tests based on log likelihood criterion
functions. It provides applications to a smooth transition threshold
autoregressive (STAR) model and a nonlinear binary choice model.

Cheng (2008) establishes results for a nonlinear regression model with
multiple sources of weak identification, whereas the present paper only
considers a single source. However, the present paper applies to a much
broader range of models.

Tests of $H_{0}:\beta =0$ versus $H_{1}:\beta \neq 0$ are tests in which a
nuisance parameter $\pi $ only appears under the alternative. Such tests
have been considered in the literature since Davies (1977). The results of
this paper cover tests of this sort, as well as tests for a whole range of
linear and nonlinear hypotheses that involve $(\beta ,\zeta ,\pi )$ and
corresponding CS's.

The weak instrument (IV) literature is closely related to this paper. This
is true especially of Stock and Wright (2000), Kleibergen (2005), and
Guggenberger, Kleibergen, Mavroeidis, and Chen (2013). In comparison to
Stock and Wright (2000), the present paper differs because it focuses on
criterion functions that are indexed by a parameter $\beta $ that determines
the strength of identification. It also differs in that it considers
subvector analysis. In contrast to Kleibergen (2005) and Guggenberger,
Kleibergen, Mavroeidis, and Chen (2013), the present paper does not focus on
Lagrange multiplier statistics. Rather, it investigates the behavior of
standard estimators and tests, as well as robust tests based on Wald and QLR
statistics. Other related papers from the weak IV literature include Nelson
and Startz (1990), Dufour (1997), Staiger and Stock (1997), Kleibergen
(2002), and Moreira (2003).

Antoine and Renault (2009, 2010) and Caner (2010) consider GMM estimation
with IV's that lie in the semi-strong category, using our terminology.
Nelson and Startz (2007) and Ma and Nelson (2008) analyze models like those
considered in this paper. They do not provide asymptotic results or robust
tests and CS's of the sort given in this paper. Andrews and Mikusheva (2011)
and Qu (2011) consider Lagrange multiplier tests in a maximum likelihood
context where identification may fail, with emphasis on dynamic stochastic
general equilibrium models. Andrews and Mikusheva (2011) consider subvector
inference based on Anderson-Rubin-type minimum distance statistics.

In likelihood scenarios, Lee and Chesher (1986) consider Lagrange multiplier
tests and Rotnitzky, Cox, Bottai, and Robbins (2000) consider maximum
likelihood estimators and likelihood ratio tests, when the model is
identified at all parameter values, but the information matrix is singular
at some parameter values, such as those in the null hypothesis. This is a
different situation than considered here for two reasons. First, the present
paper considers situations where identification fails at some parameter
values in the parameter space (and this causes the GMM variance matrix to be
singular at these parameter values). Second, this paper considers GMM-based
statistics rather than likelihood-based statistics.

Sargan (1983), Phillips (1989), and Choi and Phillips (1992) establish
finite-sample and asymptotic results for linear simultaneous equations
models when some parameters are not identified. Shi and Phillips (2011)
provide results for a nonlinear regression model with nonstationary
regressors in which identification may fail.

The remainder of the paper is organized as follows. Section \ref{Estimator &
Crit Fn Sec} defines the GMM estimators, criterion functions, tests, and
confidence sets considered in the paper and specifies the drifting sequences
of distributions that are considered. It also introduces the two examples
that are considered in the paper. Section \ref{Assumptions Sec} states the
assumptions employed. Section \ref{Estimation Results Sec} provides the
asymptotic results for the GMM estimators. Section \ref{Wald Tests Sec}
establishes the asymptotic distributions of Wald statistics under the null
and under alternatives, determines the asymptotic size of standard Wald
CS's, and introduces robust Wald tests and CS's, whose asymptotic size is
equal to their nominal size. Section \ref{QLR Tests Sec} considers QLR CS's
based on the GMM criterion function. Section \ref{Numerical Results Sec}
provides numerical results for the nonlinear regression model with
endogeneity.

Andrews and Cheng (2011b) provides five supplemental appendices to this
paper. Supplemental Appendix A verifies the assumptions of the paper for the
probit model with endogeneity. Supplemental Appendix B provides proofs of
the GMM estimation results given in Section \ref{Estimation Results Sec}. It
also provides some results for minimum distance estimators. Supplemental
Appendix C provides proofs of the Wald test and CS results given in Section %
\ref{Wald Tests Sec}. Supplemental Appendix D provides some results used in
the verification of the assumptions for the two examples. Supplemental
Appendix E provides some additional numerical results for the nonlinear
regression model with endogeneity.

All limits below are taken \textquotedblleft as $n\rightarrow \infty .$%
\textquotedblright\ We let $\lambda _{\min }(A)$ and $\lambda _{\max }(A)$
denote the smallest and largest eigenvalues, respectively, of a matrix $A.$
All vectors are column vectors. For notational simplicity, we often write $%
(a,b)$ instead of $(a^{\prime },b^{\prime })^{\prime }$ for vectors $a$ and $%
b.$ Also, for a function $f(c)$ with $c=(a,b)$ $(=(a^{\prime },b^{\prime
})^{\prime }),$ we often write $f(a,b)$ instead of $f(c).$ Let $0_{d}$
denote a $d$-vector of zeros. Because it arises frequently, we let $0$
denote a $d_{\beta }$-vector of zeros, where $d_{\beta }$ is the dimension
of a parameter $\beta .$

We let $X_{n}(\pi )=o_{p\pi }(1)$ mean that $\sup_{\pi \in \Pi }\allowbreak
\left\vert \left\vert X_{n}(\pi )\right\vert \right\vert =o_{p}(1),$ where $%
\left\vert \left\vert \cdot \right\vert \right\vert $ denotes the Euclidean
norm. We let $\Rightarrow $ denote weak convergence of a sequence of
stochastic processes indexed by $\pi \in \Pi $ for some space $\Pi .$ We
employ the uniform metric $d$ on the space $\mathcal{E}_{v}$ of $R^{v}$%
-valued functions on $\Pi .$ See AC1-SM for more details regarding this.

\section{ \hspace{-0.34in}\textbf{.}\hspace{0.2in}Estimator, Criterion
Function, and Examples\label{Estimator & Crit Fn Sec}}

\subsection{\hspace{-0.23in}\textbf{.}\hspace{0.18in}GMM Estimators \label%
{Extremum Estrs Subsec}}

\setcounter{equation}{0}\hspace{0.25in}The GMM sample criterion function is%
\begin{equation}
Q_{n}(\theta )=\overline{g}_{n}(\theta )^{\prime }\mathcal{W}_{n}(\theta )%
\overline{g}_{n}(\theta )/2,  \label{GMM CF}
\end{equation}%
where $\overline{g}_{n}(\theta ):\Theta \rightarrow R^{k}$ is a vector of
sample moment conditions and $\mathcal{W}_{n}(\theta ):\Theta \rightarrow
R^{k\times k}$ is a symmetric random weight matrix.

The paper considers inference when $\theta $ is not identified (by the
criterion function $Q_{n}(\theta )$) at some points in the parameter space.
Lack of identification occurs when $Q_{n}(\theta )$ is flat with respect to
(wrt) some sub-vector of $\theta .$ To model this identification problem, $%
\theta $ is partitioned into three sub-vectors:%
\begin{equation}
\theta =(\beta ,\zeta ,\pi )=(\psi ,\pi ),\text{ where }\psi =(\beta ,\zeta
).
\end{equation}%
The parameter $\pi \in R^{d_{\pi }}$ is unidentified when $\beta =0$ $(\in
R^{d_{\beta }}).$ The parameter $\psi =(\beta ,\zeta )\in R^{d_{\psi }}$ is
always identified. The parameter $\zeta \in R^{d_{\zeta }}$ does not effect
the identification of $\pi .$ These conditions allow for a broad range of
cases, including cases where reparametrization is used to transform a model
into the framework considered here.

The true distribution of the observations $\{W_{i}:i\geq 1\}$ is denoted $%
F_{\gamma }$ for some parameter $\gamma \in \Gamma .$ We let $P_{\gamma }$
and $E_{\gamma }$ denote probability and expectation under $F_{\gamma }.$
The parameter space $\Gamma $ for the true parameter,\ referred to as the
\textquotedblleft true parameter space,\textquotedblright\ is compact and is
of the form:%
\begin{equation}
\Gamma =\{\gamma =(\theta ,\phi ):\theta \in \Theta ^{\ast },\phi \in \Phi
^{\ast }(\theta )\},  \label{True Par Space Gamma}
\end{equation}%
where $\Theta ^{\ast }$ is a compact subset of $R^{d_{\theta }}$ and $\Phi
^{\ast }(\theta )\subset \Phi ^{\ast }$ $\forall \theta \in \Theta ^{\ast }$
for some compact metric space $\Phi ^{\ast }$ with a metric that induces
weak convergence of the bivariate distributions $(W_{i},W_{i+m})$ for all $%
i,m\geq 1.$\footnote{%
That is, the metric satisfies: if $\gamma \rightarrow \gamma _{0},$ then $%
(W_{i},W_{i+m})$ under $\gamma $ converges in distribution to $%
(W_{i},W_{i+m})$ under $\gamma _{0}$ for all $i,m\geq 1.$ For example, in an
i.i.d. situation, the metric on $\Phi ^{\ast }$ can be the uniform metric on
the distribution of $W_{i}.$ In a stationary time series context, it can be
the supremum over $m\geq 1$ of the uniform metric on the space of
distributions of the vectors $(W_{i},W_{i+m}).$ Note that $\Gamma $ is a
metric space with metric $d_{\Gamma }(\gamma _{1},\gamma _{2})=||\theta
_{1}-\theta _{2}||+d_{\Phi ^{\ast }}(\phi _{1},\phi _{2}),$ where $\gamma
_{j}=(\theta _{j},\phi _{j})\in \Gamma $ for $j=1,2$ and $d_{\Phi ^{\ast }}$
is the metric on $\Phi ^{\ast }.$}\textbf{\ }In the case of a moment
condition model, the parameter $\phi $ indexes the part of the distribution
of the observations that is not determined by the moment conditions, which
typically is infinite dimensional.

By definition, the GMM estimator $\widehat{\theta }_{n}$ (approximately)
minimizes $Q_{n}(\theta )$ over an \textquotedblleft optimization parameter
space\textquotedblright\ $\Theta $:\footnote{%
The $o(n^{-1})$ term in (\ref{Defn of Thetahat}), and in (\ref{Defn psi})
and (\ref{Defn pihat}) below, is a fixed sequence of constants that does not
depend on the true parameter $\gamma \in \Gamma $ and does not depend on $%
\pi $ in (\ref{Defn psi}).}%
\begin{equation}
\widehat{\theta }_{n}\in \Theta \text{ and }Q_{n}(\widehat{\theta }%
_{n})=\inf_{\theta \in \Theta }Q_{n}(\theta )+o(n^{-1}).
\label{Defn of Thetahat}
\end{equation}%
We assume that the interior of $\Theta $ includes the true parameter space $%
\Theta ^{\ast }$ (see Assumption B1 below). This ensures that the asymptotic
distribution of $\widehat{\theta }_{n}$ is not affected by boundary
restrictions for any sequence of true parameters in $\Theta ^{\ast }.$ The
focus of this paper is not on the effects of boundary restrictions.

Without loss of generality, the optimization parameter space $\Theta $ can
be written as%
\begin{eqnarray}
\Theta \hspace{-0.08in} &=&\hspace{-0.08in}\{\theta =(\psi ,\pi ):\psi \in
\Psi (\pi ),\pi \in \Pi \},\text{ where}  \notag \\
\Pi \hspace{-0.08in} &=&\hspace{-0.08in}\{\pi :(\psi ,\pi )\in \Theta \text{
for some }\psi \}\text{ and}  \notag \\
\Psi (\pi )\hspace{-0.08in} &=&\hspace{-0.08in}\{\psi :(\psi ,\pi )\in
\Theta \}\text{ for }\pi \in \Pi .  \label{Defn of Par Sp Pi and Psi}
\end{eqnarray}%
We allow $\Psi (\pi )$ to depend on $\pi $ and, hence, $\Theta $ need not be
a product space between $\psi $ and $\pi .$

The main focus of this paper is on GMM estimators, but the results also
apply to minimum distance (MD) estimators. However, the assumptions employed
with MD estimators are not as primitive. The MD sample criterion function is
defined exactly as the GMM criterion function is defined in (\ref{GMM CF})
except that $\overline{g}_{n}(\theta )$ is not a vector of moment
conditions, but rather, is the difference between an unrestricted estimator $%
\widehat{\xi }_{n}$ of a parameter $\xi _{0}$ and a vector of restrictions $%
h(\theta )$ on $\xi _{0}.$ That is, 
\begin{equation}
\overline{g}_{n}(\theta )=\widehat{\xi }_{n}-h(\theta ),\text{ where }\xi
_{0}=h(\theta _{0}).  \label{MD defn}
\end{equation}%
See Schorfheide (2011) for a discussion of MD estimation of dynamic
stochastic general equilibrium models and weak identification problems in
these models.

\subsection{\hspace{-0.2in}\textbf{.}\hspace{0.18in}Example 1: Nonlinear
Regression with Endogeneity}

\hspace{0.25in}The first example is a nonlinear regression model with
endogenous regressors estimated using instrumental variables (IV's). The
IV's are assumed to be strong. Potential identification failure in this
model arises due to the nonlinearity in the regression function. Let $%
h(x,\pi )\in R$ be a function of $x$ that is known up to the
finite-dimensional parameter $\pi \in R^{d_{\pi }}.$ The model is%
\begin{equation}
Y_{i}=\beta \cdot h\left( X_{1,i},\pi \right) +X_{2,i}^{\prime }\zeta +U_{i}%
\text{ and }EU_{i}Z_{i}=0
\end{equation}%
for $i=1,...,n,$ where $X_{i}=(X_{1,i},X_{2,i})\in R^{d_{X}},$ $X_{2,i}\in
R^{d_{X_{2}}},$ $Z_{i}\in R^{k},$ and $k\geq d_{X_{2}}+d_{\pi }+1.$ The
regressors $X_{i}$ may be endogenous or exogenous. The function $h(x,\pi )$
is assumed to be twice continuously differentiable wrt $\pi .$ Let $h_{\pi
}(x,\pi )$ and $h_{\pi \pi }(x,\pi )$ denote the first- and second-order
partial derivatives of $h(x,\pi )$ wrt $\pi .$ For example, Areosa, McAleer,
and Medeiros (2011) consider GMM estimation of smooth transition models with
endogeneity (which are nonlinear regression models). In their case $h(x,\pi
) $ involves the logistic function. They provide an empirical application of
this model to inflation rate targeting in Brazil.

The GMM sample criterion function is%
\begin{eqnarray}
Q_{n}(\theta )\hspace{-0.08in} &=&\hspace{-0.08in}\overline{g}_{n}(\theta
)^{\prime }\mathcal{W}_{n}\overline{g}_{n}(\theta )/2,\text{ where }%
\overline{g}_{n}(\theta )=n^{-1}\sum_{i=1}^{n}U_{i}(\theta )Z_{i},\text{ } 
\notag \\
U_{i}\left( \theta \right) \hspace{-0.08in} &=&\hspace{-0.08in}Y_{i}-\beta
h(X_{1,i},\pi )-X_{2,i}^{\prime }\zeta ,\text{ and }\mathcal{W}_{n}=\left(
n^{-1}\sum_{i=1}^{n}Z_{i}Z_{i}^{\prime }\right) ^{-1}.  \label{gmm CF}
\end{eqnarray}

For simplicity, we use the optimal weight matrix under homoskedasticity.
Alternatively, one can employ the optimal weight matrix under
heteroskedasticity using a preliminary estimator $\overline{\theta }_{n}.$
Provided $\mathcal{W}_{n}(\theta )$ and $\overline{\theta }_{n}$ satisfy the
Assumptions in Lemma \ref{Lemma Two step weight matrix}\textbf{\ }below, all
results hold for this two-step estimator as well. For example, the
preliminary estimator $\overline{\theta }_{n}$ can be the estimator obtained
under homoskedasticity, which is shown below to satisfy the Assumptions in
Lemma \ref{Lemma Two step weight matrix}.

When $\beta =0,$ $U_{i}(\theta )$ does not depend on $\pi .$ In consequence, 
$Q_{n}(\theta )$ does not depend on $\pi $ when $\beta =0.$

Suppose the random variables $\left\{ \left( X_{i},Z_{i},U_{i}\right)
:i=1,...,n\right\} $ are i.i.d. with distribution $\phi \in \Phi ^{\ast },$
where $\Phi ^{\ast }$ is a compact metric space with a metric $d_{\Phi }$
that induces weak convergence of $(X_{i},Z_{i},U_{i}).$ In this example, the
parameter of interest is $\theta =(\beta ,\zeta ,\pi )$ and the nuisance
parameter is $\phi ,$ which is infinite dimensional.

The true parameter space for $\theta $ is%
\begin{equation}
\Theta ^{\ast }=\mathcal{B}^{\ast }\times \mathcal{Z}^{\ast }\times \Pi
^{\ast },\text{ where }\mathcal{B}^{\ast }=[-b_{1}^{\ast },b_{2}^{\ast
}]\subset R,  \label{gmm theta space}
\end{equation}%
$b_{1}^{\ast }\geq 0,$ $b_{2}^{\ast }\geq 0,$ $b_{1}^{\ast }$ and $%
b_{2}^{\ast }$ are not both equal to $0,$ $\mathcal{Z}^{\ast }\subset
R^{d_{\zeta }}$ is compact, and $\Pi ^{\ast }\subset R^{d_{\pi }}$ is
compact.

Suppose $||h_{\pi \pi }(x,\pi _{1})-h_{\pi \pi }(x,\pi _{2})||\leq M_{\pi
\pi }(x)\delta $ $\forall \pi _{1},\pi _{2}\in \Pi $ with $||\pi _{1}-\pi
_{2}||\leq \delta $ for some non-stochastic function $M_{\pi \pi }(x):%
\mathcal{X}\rightarrow R^{+}$ that satisfies the conditions in (\ref{gmm phi
space}) below, where $\delta $ is some positive constant and $\mathcal{X}$
denotes the union of the supports of $X_{1,i}$ over all $\phi \in \Phi
^{\ast }.$ Define%
\begin{eqnarray}
d_{i}(\pi )\hspace{-0.08in} &=&\hspace{-0.08in}(h\left( X_{1,i},\pi \right)
,X_{2,i},h_{\pi }\left( X_{1,i},\pi \right) )\in R^{d_{X_{2}}+d_{\pi }+1}%
\text{ and}  \notag \\
d_{\psi ,i}^{\ast }(\pi _{1},\pi _{2})\hspace{-0.08in} &=&\hspace{-0.08in}%
(h\left( X_{1,i},\pi _{1}\right) ,h\left( X_{1,i},\pi _{2}\right)
,X_{2,i})\in R^{d_{X_{2}}+2}.
\end{eqnarray}%
Let $E_{\phi }$ denote expectation under $\phi .$ For any $\theta ^{\ast
}\in \Theta ^{\ast },$ the true parameter space for $\phi $ is 
\begin{eqnarray}
&&\Phi ^{\ast }(\theta ^{\ast })\overset{}{=}\{\phi \overset{}{\in }\Phi
^{\ast }:E_{\phi }U_{i}Z_{i}\overset{}{=}0,\text{ }E_{\phi
}(U_{i}^{2}|X_{i},Z_{i})=\sigma ^{2}(X_{i},Z_{i})>0\text{ a.s.},\text{ }%
E_{\phi }|U_{i}|^{4+\varepsilon }  \notag \\
&&\overset{}{\leq }C,\text{ }E_{\phi }\sup_{\pi \in \Pi }\left( ||h\left(
X_{1,i},\pi \right) ||^{2+\varepsilon }+||h_{\pi }\left( X_{1,i},\pi \right)
||^{2+\varepsilon }+||h_{\pi \pi }\left( X_{1,i},\pi \right)
||^{1+\varepsilon }\right) \overset{}{\leq }C,  \notag \\
&&E_{\phi }(\left\Vert X_{2,i}\right\Vert ^{2+\varepsilon
}+||Z_{i}||^{2+\varepsilon }+M_{\pi \pi }(X_{1,i}))\overset{}{\leq }C,\text{ 
}\lambda _{\min }(E_{\phi }Z_{i}Z_{i}^{\prime })\geq \varepsilon ,  \notag \\
&&E_{\phi }Z_{i}d_{\psi ,i}^{\ast }(\pi _{1},\pi _{2})^{\prime }\in
R^{k\times (d_{X_{2}}+2)}\text{ has full column rank }\forall \pi _{1},\pi
_{2}\overset{}{\in }\Pi \text{ with }\pi _{1}\overset{}{\neq }\pi _{2}, 
\notag \\
&&E_{\phi }Z_{i}d_{i}(\pi )\in R^{k\times (d_{X_{2}}+d_{\pi }+1)}\text{ has
full column rank }\forall \pi \overset{}{\in }\Pi \},  \label{gmm phi space}
\end{eqnarray}%
for some constants $C<\infty $ and $\varepsilon >0.$ Note that in this
example $\Phi ^{\ast }(\theta ^{\ast })$ does not depend on $\theta ^{\ast
}. $

\subsection{\hspace{-0.2in}\textbf{.}\hspace{0.18in}Example 2: Probit Model
with Endogeneity and\newline
Possibly Weak Instruments\label{Simul Probit Ex Sec}}

\hspace{0.25in}The second example is a probit model with endogeneity and
IV's that may be weak or irrelevant, which causes identification issues.
Consider the following two-equation model with endogeneity of $Y_{i}$ in the
first equation:%
\begin{eqnarray}
y_{i}^{\ast }\hspace{-0.08in} &=&\hspace{-0.08in}Y_{i}\pi +X_{i}^{\prime
}\zeta _{1}^{\ast }+U_{i}^{\ast }\text{ and}  \notag \\
Y_{i}\hspace{-0.08in} &=&\hspace{-0.08in}Z_{i}^{\prime }\beta +X_{i}^{\prime
}\zeta _{2}+V_{i},  \label{Probit-Model}
\end{eqnarray}%
where $y_{i}^{\ast },Y_{i},U_{i}^{\ast },V_{i}\in R,$ $X_{i}\in R^{d_{X}},$ $%
Z_{i}\in R^{d_{Z}},$ and $\{(X_{i},Z_{i},U_{i},V_{i}):i=1,...,n\}$ are
i.i.d. The outcome variable $y_{i}^{\ast }$ of the first equation is not
observed. Only the binary indicator $y_{i}=1(y_{i}^{\ast }>0)$ is observed,
along with $Y_{i},$ $X_{i},$ and $Z_{i}.$ That is, we observe $%
\{W_{i}=(y_{i},Y_{i},X_{i},Z_{i}):$ $i=1,...,n\}.$ Similar models with
binary, truncated, or censored endogenous variables are considered in
Amemiya (1974), Heckman (1978), Nelson and Olson (1978), Lee (1981), Smith
and Blundell (1986), Rivers and Vuong (1988), among others.

The reduced-form equations of the model are%
\begin{eqnarray}
y_{i}^{\ast }\hspace{-0.08in} &=&\hspace{-0.08in}Z_{i}^{\prime }\beta \pi
+X_{i}^{\prime }\zeta _{1}+U_{i}\text{ and}  \notag \\
Y_{i}\hspace{-0.08in} &=&\hspace{-0.08in}Z_{i}^{\prime }\beta +X_{i}^{\prime
}\zeta _{2}+V_{i},\text{ where}  \notag \\
\zeta _{1}\hspace{-0.08in} &=&\hspace{-0.08in}\zeta _{1}^{\ast }+\pi \zeta
_{2}\text{ and }U_{i}=U_{i}^{\ast }+\pi V_{i}.  \label{Probit-red eqns}
\end{eqnarray}%
The variables $(X_{i},Z_{i})$ are independent of the errors $(U_{i},V_{i})$
and the errors $(U_{i},V_{i})$ have a joint normal distribution with mean
zero and covariance matrix $\Sigma _{uv},$ where%
\begin{equation}
\Sigma _{uv}=\left( 
\begin{array}{cc}
1 & \rho \sigma _{v} \\ 
\rho \sigma _{v} & \sigma _{v}^{2}%
\end{array}%
\right) .
\end{equation}%
The parameter of interest is $\theta =(\beta ,\zeta ,\pi ),$ where $\zeta
=(\zeta _{1},\zeta _{2}).$

In this model, weak identification of $\pi $ occurs when $\beta $ is close
to $0.$ We analyze a GMM estimator of $\theta ,$ and corresponding tests
concerning functions of $\theta ,$ in the presence of weak identification or
lack of identification.

Let $L(\cdot )$ denote the distribution function of the standard normal
distribution. Let $L^{\prime }(x)$ and $L^{\prime \prime }(x)$ denote the
first- and second-order derivatives of $L(x)$ wrt $x.$ We use the
abbreviations 
\begin{equation}
L_{i}(\theta )=L(Z_{i}^{\prime }\beta \pi +X_{i}^{\prime }\zeta _{1}),\text{ 
}L_{i}^{\prime }(\theta )=L^{\prime }(Z_{i}^{\prime }\beta \pi
+X_{i}^{\prime }\zeta _{1}),\text{ and }L_{i}^{\prime \prime }(\theta
)=L^{\prime \prime }(Z_{i}^{\prime }\beta \pi +X_{i}^{\prime }\zeta _{1}).
\end{equation}

Now we specify the moment conditions for the GMM estimator. The
log-likelihood function based on the first reduced-form equation in (\ref%
{Probit-red eqns}) and $y_{i}=1(y_{i}^{\ast }>0)$ is 
\begin{equation}
\ell (\theta )=\sum_{i=1}^{n}\left[ y_{i}\log \left( L_{i}(\theta )\right)
+(1-y_{i})\log \left( 1-L_{i}(\theta )\right) \right] .
\end{equation}%
Let $a=\beta \pi $ and $a_{0}=\beta _{0}\pi _{0}.$ The log-likelihood
function $\ell (\theta )$ depends on $\theta $ only through $a$ and $\zeta
_{1}.$ The expectation of the score function wrt $(a,\zeta _{1})$ yields the
first set of moment conditions%
\begin{eqnarray}
&&E_{\gamma _{0}}w_{1,i}(\theta _{0})(y_{i}-L_{i}(\theta _{0}))\overline{Z}%
_{i}\overset{}{=}0,\text{ where }  \notag \\
&&w_{1,i}(\theta )\overset{}{=}\frac{L_{i}^{\prime }(\theta )}{L_{i}(\theta
)(1-L_{i}(\theta ))}\text{ and }\overline{Z}_{i}\overset{}{=}%
(X_{i},Z_{i})\in R^{d_{X}\times d_{Z}}.  \label{Probit-Momt1}
\end{eqnarray}%
The second reduced-form equation in (\ref{Probit-red eqns}) implies 
\begin{equation}
E_{\gamma _{0}}V_{i}(\theta _{0})\overline{Z}_{i}=0,\text{ where }%
V_{i}(\theta )=Y_{i}-Z_{i}^{\prime }\beta -X_{i}^{\prime }\zeta _{2}.
\label{Probit-Momt2}
\end{equation}

We consider a two-step GMM estimator of $\theta $ based on the moment
conditions in (\ref{Probit-Momt1}) and (\ref{Probit-Momt2}). The resulting
estimator has not appeared in the literature previously, but it is close to
estimators in the papers referenced above, e.g., see Rivers and Vuong
(1988). The GMM sample criterion function is%
\begin{eqnarray}
Q_{n}(\theta )\hspace{-0.08in} &=&\hspace{-0.08in}\overline{g}_{n}(\theta
)^{\prime }\mathcal{W}_{n}\overline{g}_{n}(\theta )/2,\text{ where}
\label{Probit-SmplCrit} \\
\overline{g}_{n}(\theta )\hspace{-0.08in} &=&\hspace{-0.08in}%
n^{-1}\sum_{i=1}^{n}e_{i}(\theta )\otimes \overline{Z}_{i}\in
R^{2(d_{X}+d_{Z})}\text{ and }e_{i}(\theta )=\binom{w_{1,i}(\theta
)(y_{i}-L_{i}(\theta ))}{Y_{i}-Z_{i}^{\prime }\beta -X_{i}^{\prime }\zeta
_{2}}.  \notag
\end{eqnarray}%
In the first step, the weight matrix $\mathcal{W}_{n}$ is the identity
matrix, yielding an estimator $\overline{\theta }_{n}.$ In the second step, $%
\mathcal{W}_{n}$ is the optimal weight matrix that takes the form%
\begin{equation}
\mathcal{W}_{n}=\mathcal{W}_{n}(\overline{\theta }_{n}),\text{ where }%
\mathcal{W}_{n}(\theta )=n^{-1}\sum_{i=1}^{n}\left( e_{i}(\theta
)e_{i}(\theta )^{\prime }\right) \otimes (\overline{Z}_{i}\overline{Z}%
_{i}^{\prime }).  \label{Probit-Weight}
\end{equation}

The optimization and true parameter spaces $\Theta $ and $\Theta ^{\ast }$
are $\Theta =\mathbf{\times }_{j=1}^{k}[-b_{L,j},b_{H,j}]\times \mathcal{Z}%
\times \Pi $ and $\Theta ^{\ast }=\mathbf{\times }_{j=1}^{k}[-b_{L,j}^{\ast
},b_{H,j}^{\ast }]\times \mathcal{Z}^{\ast }\times \Pi ^{\ast },$ where $%
b_{L,j},b_{H,j},b_{L,j}^{\ast },b_{H,j}^{\ast }\in R,$ $0\leq b_{L,j}^{\ast
}<b_{L,j},$ $0\leq b_{H,j}^{\ast }<b_{H,j},$ $b_{L,j}^{\ast },b_{H,j}^{\ast
} $ are not both $0,$ for $j=1,...,k,$ $\mathcal{Z}^{\ast }\subset int(%
\mathcal{Z})\subset R^{2d_{X}},$ $\Pi ^{\ast }\subset int(\Pi )\subset R,$ $%
\mathcal{Z}^{\ast },\mathcal{Z},\Pi ^{\ast },$ and $\Pi $ are compact.%
\footnote{%
Note that $\mathcal{Z}$ and $\mathcal{Z}^{\ast }$ are not related to the
support of $Z_{i}.$ Rather, they are the optimization and true parameter
spaces for $\zeta ,$ which has dimension $2d_{X}.$}

Define $\overline{w}_{1,i}=\sup_{\theta \in \Theta }|w_{1,i}(\theta )|$ and $%
\overline{w}_{2,i}=\sup_{\theta \in \Theta }|w_{2,i}(\theta )|,$ where $%
w_{2,i}(\theta )=L_{i}^{\prime \prime }(\theta )/$\linebreak $(L_{i}(\theta
)(1-L_{i}(\theta ))).$

The nuisance parameter $\phi $ is defined by $\phi =(\rho ,\sigma _{v},F)\in
\Phi ^{\ast },$ where $F$ is the distribution of $(X_{i},Z_{i})$ and $\Phi
^{\ast }$ is a compact metric space with a metric $d_{\Phi }$ that induces
weak convergence of $(X_{i},Z_{i}).$ We use $P_{\phi }$ and $E_{\phi }$ to
denote probability and expectation under $\phi ,$ respectively, for random
quantities that depend only on $(X_{i},Z_{i}).$ For any $\theta ^{\ast }\in
\Theta ^{\ast },$ the true parameter space for $\phi $ is%
\begin{eqnarray}
\Phi (\theta ^{\ast })\hspace{-0.08in} &=&\hspace{-0.08in}\{\phi =(\rho
,\sigma _{v},F)\in \Phi :|\rho |<1,\sigma _{v}\geq \varepsilon ,\text{ }%
P_{\phi }(\overline{Z}_{i}^{\prime }c=0)<1\text{ for any }c\neq 0,  \notag \\
&&E_{\phi }(||\overline{Z}_{i}||^{4+\varepsilon }+\overline{w}%
_{1,i}^{4+\varepsilon }+\overline{w}_{2,i}^{2+\varepsilon })\overset{}{\leq }%
C\},  \label{Simul Probit Phi Space}
\end{eqnarray}%
for some $C<\infty $ and $\varepsilon >0.$ Note that in this example, $\Phi
(\theta ^{\ast })$ does not depend on $\theta ^{\ast }.$

The verification of the assumptions of this paper for this example is given
in Supplemental Appendix A.

\subsection{\hspace{-0.23in}\textbf{.}\hspace{0.18in}Confidence Sets and
Tests}

\hspace{0.25in}We return now to the general framework. We are interested in
the effect of lack of identification or weak identification on the GMM
estimator $\widehat{\theta }_{n}.$ Also, we are interested in its effects on
CS's for various functions $r(\theta )$ of $\theta $ and on tests of null
hypotheses of the form $H_{0}:r(\theta )=v.$

A CS is obtained by inverting a test. A nominal $1-\alpha $ CS for $r(\theta
)$ is 
\begin{equation}
CS_{n}=\{v:\mathcal{T}_{n}(v)\leq c_{n,1-\alpha }(v)\},  \label{invert CI}
\end{equation}%
where $\mathcal{T}_{n}\left( v\right) $ is a test statistic, such as a $t,$
Wald, or QLR statistic, and $c_{n,1-\alpha }\left( v\right) $ is a critical
value for testing $H_{0}:r(\theta )=v.$ The critical values considered in
this paper may depend on the null value $v$ of $r(\theta )$ as well as on
the data. The coverage probability of a CS for $r(\theta )$ is%
\begin{equation}
P_{\gamma }(r(\theta )\in CS_{n})=P_{\gamma }(\mathcal{T}_{n}(r(\theta
))\leq c_{n,1-\alpha }(r(\theta ))),  \label{coverage prob}
\end{equation}%
where $P_{\gamma }\left( \cdot \right) $ denotes probability when $\gamma $
is the true value.

We are interested in the finite-sample size of the CS, which is the smallest
finite-sample coverage probability of the CS over the parameter space. It is
approximated by the asymptotic size, which is defined to be%
\begin{equation}
AsySz=\underset{n\rightarrow \infty }{\lim \inf \text{ }}\underset{\gamma
\in \Gamma }{\inf }\text{ }P_{\gamma }(r(\theta )\in CS_{n}).
\label{AsySz Confid Set defn}
\end{equation}

For a test, we are interested in its null rejection probabilities and in
particular its maximum null rejection probability, which is the size of the
test. A test's asymptotic size is an approximation to the latter. The null
rejection probabilities and asymptotic size of a test are given by%
\begin{eqnarray}
&&P_{\gamma }(\mathcal{T}_{n}(v)>c_{n,1-\alpha }(v))\text{ for }\gamma 
\overset{}{=}(\theta ,\phi )\overset{}{\in }\Gamma \text{ with }r(\theta )%
\underset{}{=}v\text{ and}  \notag \\
&&AsySz\overset{}{=}\underset{n\rightarrow \infty }{\lim \sup }\text{ }%
\underset{\gamma \in \Gamma :r(\theta )=v}{\sup }\text{ }P_{\gamma }(%
\mathcal{T}_{n}(v)>c_{n,1-\alpha }(v)).  \label{AsySz Test defn}
\end{eqnarray}

\subsection{\hspace{-0.23in}\textbf{.}\hspace{0.18in}Drifting Sequences of
Distributions\label{Drifting Seqs of Distns}}

\hspace{0.25in}To determine the asymptotic size of a CS or test, we need to
derive the asymptotic distribution of the test statistic $\mathcal{T}%
_{n}(v_{n})$ under sequences of true parameters $\gamma _{n}=(\theta
_{n},\phi _{n})$ and $v_{n}=r(\theta _{n})$ that may depend on $n.$ The
reason is that the value of $\gamma $ at which the finite-sample size of a
CS or test is attained may vary with the sample size. Similarly, to
investigate the finite-sample behavior of the GMM estimator under weak
identification, we need to consider its asymptotic behavior under drifting
sequences of true distributions---as in Stock and Wright (2000).

Results in Andrews and Guggenberger (2009, 2010) and Andrews, Cheng, and
Guggenberger (2009) show that the asymptotic size of CS's and tests are
determined by certain drifting sequences of distributions. In this paper,
the following sequences $\{\gamma _{n}\}$ are key: 
\begin{eqnarray}
\Gamma \left( \gamma _{0}\right) \hspace{-0.08in} &=&\hspace{-0.08in}\left\{
\{\gamma _{n}\in \Gamma :n\geq 1\}:\gamma _{n}\rightarrow \gamma _{0}\in
\Gamma \right\} ,  \label{Seq's of Gamma Par's} \\
\Gamma \left( \gamma _{0},0,b\right) \hspace{-0.08in} &=&\hspace{-0.08in}%
\left\{ \{\gamma _{n}\}\in \Gamma \left( \gamma _{0}\right) :\beta _{0}=0%
\text{ and }n^{1/2}\beta _{n}\rightarrow b\in (R\cup \left\{ \pm \infty
\right\} )^{d_{\beta }}\right\} ,\text{ and}  \notag \\
\Gamma \left( \gamma _{0},\infty ,\omega _{0}\right) \hspace{-0.08in} &=&%
\hspace{-0.08in}\left\{ \{\gamma _{n}\}\in \Gamma (\gamma
_{0}):n^{1/2}||\beta _{n}||\rightarrow \infty \text{ and }\beta _{n}/||\beta
_{n}||\rightarrow \omega _{0}\in R^{d_{\beta }}\right\} ,  \notag
\end{eqnarray}%
where $\gamma _{0}=(\beta _{0},\zeta _{0},\pi _{0},\phi _{0})$ and $\gamma
_{n}=(\beta _{n},\zeta _{n},\pi _{n},\phi _{n}).$

The sequences in $\Gamma \left( \gamma _{0},0,b\right) $ are in Categories I
and II and are sequences for which $\{\beta _{n}\}$ is \emph{close} to $0$: $%
\beta _{n}\rightarrow 0.$ When $||b||<\infty ,$ $\{\beta _{n}\}$ is within $%
O(n^{-1/2})$ of $0$ and the sequence is in Category I. The sequences in $%
\Gamma \left( \gamma _{0},\infty ,\omega _{0}\right) $ are in Categories II
and III and are more \emph{distant }from $\beta =0$: $n^{1/2}||\beta
_{n}||\rightarrow \infty .$ The sets $\Gamma (\gamma _{0},0,b)$ and $\Gamma
(\gamma _{0},\infty ,\omega _{0})$ are \emph{not} disjoint. Both contain
sequences in Category II.

Throughout the paper we use the terminology: \textquotedblleft under $%
\{\gamma _{n}\}\in \Gamma (\gamma _{0})$\textquotedblright\ means
\textquotedblleft when the true parameters are $\{\gamma _{n}\}\in \Gamma
(\gamma _{0})$ for any $\gamma _{0}\in \Gamma ;$\textquotedblright\
\textquotedblleft under $\{\gamma _{n}\}\in \Gamma (\gamma _{0},0,b)$%
\textquotedblright\ means \textquotedblleft when the true parameters are $%
\{\gamma _{n}\}\in \Gamma (\gamma _{0},0,b)$ for any $\gamma _{0}\in \Gamma $
with $\beta _{0}=0$ and any $b\in (R\cup \left\{ \pm \infty \right\}
)^{d_{\beta }};$\textquotedblright\ and \textquotedblleft under $\{\gamma
_{n}\}\in \Gamma (\gamma _{0},\infty ,\omega _{0})$\textquotedblright\ means
\textquotedblleft when the true parameters are $\{\gamma _{n}\}\in \Gamma
(\gamma _{0},\infty ,\omega _{0})$ for any $\gamma _{0}\in \Gamma $ and any $%
\omega _{0}\in R^{d_{\beta }}$ with $||\omega _{0}||=1.$\textquotedblright

\section{ \hspace{-0.34in}\textbf{.}\hspace{0.2in}Assumptions \label%
{Assumptions Sec}}

\setcounter{equation}{0}\hspace{0.25in}This section provides relatively
primitive sufficient conditions for GMM estimators.

\subsection{\hspace{-0.23in}\textbf{.}\hspace{0.18in}Assumption GMM1\label%
{Section GMM}}

\hspace{0.25in}The first assumption specifies the basic identification
problem. It also provides conditions that are used to determine the
probability limit of the GMM estimator, when it exists, under all categories
of drifting sequences of distributions.\medskip

\noindent \textbf{Assumption GMM1. }(i) If $\beta =0,$ $\overline{g}%
_{n}(\theta )$ and $\mathcal{W}_{n}(\theta )$ do not depend on $\pi ,$ $%
\forall \theta \in \Theta ,$ $\forall n\geq 1,$ for any true parameter $%
\gamma ^{\ast }\in \Gamma .$

\noindent (ii) Under $\{\gamma _{n}\}\in \Gamma (\gamma _{0}),$ $%
\sup_{\theta \in \Theta }||\overline{g}_{n}(\theta )-g_{0}(\theta ;\gamma
_{0})||\rightarrow _{p}0$ and $\sup_{\theta \in \Theta }||\mathcal{W}%
_{n}(\theta )-\mathcal{W}(\theta ;\gamma _{0})||\allowbreak \rightarrow
_{p}0 $ for some non-random functions $g_{0}(\theta ;\gamma _{0}):\Theta
\times \Gamma \rightarrow R^{k}$ and $\mathcal{W}(\theta ;\gamma
_{0}):\Theta \times \Gamma \rightarrow R^{k\times k}.$

\noindent (iii) When $\beta _{0}=0,$ $g_{0}(\psi ,\pi ;\gamma _{0})=0$ if
and only if $\psi =\psi _{0},$ $\forall \pi \in \Pi ,$ $\forall \gamma
_{0}\in \Gamma .$

\noindent (iv) When $\beta _{0}\neq 0,$ $g_{0}(\theta ;\gamma _{0})=0$ if
and only if $\theta =\theta _{0},$ $\forall \gamma _{0}\in \Gamma .$

\noindent (v) $g_{0}(\theta ;\gamma _{0})$ is continuously differentiable in 
$\theta $ on $\Theta ,$ with its partial derivatives wrt $\theta $ and $\psi 
$ denoted by $g_{\theta }(\theta ;\gamma _{0})\in R^{k\times d_{\theta }}$
and $g_{\psi }(\theta ;\gamma _{0})\in R^{k\times d_{\psi }},$ respectively.

\noindent (vi) $\mathcal{W}(\theta ;\gamma _{0})$ is continuous in $\theta $
on $\Theta $ $\forall \gamma _{0}\in \Gamma .$

\noindent (vii) $0<\lambda _{\min }(\mathcal{W}(\psi _{0},\pi ;\gamma
_{0}))\leq \lambda _{\max }(\mathcal{W}(\psi _{0},\pi ;\gamma _{0}))<\infty
, $ $\forall \pi \in \Pi ,$ $\forall \gamma _{0}\in \Gamma .$

\noindent (viii) $\lambda _{\min }(g_{\psi }(\psi _{0},\pi ;\gamma
_{0})^{\prime }\mathcal{W}(\psi _{0},\pi ;\gamma _{0})g_{\psi }(\psi
_{0},\pi ;\gamma _{0}))>0,$ $\forall \pi \in \Pi ,$ $\forall \gamma _{0}\in
\Gamma $ with $\beta _{0}=0.$

\noindent (ix) $\Psi (\pi )$ is compact $\forall \pi \in \Pi ,$ and $\Pi $
and $\Theta $ are compact.

\noindent (x) $\forall \varepsilon >0,$ $\exists \delta >0$ such that $%
d_{H}\left( \Psi \left( \pi _{1}\right) ,\Psi \left( \pi _{2}\right) \right)
<\varepsilon $ $\forall \pi _{1},\pi _{2}\in \Pi $ with $\left\Vert \pi
_{1}-\pi _{2}\right\Vert <\delta ,$ where $d_{H}\left( \cdot \right) $ is
the Hausdorff metric.\medskip

Assumption GMM1(i) is the key condition that concerns the lack of
identification (by the moment functions) when $\beta =0.$ Assumptions
GMM1(ii)-(x) are mostly fairly standard GMM regularity conditions, but with
some adjustments due to the lack of identification of $\pi $ when $\beta =0,$
e.g., see Assumption GMM1(iii). Note that Assumption GMM1(viii) involves the
derivative matrix of $g_{0}(\theta ;\gamma _{0})$ with respect to $\psi $
only, not $\theta =(\psi ,\pi ).$ In consequence, this assumption is not
restrictive.

The weight matrix $\mathcal{W}_{n}(\theta )$ depends on $\theta $ only when
a continuous updating GMM estimator is considered. For a two-step estimator, 
$\mathcal{W}_{n}(\theta )$ depends on a preliminary estimator $\overline{%
\theta }_{n},$ but does not depend on $\theta .$ Let $\mathcal{W}_{n}(%
\overline{\theta }_{n})$ be the weight matrix for a two-step estimator.
(This is a slight abuse of notation because in (\ref{GMM CF}) $\mathcal{W}%
_{n}(\theta )$ and $\overline{g}_{n}(\theta )$ are indexed by the same $%
\theta ,$ whereas here they are different.)

For the weight matrix of a two-step estimator to satisfy Assumption
GMM1(ii), we need 
\begin{equation}
\mathcal{W}_{n}(\overline{\theta }_{n})\rightarrow _{p}\mathcal{W}(\theta
_{0};\gamma _{0})  \label{conv in prob}
\end{equation}%
for some non-random matrix $\mathcal{W}(\theta _{0};\gamma _{0})$ under $%
\{\gamma _{n}\}\in \Gamma (\gamma _{0}).$ This is not an innocuous
assumption in the weak identification scenario because the preliminary
estimator $\overline{\theta }_{n}$ may be inconsistent. Lemma \ref{Lemma Two
step weight matrix} below shows that (\ref{conv in prob}) holds despite the
inconsistency of $\overline{\pi }_{n}$ that occurs under $\{\gamma _{n}\}\in
\Gamma (\gamma _{0},0,b)$ with $||b||<\infty ,$ where $\overline{\theta }%
_{n}=(\overline{\psi }_{n},\overline{\pi }_{n}).$

\begin{lemma}
\hspace{-0.08in}\textbf{.} \label{Lemma Two step weight matrix}Suppose $%
\overline{\theta }_{n}=(\overline{\psi }_{n},\overline{\pi }_{n})$ is an
estimator of $\theta $ such that \emph{(i)} $\overline{\theta }%
_{n}\rightarrow _{p}\theta _{0}$ under $\{\gamma _{n}\}\in \Gamma (\gamma
_{0}),$ $\forall \gamma _{0}\in \Gamma $ with $\beta _{0}\neq 0,$ \emph{(ii)}
$\overline{\psi }_{n}\rightarrow _{p}\psi _{0}$ under $\{\gamma _{n}\}\in
\Gamma (\gamma _{0}),$ $\forall \gamma _{0}\in \Gamma $ with $\beta _{0}=0,$ 
\emph{(iii)} $\mathcal{W}_{n}(\theta )$ satisfies Assumptions \emph{GMM1(i),}
\emph{GMM1(ii),} and \emph{GMM1(vi),} and \emph{(iv)} $\Pi $ is compact.
Then, $\mathcal{W}_{n}(\overline{\theta }_{n})\rightarrow _{p}\mathcal{W}%
(\theta _{0};\gamma _{0})$ under $\{\gamma _{n}\}\in \Gamma (\gamma _{0})$ $%
\forall \gamma _{0}\in \Gamma .$
\end{lemma}

\noindent \textbf{Comments.} \textbf{1.} Lemma \ref{Lemma Two step weight
matrix} allows for inconsistency of $\overline{\pi }_{n},$ i.e., $\overline{%
\pi }_{n}-\pi _{n}\neq o_{p}(1),$ under $\{\gamma _{n}\}\in \Gamma (\gamma
_{0})$ with $\beta _{0}=0.$ Inconsistency occurs under $\{\gamma _{n}\}\in
\Gamma (\gamma _{0},0,b)$ with $||b||<\infty ,$ see Theorem \ref{Thm dist'n
of estimator b=finite}(a) below.

\textbf{2.} Typically, the preliminary estimator $\overline{\theta }_{n}$ is
obtained by minimizing $Q_{n}(\theta )$ in (\ref{GMM CF}) with a weight
matrix $\mathcal{W}_{n}(\theta )$ that does not depend on $\theta $ or any
estimator of $\theta .$ In such cases, the properties of $\overline{\theta }%
_{n}$ assumed in Lemma \ref{Lemma Two step weight matrix} hold provided
Assumption GMM1 holds with the specified weight matrix.\footnote{%
This follows from the combination of Lemma \ref{Lemma GMM A and B3} in
Appendix A and Lemma 3.1 of AC1.}\medskip

\noindent \textbf{Example 1 (cont.). }For this example, the key quantities
in Assumption GMM1 are 
\begin{eqnarray}
g_{0}(\theta ;\gamma _{0})\hspace{-0.08in} &=&\hspace{-0.08in}E_{\phi
_{0}}(\beta _{0}h(X_{1,i},\pi _{0})-\beta h(X_{1,i},\pi )+X_{2,i}^{\prime
}(\zeta _{0}-\zeta ))Z_{i},  \notag \\
\mathcal{W}(\theta ;\gamma _{0})\hspace{-0.08in} &=&\hspace{-0.08in}\mathcal{%
W}(\gamma _{0})=\left( E_{\phi _{0}}Z_{i}Z_{i}^{\prime }\right) ^{-1}, 
\notag \\
g_{\psi }(\theta ;\gamma _{0})\hspace{-0.08in} &=&\hspace{-0.08in}-E_{\phi
_{0}}Z_{i}d_{\psi ,i}(\pi )^{\prime },\text{ and }g_{\theta }(\theta ;\gamma
_{0})=-E_{\phi _{0}}Z_{i}d_{\theta ,i}(\pi )^{\prime },\text{ where}  \notag
\\
d_{\psi ,i}(\pi )\hspace{-0.08in} &=&\hspace{-0.08in}(h(X_{1,i},\pi
),X_{2,i})\in R^{d_{X_{2}}+1}\text{ and }  \notag \\
d_{\theta ,i}(\pi )\hspace{-0.08in} &=&\hspace{-0.08in}(h(X_{1,i},\pi
),X_{2,i},\beta h_{\pi }(X_{1,i},\pi ))\in R^{d_{X_{2}}+d_{\pi }+1}.
\label{form of g_0 and weight}
\end{eqnarray}

Assumption GMM1(i) holds by the form of $\overline{g}_{n}(\theta )$ and $%
\mathcal{W}_{n}$ in (\ref{gmm CF}) and the fact that $U_{i}(\theta )$ does
not depend on $\pi $ when $\beta =0.$ Assumption GMM1(ii) holds by the
uniform LLN in Lemma \ref{Lemma uniform convergence} in Supplemental
Appendix D under the conditions in (\ref{gmm phi space}).

To verify Assumption GMM1(iii), we write%
\begin{equation}
g_{0}(\psi ,\pi ;\gamma _{0})-g_{0}(\psi _{0},\pi ;\gamma _{0})=E_{\phi
_{0}}(-\beta h(X_{1,i},\pi )+X_{2,i}^{\prime }(\zeta _{0}-\zeta ))Z_{i}= 
\left[ E_{\phi _{0}}Z_{i}d_{\psi ,i}(\pi )^{\prime }\right] \Delta ,
\label{GMM1(iii)}
\end{equation}%
where $\Delta =(-\beta ,\zeta _{0}-\zeta )\in R^{d_{X_{2}}+1}.$ We need to
show that when $\beta _{0}=0$ the quantity in (\ref{GMM1(iii)}) does not
equal zero $\forall \psi \neq \psi _{0}$ and $\forall \pi \in \Pi .$ This
holds because $d_{\psi ,i}(\pi )$ is a sub-vector of $d_{\psi ,i}^{\ast
}(\pi _{1},\pi _{2})$ and $E_{\phi }Z_{i}d_{\psi ,i}^{\ast }(\pi _{1},\pi
_{2})^{\prime }$ has full column rank $\forall \pi _{1},\pi _{2}\in \Pi $
with $\pi _{1}\neq \pi _{2}$ by (\ref{gmm phi space}).

To verify Assumption GMM1(iv), we write%
\begin{eqnarray}
g_{0}(\theta ;\gamma _{0})-g_{0}(\theta _{0};\gamma _{0})\hspace{-0.08in} &=&%
\hspace{-0.08in}E_{\phi _{0}}(\beta _{0}h(X_{1,i},\pi _{0})-\beta
h(X_{1,i},\pi )+X_{2,i}^{\prime }(\zeta _{0}-\zeta ))Z_{i}  \notag \\
&=&\hspace{-0.08in}\left[ E_{\phi _{0}}Z_{i}d_{\psi ,i}^{\ast }(\pi _{0},\pi
)^{\prime }\right] c,  \label{GMM1(iv)}
\end{eqnarray}%
where $c=(\beta _{0},-\beta ,\zeta _{0}-\zeta )\in R^{d_{X_{2}}+2}.$ We need
to show that when $\beta _{0}\neq 0$ the quantity in (\ref{GMM1(iv)}) does
not equal zero when $\theta \neq \theta _{0}.$ This holds when $\pi \neq \pi
_{0}$ because $E_{\phi _{0}}Z_{i}d_{\psi ,i}^{\ast }(\pi _{0},\pi )^{\prime
} $ has full column rank for $\pi \neq \pi _{0}$ by (\ref{gmm phi space}).
When $\pi =\pi _{0},$%
\begin{equation}
g_{0}(\theta ;\gamma _{0})-g_{0}(\theta _{0};\gamma _{0})=g_{0}(\psi ,\pi
_{0};\gamma _{0})-g_{0}(\psi _{0},\pi _{0};\gamma _{0})=\left[ E_{\phi
_{0}}Z_{i}d_{\psi ,i}(\pi _{0})^{\prime }\right] \Delta _{1},
\label{GMM1(iv)-2}
\end{equation}%
where $\Delta _{1}=(\beta _{0}-\beta ,\zeta _{0}-\zeta )\in R^{d_{X_{2}}+1}.$
The quantity in (\ref{GMM1(iv)-2}) does not equal zero for $\psi \neq \psi
_{0}$ because $E_{\phi _{0}}Z_{i}d_{\psi ,i}(\pi _{0})^{\prime }$ has full
column rank. This completes the verification of Assumption GMM1(iv).

Assumption GMM1(v) holds by the assumption that $h(x,\pi )$ is twice
continuously differentiable wrt $\pi $ and the moment conditions in (\ref%
{gmm phi space}). Assumption GMM1(vi) holds automatically because $\mathcal{W%
}(\theta ;\gamma _{0})=(E_{\phi _{0}}Z_{i}Z_{i}^{\prime })^{-1}$ does not
depend on $\theta .$ Assumption GMM1(vii) holds because $E_{\phi
_{0}}Z_{i}Z_{i}^{\prime }\in R^{k\times k}$ is positive definite $\forall
\gamma _{0}\in \Gamma .$ Assumption GMM1(viii) holds because $\mathcal{W}%
(\psi _{0},\pi ;\gamma _{0})=E_{\phi _{0}}Z_{i}Z_{i}^{\prime }$ is positive
definite and $g_{\psi }(\psi _{0},\pi ;\gamma _{0})$ has full rank by the
conditions in (\ref{gmm phi space}). Assumption GMM1(ix) holds because $%
\Theta =\mathcal{B}\times \mathcal{Z}\times \Pi ,\ $and $\mathcal{B},$ $%
\mathcal{Z},$ $\Pi $, and $\Psi =\mathcal{B}\times \mathcal{Z}$ are all
compact. Assumption GMM1(x) holds automatically because $\Psi $ does not
depend on $\pi .$ $\square $

For brevity, the verifications of Assumptions GMM1 and GMM2-GMM5 below for
the probit model with endogeneity are given in Section \ref{Example 2 Verif
of As.s Sec}.

\subsection{\hspace{-0.23in}\textbf{.}\hspace{0.18in}Assumption GMM2}

\hspace{0.25in}The next assumption, Assumption GMM2, is used when verifying
that the GMM criterion function satisfies a quadratic approximation with
respect to $\psi $ when $\{\gamma _{n}\}\in \Gamma (\gamma _{0},0,b)$ and
with respect to $\theta $ when $\{\gamma _{n}\}\in \Gamma (\gamma
_{0},\infty ,\omega _{0}).$ In the former case, the expansion is around the
value 
\begin{equation}
\psi _{0,n}=(0,\zeta _{n}),
\end{equation}%
rather than around the true value $\psi _{n}=(\beta _{n},\zeta _{n}).$ The
reason for expanding around $\psi _{0,n}$ is that the first term in the
expansion of $Q_{n}(\psi ,\pi )$ does not depend on $\pi $ when $\psi =\psi
_{0,n}$ by Assumption GMM1(i).

Under $\{\gamma _{n}\}\in \Gamma (\gamma _{0}),$ define the centered sample
moment conditions by 
\begin{equation}
\widetilde{g}_{n}\left( \theta ;\gamma _{0}\right) =\overline{g}_{n}\left(
\theta \right) -g_{0}\left( \theta ;\gamma _{0}\right) .
\end{equation}

We define a matrix $B(\beta )$ that is used to normalize the (generalized)
first-derivative matrix\ of the sample moments $\overline{g}_{n}(\theta )$
so that it is full-rank asymptotically. Let $B(\beta )$ be the $d_{\theta
}\times d_{\theta }$ diagonal matrix defined by%
\begin{equation}
B(\beta )=Diag\{1_{d_{\psi }}^{\prime },\iota (\beta )1_{d_{\pi }}^{\prime
}\},
\end{equation}%
where $\iota (\beta )=\beta $ if $\beta $ is a scalar and $\iota (\beta
)=||\beta ||$ if $\beta $ is a vector.\footnote{%
The matrix $B(\beta )$ is defined differently in the scalar and vector $%
\beta $ cases because in the scalar case the use of $\beta ,$ rather than $%
||\beta ||,$ produces noticeably simpler (but equivalent) formulae, but in
the vector case $||\beta ||$ is required.}\medskip

\noindent \textbf{Assumption GMM2. }(i) Under $\{\gamma _{n}\}\in \Gamma
(\gamma _{0},0,b),$ \newline
$\sup_{\psi \in \Psi (\pi ):||\psi -\psi _{0,n}||\leq \delta _{n}}||%
\widetilde{g}_{n}(\psi ,\pi ;\gamma _{0})-\widetilde{g}_{n}(\psi _{0,n},\pi
;\gamma _{0})||/(n^{-1/2}+||\psi -\psi _{0,n}||)=o_{p\pi }(1)$ for all
constants $\delta _{n}\rightarrow 0.$

\noindent (ii) Under $\{\gamma _{n}\}\in \Gamma (\gamma _{0},\infty ,\omega
_{0}),$ $\sup_{\theta \in \Theta _{n}\left( \delta _{n}\right) }||\widetilde{%
g}_{n}(\theta ;\gamma _{0})-\widetilde{g}_{n}(\theta _{n};\gamma
_{0})||/(n^{-1/2}+||B(\beta _{n})(\theta -\theta _{n})||)=o_{p}(1)$ for all
constants $\delta _{n}\rightarrow 0,$ where $\Theta _{n}\left( \delta
_{n}\right) =\{\theta \in \Theta :\left\Vert \psi -\psi _{n}\right\Vert \leq
\delta _{n}\left\Vert \beta _{n}\right\Vert $ and $\left\Vert \pi -\pi
_{n}\right\Vert \leq \delta _{n}\}.$\medskip

When $\overline{g}_{n}\left( \theta \right) $ is continuously differentiable
in $\theta ,$ Assumption GMM2 is easy to verify. In this case, Assumption
GMM2$^{\ast }$ below is a set of sufficient conditions for Assumption GMM2.

Assumption GMM2 allows for non-smooth sample moment conditions. It is
analogous to Assumption GMM2(d) of Andrews (2002), which in turn is shown to
be equivalent to condition (iii) of Theorem 3.3 of Pakes and Pollard (1989).
In contrast to these conditions in the literature, Assumption GMM2 applies
under drifting sequences of true parameters and provides conditions that
allow for weak identification. Nevertheless, Assumption GMM2 can be verified
by methods used in Pakes and Pollard (1989) and Andrews (2002).\medskip

\noindent \textbf{Assumption GMM2}$^{\ast }$\textbf{.} (i) $\overline{g}%
_{n}(\theta )$ is continuously differentiable in $\theta $ on $\Theta $ $%
\forall n\geq 1.$

\noindent (ii) Under $\{\gamma _{n}\}\in \Gamma (\gamma _{0},0,b),$ $%
\sup_{\theta \in \Theta :||\psi -\psi _{0,n}||\leq \delta _{n}}\left\Vert
(\partial /\partial \psi ^{\prime })\overline{g}_{n}(\theta )-g_{\psi
}(\theta ;\gamma _{0})\right\Vert =o_{p}(1)$ for all constants $\delta
_{n}\rightarrow 0.$

\noindent (iii) Under $\{\gamma _{n}\}\in \Gamma (\gamma _{0},\infty ,\omega
_{0}),$ $\sup_{\theta \in \Theta _{n}\left( \delta _{n}\right) }\left\Vert
\left( (\partial /\partial \theta ^{\prime })\overline{g}_{n}(\theta
)-g_{\theta }(\theta ;\gamma _{0})\right) B^{-1}(\beta _{n})\right\Vert
=o_{p}(1)$ for all constants $\delta _{n}\rightarrow 0.$\medskip

When $\overline{g}_{n}(\theta )$ takes the form of a sample average,
Assumption GMM2$^{\ast }$ can be verified by a uniform LLN and the switch of 
$E$ and $\partial $ under some regularity conditions.\medskip

\begin{lemma}
\hspace{-0.08in}\textbf{.} \label{Lemma GMM Smooth Sufficient}Assumption 
\emph{GMM2}$^{\ast }$ implies Assumption \emph{GMM2.}
\end{lemma}

\noindent \textbf{Example 1 (cont.). }We verify Assumption GMM2 in this
example using the sufficient condition Assumption GMM2$^{\ast }.$ The key
quantities in Assumption GMM2$^{\ast }$ are%
\begin{equation}
\frac{\partial }{\partial \psi ^{\prime }}\overline{g}_{n}(\theta
)=n^{-1}\sum_{i=1}^{n}Z_{i}d_{\psi ,i}(\pi )^{\prime }\text{ and }\frac{%
\partial }{\partial \theta ^{\prime }}\overline{g}_{n}(\theta
)=n^{-1}\sum_{i=1}^{n}Z_{i}d_{\theta ,i}(\pi )^{\prime }.
\label{g_bar  partial der}
\end{equation}

Assumption GMM2$^{\ast }$(i) holds with the partial derivatives given in (%
\ref{g_bar partial der}). Assumption GMM2$^{\ast }$(ii) holds by the uniform
LLN given in Lemma \ref{Lemma uniform convergence} in Supplemental Appendix
D under the conditions in (\ref{gmm phi space}). Assumption GMM2$^{\ast }$%
(iii) holds by this uniform LLN and $\beta /\beta _{n}=1+o(1)$ for $\theta
\in \Theta _{n}(\delta _{n}).$ $\square $

\subsection{\hspace{-0.23in}\textbf{.}\hspace{0.18in}Assumption GMM3}

\hspace{0.25in}Under Assumptions GMM1 and GMM2, Assumption GMM3 below is
used when establishing the asymptotic distribution of the GMM estimator
under weak and semi-strong identification, i.e., when $\{\gamma _{n}\}\in
\Gamma (\gamma _{0},0,b).$

Define the $k\times d_{\beta }$ matrix of partial derivatives of the average
population moment function wrt the true $\beta $ value, $\beta ^{\ast },$ to
be%
\begin{equation}
K_{n,g}(\theta ;\gamma ^{\ast })=n^{-1}\sum_{i=1}^{n}\frac{\partial }{%
\partial \beta ^{\ast \prime }}E_{\gamma ^{\ast }}g(W_{i},\theta ),
\label{exp fn2}
\end{equation}%
where $\gamma ^{\ast }=(\beta ^{\ast },\zeta ^{\ast },\pi ^{\ast },\phi
^{\ast }).$ The domain of the function $K_{n,g}(\theta ;\gamma ^{\ast })$ is 
$\Theta _{\delta }\times \Gamma _{0},$ where $\Theta _{\delta }=\{\theta \in
\Theta :||\beta ||<\delta \}$ and $\Gamma _{0}=\{\gamma _{a}=(a\beta ,\zeta
,\pi ,\phi )\in \Gamma :$ $\gamma =(\beta ,\zeta ,\pi ,\phi )\in \Gamma $
with $||\beta ||<\delta $ and $a\in \lbrack 0,1]\}$ for some $\delta >0.$%
\footnote{%
The constant $\delta >0$ is as in Assumption B2(iii) stated below. The set $%
\Gamma _{0}$ is not empty by Assumption B2(ii).}\medskip

\noindent \textbf{Assumption GMM3.} (i) $\overline{g}_{n}\left( \theta
\right) $ takes the form $\overline{g}_{n}(\theta
)=n^{-1}\sum_{i=1}^{n}g(W_{i},\theta )$ for some function $g\left(
W_{i},\theta \right) \in R^{k}$ $\forall \theta \in \Theta .$

\noindent (ii) $E_{\gamma ^{\ast }}g(W_{i},\psi ^{\ast },\pi )=0$ $\forall
\pi \in \Pi ,$ $\forall i\geq 1$ when the true parameter is $\gamma ^{\ast }$
$\forall \gamma ^{\ast }=(\psi ^{\ast },\pi ^{\ast },\phi ^{\ast })\in
\Gamma $ with $\beta ^{\ast }=0.$

\noindent (iii) Under $\{\gamma _{n}\}\in \Gamma (\gamma _{0},0,b),$ $%
n^{-1/2}\sum_{i=1}^{n}(g(W_{i},\psi _{0,n},\pi _{n})-E_{\gamma
_{n}}g(W_{i},\psi _{0,n},\pi _{n}))\rightarrow _{d}N(0,\allowbreak \Omega
_{g}(\gamma _{0}))$ for some $k$ by $k$ matrix $\Omega _{g}(\gamma _{0}).$

\noindent (iv) (a) $K_{n,g}(\theta ;\gamma ^{\ast })$ exists $\forall
(\theta ,\gamma ^{\ast })\in \Theta _{\delta }\times \Gamma _{0},$ $\forall
n\geq 1.$ (b) For some non-stochastic $k\times d_{\beta }$ matrix-valued
function $K_{g}(\psi _{0},\pi ;\gamma _{0}),$ $K_{n,g}(\psi _{n},\pi ;%
\widetilde{\gamma }_{n})\rightarrow K_{g}(\psi _{0},\pi ;\gamma _{0})$
uniformly over $\pi \in \Pi $ for all non-stochastic sequences $\{\psi
_{n}\} $ and $\{\widetilde{\gamma }_{n}\}$ such that $\widetilde{\gamma }%
_{n}\in \Gamma ,$ $\widetilde{\gamma }_{n}\rightarrow \gamma _{0}=(0,\zeta
_{0},\pi _{0},\phi _{0})$ for some $\gamma _{0}\in \Gamma ,$ $(\psi _{n},\pi
)\in \Theta ,$ and $\psi _{n}\rightarrow \psi _{0}=(0,\zeta _{0}).$ (c) $%
K_{g}(\psi _{0},\pi ;\gamma _{0})$ is continuous on $\Pi $ $\forall \gamma
_{0}\in \Gamma $ with $\beta _{0}=0.$

\noindent (v) $\forall \omega _{0}\in R^{d_{\beta }}$ with $||\omega
_{0}||=1,$ $K_{g}(\psi _{0},\pi ;\gamma _{0})\omega _{0}=g_{\psi }(\psi
_{0},\pi ;\gamma _{0})S$ for some $S\in R^{d_{\psi }}$ if and only if $\pi
=\pi _{0}.$

\noindent (vi) Under $\{\gamma _{n}\}\in \Gamma (\gamma _{0},0,b),$ $%
n^{-1}\sum_{i=1}^{n}(\partial /\partial \psi ^{\prime })E_{\gamma
_{n}}g(W_{i},\psi ,\pi )|_{(\psi ,\pi )=\theta _{n}}\rightarrow g_{\psi
}(\theta _{0};\gamma _{0}).$\medskip

Assumption GMM3(iii) can be verified using a triangular array CLT. Although
Assumption GMM3(iv) is somewhat complicated, it is not restrictive, see the
verification of it in the two examples. A set of primitive sufficient
conditions for Assumption GMM3(iv) is given in Appendix A of AC1-SM.%
\footnote{%
The sufficient conditions are for Assumption C5 of AC1, which is the same as
Assumption GMM3(iv) but with $m(W_{i},\theta )$ of AC1 in place of $%
g(W_{i},\theta ).$}

In Assumption GMM3(v), the equality holds for $\pi =\pi _{0}$ with $%
S=-[I_{d_{\beta }}:0_{d_{\beta }\times d_{\zeta }}]^{\prime }\omega _{0}$ by
Lemma 9.3 in AC1-SM under the assumptions therein. For any $\pi \neq \pi
_{0},$ Assumption GMM(v) requires that any linear combination of the columns
of $K_{g}(\psi _{0},\pi ;\gamma _{0})$ cannot be in the column space of $%
g_{\psi }(\psi _{0},\pi ;\gamma _{0}).$

With identically distributed observations, Assumption GMM3(vi) can be
verified by the exchange of $E$ and $\partial $ under suitable regularity
conditions.\medskip

\noindent \textbf{Example 1 (cont.). }For this example, the key quantities
in Assumption GMM3 are%
\begin{eqnarray}
g(W_{i},\theta )\hspace{-0.08in} &=&\hspace{-0.08in}(Y_{i}-\beta
h(X_{1,i},\pi )-X_{2,i}^{\prime }\zeta )Z_{i},  \notag \\
\Omega _{g}(\gamma _{0})\hspace{-0.08in} &=&\hspace{-0.08in}E_{\phi
_{0}}U_{i}^{2}Z_{i}Z_{i}^{\prime },\text{ and}  \notag \\
K_{g,n}(\theta ,\gamma ^{\ast })\hspace{-0.08in} &=&\hspace{-0.08in}%
K_{g}(\theta ,\gamma ^{\ast })=E_{\phi ^{\ast }}h(X_{1,i},\pi ^{\ast })Z_{i}.
\label{GMM3_key quant}
\end{eqnarray}

Assumption GMM3(i) holds with $g(W_{i},\theta )$ in (\ref{GMM3_key quant}).

To verify Assumption GMM3(ii), we have%
\begin{equation}
E_{\phi ^{\ast }}g(W_{i},\theta )=E_{\phi ^{\ast }}\left( U_{i}+\beta ^{\ast
}h(X_{1,i},\pi ^{\ast })-\beta h(X_{1,i},\pi )+X_{2,i}^{\prime }(\zeta
^{\ast }-\zeta )\right) Z_{i}.  \label{GMM3 - Eg fn}
\end{equation}%
When $\beta =\beta ^{\ast }=0$ and $\zeta =\zeta ^{\ast },$ $E_{\phi ^{\ast
}}g(W_{i},\theta )=0$ $\forall \pi \in \Pi .$

Next, we show that Assumption GMM3(iii) holds with $\Omega _{g}(\gamma _{0})$
in (\ref{GMM3_key quant}). Define 
\begin{eqnarray}
G_{g,n}(\pi _{n})\hspace{-0.08in} &=&\hspace{-0.08in}n^{-1/2}\sum_{i=1}^{n}%
\left( g(W_{i},\psi _{0,n},\pi _{n})-E_{\phi _{n}}g(W_{i},\psi _{0,n},\pi
_{n})\right)  \label{gmm G_g} \\
\hspace{-0.08in} &=&\hspace{-0.08in}n^{-1/2}\sum_{i=1}^{n}U_{i}Z_{i}+\beta
_{n}[n^{-1/2}\sum_{i=1}^{n}\left( h(X_{i},\pi _{n})Z_{i}-E_{\phi
_{n}}h(X_{i},\pi _{n})Z_{i}\right) ].  \notag
\end{eqnarray}%
By the CLT for triangular arrays of row-wise i.i.d. random variables given
in Lemma \ref{Lemma CLT, array} in Supplemental Appendix C, $%
n^{-1/2}\sum_{i=1}^{n}U_{i}Z_{i}\rightarrow _{d}N(0,\Omega _{g}(\gamma
_{0})).$ The second term in the second line of (\ref{gmm G_g}) is $o_{p}(1)$
because $\beta _{n}\rightarrow 0$ and $n^{-1/2}\sum_{i=1}^{n}(h(X_{i},\pi
_{n})Z_{i}-E_{\phi _{n}}h(X_{i},\allowbreak \pi _{n})Z_{i})=O_{p}(1)$ by the
CLT in Lemma \ref{Lemma CLT, array} in Supplemental Appendix C. Hence, $%
G_{g,n}(\pi _{n})\rightarrow _{d}N(0,\Omega _{g}(\gamma _{0})).$

Next, we show that Assumption GMM3(iv) holds with $K_{g,n}(\theta ,\gamma
^{\ast })$ and $K_{g}(\theta ,\gamma ^{\ast })$ in (\ref{GMM3_key quant}).
Assumption GMM3(iv)(a) is implied by (\ref{GMM3 - Eg fn}) and the moment
conditions in (\ref{gmm phi space}). The convergence in Assumption
GMM3(iv)(b) holds because $\phi _{n}\rightarrow \phi _{0}$ induces weak
convergence of $(X_{i},Z_{i})$ by the definition of the metric on $\Phi
^{\ast }$ and $E_{\phi }\sup_{\pi \in \Pi }||h(X_{1,i},\allowbreak \pi
)Z_{i}||^{1+\delta }\leq C$ for some $\delta >0$ and $C<\infty $ by the
conditions in (\ref{gmm phi space}). The convergence holds uniformly over $%
\pi \in \Pi $ by Lemma \ref{Lemma uniform convergence} in Supplemental
Appendix D because $\Pi $ is compact and $E_{\phi ^{\ast }}\sup_{\pi \in \Pi
}||h_{\pi }(X_{1,i},\pi )||\cdot ||Z_{i}||\leq C$ for some $C<\infty .$
Assumption GMM3(iv)(c) holds because $\Pi $ is compact, $h(x,\pi )$ is
continuous in $\pi ,$ and $E_{\phi ^{\ast }}\sup_{\pi \in \Pi
}||h(X_{1,i},\pi )||\cdot ||Z_{i}||\leq C$ for some $C<\infty $ by the
conditions in (\ref{gmm phi space}). This completes the verification of
Assumption GMM(iv).

To verify Assumption GMM3(v), note that for $S\in R^{d_{X_{2}}+1}$ we have%
\begin{eqnarray}
&&K_{g}(\psi _{0},\pi ;\gamma _{0})\omega _{0}-g_{\psi }(\psi _{0},\pi
;\gamma _{0})S  \notag \\
\hspace{-0.08in} &=&\hspace{-0.08in}E_{\phi _{0}}Z_{i}h(X_{1,i},\pi
_{0})\omega _{0}+E_{\phi _{0}}Z_{i}d_{\psi ,i}(\pi )^{\prime }S  \notag \\
\hspace{-0.08in} &=&\hspace{-0.08in}E_{\phi _{0}}Z_{i}d_{\psi ,i}^{\ast
}(\pi _{0},\pi )^{\prime }\Delta _{2},\text{ where }\Delta _{2}=(\omega
_{0},S)\neq 0_{d_{\zeta }+2}.
\end{eqnarray}%
Because $E_{\phi _{0}}Z_{i}d_{\psi ,i}^{\ast }(\pi _{0},\pi )^{\prime }$ has
full column rank for all $\pi \neq \pi _{0}$ by (\ref{gmm phi space}), $%
K_{g}(\psi _{0},\pi ;\gamma _{0})\omega _{0}\neq g_{\psi }(\psi _{0},\pi
;\gamma _{0})S$ for any $\pi \neq \pi _{0}.$ When $\pi =\pi _{0},$ $%
K_{g}(\psi _{0},\pi ;\gamma _{0})\omega _{0}=g_{\psi }(\psi _{0},\pi ;\gamma
_{0})S$ if $S=(-\omega _{0},0_{d_{\zeta }})$ $(\in R^{d_{\zeta }+1}).$ This
completes the verification of Assumption GMM3 for this example. $\square $

\subsection{\hspace{-0.23in}\textbf{.}\hspace{0.18in}Assumption GMM4}

\hspace{0.25in}To obtain the asymptotic distribution of $\widehat{\pi }_{n}$
when $\beta _{n}=O(n^{-1/2})$ via the continuous mapping theorem, we use
Assumption GMM4 stated below.

Under Assumptions GMM1(i) and GMM1(ii), $\mathcal{W}(\psi _{0},\pi ;\gamma
_{0})$ does not depend on $\pi $ when $\beta _{0}=0.$ For simplicity, let $%
\mathcal{W}(\psi _{0};\gamma _{0})$ abbreviate $\mathcal{W}(\psi _{0},\pi
;\gamma _{0})$ when $\beta _{0}=0.$

The following quantities arise in the asymptotic distributions of $\widehat{%
\theta }_{n}$ and various test statistics when $\{\gamma _{n}\}\in \Gamma
(\gamma _{0},0,b)$ and $||b||<\infty .$ Define%
\begin{eqnarray}
\Omega (\pi _{1},\pi _{2};\gamma _{0})\hspace{-0.08in} &=&\hspace{-0.08in}%
g_{\psi }(\psi _{0},\pi _{1};\gamma _{0})^{\prime }\mathcal{W}(\psi
_{0};\gamma _{0})\Omega _{g}(\gamma _{0})\mathcal{W}(\psi _{0};\gamma
_{0})g_{\psi }(\psi _{0},\pi _{2};\gamma _{0}),  \notag \\
H(\pi ;\gamma _{0})\hspace{-0.08in} &=&\hspace{-0.08in}g_{\psi }(\psi
_{0},\pi ;\gamma _{0})^{\prime }\mathcal{W}(\psi _{0};\gamma _{0})g_{\psi
}(\psi _{0},\pi ;\gamma _{0}),\text{ and}  \notag \\
K(\psi _{0},\pi ;\gamma _{0})\hspace{-0.08in} &=&\hspace{-0.08in}g_{\psi
}(\psi _{0},\pi ;\gamma _{0})^{\prime }\mathcal{W}(\psi _{0};\gamma
_{0})K_{g}(\psi _{0},\pi ;\gamma _{0}).  \label{Omega, H, and K defns}
\end{eqnarray}%
Let $G(\cdot ;\gamma _{0})$ denote a mean zero Gaussian process indexed by $%
\pi \in \Pi $ with bounded continuous sample paths and covariance kernel $%
\Omega (\pi _{1},\pi _{2};\gamma _{0})$ for $\pi _{1},\pi _{2}\in \Pi .$

Next, we define a \textquotedblleft weighted non-central
chi-square\textquotedblright\ process $\{\xi (\pi ;\gamma _{0},b):\pi \in
\Pi \}$ that arises in the asymptotic distributions. Let%
\begin{equation}
\xi (\pi ;\gamma _{0},b)=-\frac{1}{2}\left( G(\pi ;\gamma _{0})+K(\pi
;\gamma _{0}\right) b)^{\prime }H^{-1}(\pi ;\gamma _{0})\left( G(\pi ;\gamma
_{0})+K(\pi ;\gamma _{0})b\right) .  \label{Defn of Xi Stoch Process}
\end{equation}%
Under Assumptions GMM1-GMM3, $\{\xi (\pi ;\gamma _{0},b):\pi \in \Pi \}$ has
bounded continuous sample paths a.s.\medskip

\noindent \textbf{Assumption GMM4.\ }Each sample path of the stochastic
process $\{\xi (\pi ;\gamma _{0},b):\pi \in \Pi \}$ in some set $A(\gamma
_{0},b)$ with $P_{\gamma _{0}}(A(\gamma _{0},b))=1$ is minimized over $\Pi $
at a unique point (which may depend on the sample path), denoted $\pi ^{\ast
}(\gamma _{0},b),$ $\forall \gamma _{0}\in \Gamma $ with $\beta _{0}=0,$ $%
\forall b$ with $||b||<\infty .$\medskip

In Assumption GMM4, $\pi ^{\ast }(\gamma _{0},b)$ is random.

Next, we provide a sufficient condition for Assumption GMM4. We partition $%
g_{\psi }(\theta ;\gamma _{0})\allowbreak \in R^{k\times d_{\psi }}$ as%
\begin{equation}
g_{\psi }(\theta ;\gamma _{0})=[g_{\beta }(\theta ;\gamma _{0}):g_{\zeta
}(\theta ;\gamma _{0})],
\end{equation}%
where $g_{\beta }(\theta ;\gamma _{0})\in R^{k\times d_{\beta }}$ and $%
g_{\zeta }(\theta ;\gamma _{0})\in R^{k\times d_{\zeta }}.$ When $\beta
_{0}=0,$ $g_{\zeta }(\psi _{0},\pi ;\gamma _{0})$ does not depend on $\pi $
by Assumptions GMM1(i) and GMM3(i) and is denoted by $g_{\zeta }(\psi
_{0};\gamma _{0})$ for simplicity$.$ When $d_{\beta }=1$ and $\beta _{0}=0,$
define 
\begin{equation}
g_{\psi }^{\ast }(\psi _{0},\pi _{1},\pi _{2};\gamma _{0})=[g_{\beta }(\psi
_{0},\pi _{1};\gamma _{0}):g_{\beta }(\psi _{0},\pi _{2};\gamma
_{0}):g_{\zeta }(\psi _{0};\gamma _{0})]\in R^{k\times (d_{\zeta }+2)}.
\end{equation}

\noindent \textbf{Assumption GMM4}$^{\ast }$\textbf{.} (i) $d_{\beta }=1$
(e.g., $\beta $ is a scalar).

\noindent (ii) $g_{\psi }^{\ast }(\psi _{0},\pi _{1},\pi _{2};\gamma _{0})$
has full column rank, $\forall \pi _{1},\pi _{2}\in \Pi $ with $\pi _{1}\neq
\pi _{2},$ $\forall \gamma _{0}\in \Gamma $ with $\beta _{0}=0.$

\noindent (iii) $\Omega _{g}(\gamma _{0})$ is positive definite, $\forall
\gamma _{0}\in \Gamma $ with $\beta _{0}=0.$\medskip

\begin{lemma}
\hspace{-0.08in}\textbf{.} \label{Lemma As GMM4}Assumptions \emph{GMM1-GMM3 }%
and \emph{GMM4}$^{\ast }$ imply Assumption \emph{GMM4.}
\end{lemma}

\noindent \textbf{Example 1 (cont.). }We verify Assumption GMM4 in this
example using the sufficient condition Assumption GMM4$^{\ast }.$ The key
quantity in Assumption GMM4$^{\ast }$ is 
\begin{equation}
g_{\psi }^{\ast }(\psi _{0},\pi _{1},\pi _{2};\gamma _{0})=-E_{\phi
_{0}}Z_{i}(h(X_{1,i},\pi _{1}),h(X_{1,i},\pi _{2}),X_{2,i}^{\prime
})=-E_{\phi _{0}}Z_{i}d_{\psi ,i}^{\ast }(\pi _{1},\pi _{2}).
\end{equation}

Assumption GMM4$^{\ast }$(i) holds automatically. Assumption GMM4$^{\ast }$%
(ii) holds because $E_{\phi _{0}}Z_{i}d_{\psi ,i}^{\ast }(\pi _{1},\pi _{2})$
has full column rank $\forall \pi _{1},\pi _{2}\in \Pi $ with $\pi _{1}\neq
\pi _{2}$ by (\ref{gmm phi space}). Assumption GMM4$^{\ast }$(iii) holds
with $\Omega _{g}(\gamma _{0})=E_{\phi _{0}}U_{i}^{2}Z_{i}Z_{i}^{\prime }$
because $E_{\phi _{0}}Z_{i}Z_{i}^{\prime }$ is positive definite and $%
E(U_{i}^{2}|Z_{i})>0$ a.s. This completes the verification of Assumption
GMM4. $\square $

\subsection{\hspace{-0.23in}\textbf{.}\hspace{0.18in}Assumption GMM5}

\hspace{0.25in}Under Assumptions GMM1 and GMM2, Assumption GMM5 is used
below to establish the asymptotic distribution of the GMM estimator under
semi-strong and strong identification, i.e., when $\{\gamma _{n}\}\in \Gamma
(\gamma _{0},\infty ,\omega _{0}).$\medskip

\noindent \textbf{Assumption GMM5. }Under $\{\gamma _{n}\}\in \Gamma (\gamma
_{0},\infty ,\omega _{0}),$

\noindent (i) $n^{1/2}\overline{g}_{n}(\theta _{n})\rightarrow
_{d}N(0,V_{g}(\gamma _{0}))$ for some symmetric and positive definite $%
d_{\theta }\times d_{\theta }$ matrix $V_{g}(\gamma _{0}),$

\noindent (ii) for all constants $\delta _{n}\rightarrow 0,$ $\sup_{\theta
\in \Theta _{n}\left( \delta _{n}\right) }||(g_{\theta }(\theta ;\gamma
_{0})-g_{\theta }(\theta _{n};\gamma _{0}))B^{-1}(\beta _{n})||=o(1),$ and

\noindent (iii) $g_{\theta }(\theta _{n};\gamma _{0})B^{-1}(\beta
_{n})\rightarrow J_{g}(\gamma _{0})$ for some matrix $J_{g}(\gamma _{0})\in
R^{k\times d_{\theta }}$ with full column rank.\footnote{%
In the vector $\beta $ case, $J_{g}(\gamma _{0})$ may depend on $\omega _{0}$
as well as $\gamma _{0}.$}\medskip

Now, we define two key quantities that arise in the asymptotic distribution
of the estimator $\widehat{\theta }_{n}$ when $\{\gamma _{n}\}\in \Gamma
(\gamma _{0},\infty ,\omega _{0}).$ Let%
\begin{eqnarray}
V(\gamma _{0})\hspace{-0.08in} &=&\hspace{-0.08in}J_{g}(\gamma _{0})^{\prime
}\mathcal{W}(\theta _{0};\gamma _{0})V_{g}\left( \gamma _{0}\right) \mathcal{%
W}(\theta _{0};\gamma _{0})J_{g}\left( \gamma _{0}\right) \text{ and}  \notag
\\
J(\gamma _{0})\hspace{-0.08in} &=&\hspace{-0.08in}J_{g}(\gamma _{0})^{\prime
}\mathcal{W}(\theta _{0};\gamma _{0})J_{g}\left( \gamma _{0}\right) .
\label{J and V Matrices}
\end{eqnarray}%
Let $G^{\ast }(\gamma _{0})\sim N(0_{d_{\theta }},V(\gamma _{0}))$ for $%
\gamma _{0}\in \Gamma .\medskip $

\noindent \textbf{Example 1 (cont.). }The key quantities in Assumption GMM5
for this example are%
\begin{equation}
V_{g}(\gamma _{0})=E_{\phi _{0}}U_{i}^{2}Z_{i}Z_{i}^{\prime }\text{ and }%
J_{g}(\gamma _{0})=-E_{\phi _{0}}Z_{i}d_{i}(\pi _{0})^{\prime }.
\label{GMM C5 -1}
\end{equation}

Assumption GMM5(i) holds by the CLT for triangular arrays of row-wise i.i.d.
random variables given in Lemma \ref{Lemma CLT, array} in Supplemental
Appendix C. Assumption GMM5(ii) holds with $g_{\theta }(\theta ;\gamma _{0})$
defined as in (\ref{form of g_0 and weight}) because $\beta _{n}/\beta
=1+o(1)$ for $\theta \in \Theta _{n}(\delta _{n})$ and $g_{\theta }(\theta
;\gamma _{0})B^{-1}(\beta )=-E_{\phi _{0}}Z_{i}d_{i}(\pi )^{\prime }$ is
continuous in $\pi $ uniformly over $\pi \in \Pi ,$ which in turn holds by
the moment conditions in (\ref{gmm phi space}) and the compactness of $\Pi .$

Assumption GMM5(iii) holds because 
\begin{equation}
g_{\theta }(\theta _{n};\gamma _{n})B^{-1}(\beta _{n})=-E_{\phi
_{n}}Z_{i}d_{i}(\pi _{n})^{\prime }\rightarrow -E_{\phi _{0}}Z_{i}d_{i}(\pi
_{0})^{\prime },
\end{equation}%
where the convergence holds because (i) $E_{\phi _{n}}Z_{i}d_{i}(\pi
)^{\prime }\rightarrow E_{\phi _{0}}Z_{i}d_{i}(\pi )$ uniformly over $\pi
\in \Pi $ by arguments analogous to those used in the verification of
Assumption GMM3(iv)(b) and (ii) $\pi _{n}\rightarrow \pi _{0}.$ The matrix $%
J_{g}(\gamma _{0})$ has full column rank by (\ref{gmm phi space}). This
completes the verification of Assumption GMM5. $\square $

\subsection{\hspace{-0.23in}\textbf{.}\hspace{0.18in}Minimum Distance
Estimators}

\hspace{0.25in}Assumptions GMM1, GMM2, GMM4, and GMM5 apply equally well to
the MD estimator as to the GMM estimator. Only Assumption GMM3 does not
apply to the MD estimator. In place of part of Assumption GMM3, we employ
the following assumption for MD estimators.\medskip

\noindent \textbf{Assumption MD. }Under $\{\gamma _{n}\}\in \Gamma (\gamma
_{0},0,b),$ $n^{1/2}\overline{g}_{n}(\psi _{0,n},\pi _{n})=O_{p}(1).$\medskip

\subsection{\hspace{-0.23in}\textbf{.}\hspace{0.18in}Parameter Space
Assumptions\label{Par Space Subsec}}

\hspace{0.25in}Next, we specify conditions on the parameter spaces $\Theta $
and $\Gamma .$

Define $\Theta _{\delta }^{\ast }=\{\theta \in \Theta ^{\ast }:||\beta
||<\delta \},$ where $\Theta ^{\ast }$ is the true parameter space for $%
\theta ,$ see (\ref{True Par Space Gamma}). The optimization parameter space 
$\Theta $ satisfies:\medskip

\noindent \textbf{Assumption B1.} (i) $int(\Theta )\supset \Theta ^{\ast }.$

\noindent (ii) For some $\delta >0,$ $\Theta \supset \{\beta \in R^{d_{\beta
}}:||\beta ||<\delta \}\times \mathcal{Z}^{0}\times \Pi \supset \Theta
_{\delta }^{\ast }$ for some non-empty open set $\mathcal{Z}^{0}\mathcal{%
\subset }R^{d_{\zeta }}$ and $\Pi $ as in (\ref{Defn of Par Sp Pi and Psi}).

\noindent (iii) $\Pi $ is compact.\medskip

\noindent Because the optimization parameter space is user selected,
Assumption B1 can be made to hold by the choice of $\Theta .$

The true parameter space $\Gamma $ satisfies:\medskip

\noindent \textbf{Assumption B2.} \noindent (i) $\Gamma $ is compact and (%
\ref{True Par Space Gamma}) holds.

\noindent (ii) $\forall \delta >0,$ $\exists \gamma =(\beta ,\zeta ,\pi
,\phi )\in \Gamma $ with $0<||\beta ||\,<\delta .$

\noindent (iii) $\forall \gamma =(\beta ,\zeta ,\pi ,\phi )\in \Gamma $ with 
$0<||\beta ||\,<\delta $ for some $\delta >0,$ $\gamma _{a}=(a\beta ,\zeta
,\pi ,\phi )\in \Gamma $ $\forall a\in \lbrack 0,1].$\medskip

\noindent Assumption B2(ii) guarantees that $\Gamma $ is not empty and that
there are elements $\gamma $ of $\Gamma $ whose $\beta $ values are non-zero
but are arbitrarily close to $0,$ which is the region of the true parameter
space where near lack of identification occurs. Assumption B2(iii) ensures
that $\Gamma $ is compatible with the existence of the partial derivatives
that arise in (\ref{exp fn2}) and Assumption GMM3.\medskip

\noindent \textbf{Example 1 (cont.). }Given the definitions in (\ref{gmm
theta space})-(\ref{gmm phi space}), the true parameter space\ $\Gamma $ is
of the form in (\ref{True Par Space Gamma}). Thus, Assumption B2(i) holds.
Assumption B2(ii) follows from the form of $\mathcal{B}^{\ast }$ given in (%
\ref{gmm theta space}). Assumption B2(iii) follows from the form of $%
\mathcal{B}^{\ast }$ and the fact that $\Theta ^{\ast }$ is a product space
and $\Phi ^{\ast }(\theta ^{\ast })$ does not depend on $\beta ^{\ast }.$
Hence, the true parameter space $\Gamma $ satisfies Assumption B2.

The optimization parameter space $\Theta $ takes the form%
\begin{equation}
\Theta =\mathcal{B}\times \mathcal{Z}\times \Pi ,\text{ where }\mathcal{B}%
=[-b_{1},b_{2}]\subset R,
\end{equation}%
$b_{1}>b_{1}^{\ast },$ $b_{2}>b_{2}^{\ast },$ $\mathcal{Z}\subset
R^{d_{\zeta }}$ is compact, $\Pi \subset R^{d_{\pi }}$ is compact, $\mathcal{%
Z}^{\ast }\subset int(\mathcal{Z}),$ and $\mathcal{B}^{\ast }\subset int(%
\mathcal{B}).$ Given these conditions, Assumptions B1(i) and B1(iii) follow
immediately. Assumption B1(ii) holds by taking $\delta <\min \{b_{1}^{\ast
},b_{2}^{\ast }\}$ and $\mathcal{Z}^{0}=int(\mathcal{Z}).$ $\square $

\section{ \hspace{-0.34in}\textbf{.}\hspace{0.2in}GMM Estimation Results 
\label{Estimation Results Sec}}

\setcounter{equation}{0}\hspace{0.25in}This section provides the asymptotic
results of the paper for the GMM estimator $\widehat{\theta }_{n}.$ Define a
concentrated GMM estimator $\widehat{\psi }_{n}(\pi )$ $(\in \Psi (\pi ))$
of $\psi $ for given $\pi \in \Pi $ by%
\begin{equation}
Q_{n}(\widehat{\psi }_{n}(\pi ),\pi )=\inf_{\psi \in \Psi (\pi )}Q_{n}(\psi
,\pi )+o(n^{-1}).  \label{Defn psi}
\end{equation}

Let $Q_{n}^{c}(\pi )$ denote the concentrated GMM criterion function $Q_{n}(%
\widehat{\psi }_{n}(\pi ),\pi ).$ Define an extremum estimator $\widehat{\pi 
}_{n}$ $(\in \Pi )$ by 
\begin{equation}
Q_{n}^{c}(\widehat{\pi }_{n})=\inf_{\pi \in \Pi }Q_{n}^{c}(\pi )+o(n^{-1}).
\label{Defn pihat}
\end{equation}

We assume that the GMM estimator $\widehat{\theta }_{n}$ in (\ref{Defn of
Thetahat}) can be written as $\widehat{\theta }_{n}=(\widehat{\psi }_{n}(%
\widehat{\pi }_{n}),\widehat{\pi }_{n}).$ Note that if (\ref{Defn psi}) and (%
\ref{Defn pihat}) hold and $\widehat{\theta }_{n}=(\widehat{\psi }_{n}(%
\widehat{\pi }_{n}),\widehat{\pi }_{n}),$ then (\ref{Defn of Thetahat})
automatically holds.

For $\gamma _{n}=(\beta _{n},\zeta _{n},\pi _{n},\phi _{n})\in \Gamma ,$ let 
$Q_{0,n}=Q_{n}(\psi _{0,n},\pi ),$ where $\psi _{0,n}=(0,\zeta _{n}).$ Note
that $Q_{0,n}$ does not depend on $\pi $ by Assumption GMM1(i).

Define the Gaussian process $\{\tau (\pi ;\gamma _{0},b):\pi \in \Pi \}$ by%
\begin{equation}
\tau (\pi ;\gamma _{0},b)=-H^{-1}(\pi ;\gamma _{0})(G(\pi ;\gamma
_{0})+K(\pi ;\gamma _{0})b)-(b,0_{d_{\zeta }}),  \label{Defn of Tau Process}
\end{equation}%
where $(b,0_{d_{\zeta }})\in R^{d_{\psi }}.$ Note that, by (\ref{Defn of Xi
Stoch Process}) and (\ref{Defn of Tau Process}), $\xi (\pi ;\gamma
_{0},b)=-(1/2)(\tau (\pi ;\gamma _{0},b)+(b,0_{d_{\zeta }}))^{\prime }H(\pi
;\gamma _{0})(\tau (\pi ;\gamma _{0},b)+(b,0_{d_{\zeta }})).$ Let%
\begin{equation}
\pi ^{\ast }(\gamma _{0},b)=\text{ }\underset{\pi \in \Pi }{\arg \min }\text{
}\xi (\pi ;\gamma _{0},b).  \label{PiStar Process}
\end{equation}

\begin{theorem}
\hspace{-0.08in}\textbf{. }\label{Thm dist'n of estimator b=finite}Suppose
Assumptions \emph{GMM1-GMM4,} \emph{B1, }and \emph{B2} hold. Under $\{\gamma
_{n}\}\in \Gamma (\gamma _{0},0,b)$ with $||b||<\infty ,$

\noindent $\emph{(a)}$ $\left( 
\begin{array}{c}
n^{1/2}(\widehat{\psi }_{n}-\psi _{n}) \\ 
\widehat{\pi }_{n}%
\end{array}%
\right) \overset{}{\rightarrow _{d}}\left( 
\begin{array}{c}
\tau (\pi ^{\ast }(\gamma _{0},b);\gamma _{0},b) \\ 
\pi ^{\ast }(\gamma _{0},b)%
\end{array}%
\right) ,$ and

\noindent $\emph{(b)}$ $n\left( Q_{n}(\widehat{\theta }_{n})-Q_{0,n}\right) 
\overset{}{\rightarrow _{d}}\inf_{\pi \in \Pi }\xi (\pi ;\gamma _{0},b).$
\end{theorem}

\noindent \textbf{Comments. 1. }The results of Theorem \ref{Thm dist'n of
estimator b=finite} and Theorem \ref{Thm dist'n of estimator b=inf} below
are the same as those in Theorems 3.1 and 3.2 of AC1, but they are obtained
under more primitive conditions, which are designed for GMM estimators.

\textbf{2. }Define the Gaussian process $\{\tau _{\beta }(\pi ;\gamma
_{0},b):\pi \in \Pi \}$ by 
\begin{equation}
\tau _{\beta }(\pi ;\gamma _{0},b)=S_{\beta }\tau (\pi ;\gamma _{0},b)+b,
\label{Asy Distn of Betahat(pi)}
\end{equation}%
where $S_{\beta }=[I_{d_{\beta }}:0_{d_{\beta }\times d_{\zeta }}]$ is the $%
d_{\beta }\times d_{\psi }$ selector matrix that selects $\beta $ out of $%
\psi .$ The asymptotic distribution of $n^{1/2}\widehat{\beta }_{n}$
(without centering at $\beta _{n}$) under $\Gamma (\gamma _{0},0,b)$ with $%
||b||<\infty $ is given by $\tau _{\beta }(\pi ^{\ast }(\gamma
_{0},b);\gamma _{0},b).$ This quantity appears in the asymptotic
distributions of the Wald and $t$ statistics below.

\textbf{3. }Assumption GMM4 is not needed for Theorem \ref{Thm dist'n of
estimator b=finite}(b).

\begin{theorem}
\hspace{-0.08in}\textbf{. }\label{Thm dist'n of estimator b=inf}Suppose
Assumptions \emph{GMM1-GMM5, B1, }and \emph{B2} hold. Under $\{\gamma
_{n}\}\in \Gamma (\gamma _{0},\infty ,\omega _{0}),$

\noindent \emph{(a)} $n^{1/2}B(\beta _{n})(\widehat{\theta }_{n}-\theta
_{n})\rightarrow _{d}-J^{-1}(\gamma _{0})G^{\ast }(\gamma _{0})\sim
N(0_{d_{\theta }},J^{-1}(\gamma _{0})V(\gamma _{0})J^{-1}(\gamma _{0})),$ and

\noindent \emph{(b)} $n(Q_{n}(\widehat{\theta }_{n})-Q_{n}(\theta
_{n}))\rightarrow _{d}-\frac{1}{2}G^{\ast }(\gamma _{0})^{\prime
}J^{-1}(\gamma _{0})G^{\ast }(\gamma _{0}).$
\end{theorem}

\noindent \textbf{Comment. }The results of Theorems \ref{Thm dist'n of
estimator b=finite} and \ref{Thm dist'n of estimator b=inf} hold for minimum
distance estimators under the assumptions listed in Supplemental Appendix B.

\section{ \hspace{-0.34in}\textbf{.}\hspace{0.2in}Wald Confidence Sets and
Tests\label{Wald Tests Sec}}

\setcounter{equation}{0}\hspace{0.25in}In this section, we consider a CS for
a function $r(\theta )$ of $\theta $ by inverting a Wald test of the
hypotheses $H_{0}:r(\theta )=v$ for $v\in r(\Theta ).$ We also consider Wald
tests of $H_{0}.$ We establish the asymptotic distributions of the Wald
statistic under drifting sequences of null and alternative distributions
that cover the entire range of strengths of identification. We determine the
asymptotic size of standard Wald CS's. We introduce robust Wald CS's whose
asymptotic size is guaranteed to equal their nominal size. The results in
this section apply not just to Wald statistics based on GMM estimators, but
to Wald tests based on any of the estimators considered in AC1 and AC2 as
well.

\subsection{\hspace{-0.23in}\textbf{.}\hspace{0.18in}Wald Statistics}

\hspace{0.25in}The Wald statistics are defined as follows. Let%
\begin{equation}
\Sigma (\gamma _{0})=J^{-1}\left( \gamma _{0}\right) ^{\prime }V(\gamma
_{0})J^{-1}(\gamma _{0})\text{ and }\widehat{\Sigma }_{n}=\widehat{J}%
_{n}^{-1}\widehat{V}_{n}\widehat{J}_{n}^{-1},  \label{Variance Matrix Defns}
\end{equation}%
where $\widehat{J}_{n}$ and $\widehat{V}_{n}$ are estimators of $J(\gamma
_{0})$ and $V(\gamma _{0}).$ The Wald statistic takes the form%
\begin{equation}
W_{n}(v)=n(r(\widehat{\theta }_{n})-v)^{\prime }(r_{\theta }(\widehat{\theta 
}_{n})B^{-1}(\widehat{\beta }_{n})\widehat{\Sigma }_{n}B^{-1}(\widehat{\beta 
}_{n})r_{\theta }(\widehat{\theta }_{n})^{\prime })^{-1}(r(\widehat{\theta }%
_{n})-v),  \label{Defn Wald}
\end{equation}%
where $r_{\theta }(\theta )=(\partial /\partial \theta ^{\prime })r(\theta
)\in R^{d_{r}\times d_{\theta }}.$

When $d_{r}=1,$ the $t$ statistic takes the form%
\begin{equation}
T_{n}(v)=\frac{n^{1/2}(r(\widehat{\theta }_{n})-v)}{(r_{\theta }(\widehat{%
\theta }_{n})B^{-1}(\widehat{\beta }_{n})\widehat{\Sigma }_{n}B^{-1}(%
\widehat{\beta }_{n})r_{\theta }(\widehat{\theta }_{n})^{\prime })^{1/2}}.
\label{Defn T}
\end{equation}

\noindent Although these definitions of the Wald and $t$ statistics involve $%
B^{-1}(\widehat{\beta }_{n}),$ they are the same as the standard definitions
used in practice. By Theorem \ref{Thm dist'n of estimator b=inf}(a), when $%
\beta _{0}\neq 0,$ $B^{-1}(\beta _{0})\Sigma (\gamma _{0})B^{-1}(\beta _{0})$
is the asymptotic covariance matrix of $\widehat{\theta }_{n}.$ In the Wald
statistics, the asymptotic covariance is replaced by the estimator $B^{-1}(%
\widehat{\beta }_{n})\widehat{\Sigma }_{n}B^{-1}(\widehat{\beta }_{n}).$ The
same form of the Wald statistics is used under all sequences of true
parameters $\gamma _{n}\in \Gamma (\gamma _{0}).$

In the results below (except in Section \ref{Wald under Alt Subsec}), we
consider the behavior of the Wald statistics when the null hypothesis holds.
Thus, under a sequence $\{\gamma _{n}\},$ we consider the sequence of null
hypotheses $H_{0}:r(\theta )=v_{n},$ where $v_{n}$ equals $r(\theta _{n})$
and $\gamma _{n}=(\theta _{n},\phi _{n}).$ We employ the following
notational simplification:%
\begin{equation}
W_{n}=W_{n}(v_{n}),\text{ where }v_{n}=r(\theta _{n}).
\label{Null Defn of Wald and t Stats}
\end{equation}

\subsection{\hspace{-0.23in}\textbf{.}\hspace{0.18in}Rotation}

\hspace{0.25in}To obtain the asymptotic distribution of the Wald statistic
we consider a rotation of $r(\widehat{\theta }_{n})$ and $r_{\theta }(%
\widehat{\theta }_{n})$ by a matrix $A(\widehat{\theta }_{n}).$ The rotation
is designed to separate the effects of the randomness in $\widehat{\psi }%
_{n} $ and $\widehat{\pi }_{n},$ which have different rates of convergence
for some sequences $\{\gamma _{n}\}.$ Similar rotations are carried out in
the analysis of partially-identified models in Sargan (1983) and Phillips
(1989), in the nonstationary time series literature, e.g., see Park and
Phillips (1988), and in the GMM analysis in Antoine and Renault (2009, 2010).

We partition $r_{\theta }(\theta )$ conformably with $\theta =(\psi ,\pi )$:%
\begin{equation}
r_{\theta }(\theta )=[r_{\psi }(\theta ):r_{\pi }(\theta )].
\end{equation}%
Suppose $rank(r_{\pi }(\theta ))=d_{\pi }^{\ast }$ $(\leq \min (d_{r},d_{\pi
}))$ $\forall \theta \in \Theta _{\delta }$ for some $\delta >0.$ (This is
Assumption R1(iii) below). For $\theta \in \Theta _{\delta },$ let $A(\theta
)=[A_{1}(\theta )^{\prime }:A_{2}(\theta )^{\prime }]^{\prime }\in O(d_{r}),$
where the rows of $A_{1}(\theta )\in R^{(d_{r}-d_{\pi }^{\ast })\times
d_{r}} $ span the null space of $r_{\pi }(\theta )^{\prime },$ the rows of $%
A_{2}(\theta )\in R^{d_{\pi }^{\ast }\times d_{r}}$ span the column space of 
$r_{\pi }(\theta ),$ and $O(d_{r})$ stands for the orthogonal group of
degree $d_{r}$ over the real space. Hence,%
\begin{equation}
A(\theta )r_{\pi }(\theta )=\left[ 
\begin{array}{c}
A_{1}(\theta )r_{\pi }(\theta ) \\ 
A_{2}(\theta )r_{\pi }(\theta )%
\end{array}%
\right] =\left[ 
\begin{array}{c}
0_{(d_{r-}d_{\pi }^{\ast })\times d_{\pi }} \\ 
r_{\pi }^{\ast }(\theta )%
\end{array}%
\right] ,
\end{equation}%
where $r_{\pi }^{\ast }(\theta )\in R^{d_{\pi }^{\ast }\times d_{\pi }}$ has
full row rank $d_{\pi }^{\ast }.$ For simplicity, hereafter we write the $0$
matrix as $0$ when there is no confusion about its dimension.

With the $A(\theta )$ rotation, the derivative matrix $r_{\theta }(\theta )$
becomes 
\begin{equation}
r_{\theta }^{A}(\theta )=A(\theta )r_{\theta }(\theta )=\left[ 
\begin{array}{cc}
r_{\psi }^{\ast }(\theta ) & 0 \\ 
r_{\psi }^{0}(\theta ) & r_{\pi }^{\ast }(\theta )%
\end{array}%
\right] ,  \label{r_A matrix}
\end{equation}%
where the $(d_{r}-d_{\pi }^{\ast })\times d_{\psi }$ matrix $r_{\psi }^{\ast
}(\theta )$ has full row rank $d_{r}-d_{\pi }^{\ast }.$ When $d_{\pi }^{\ast
}=d_{r},$ $A_{1}(\theta )$ and $[r_{\psi }^{\ast }(\theta ):0]$ disappear.
When $d_{\pi }^{\ast }=0,$ $A_{2}(\theta )$ and $[r_{\psi }^{0}(\theta
):r_{\pi }^{\ast }(\theta )]$ disappear.

The effect of randomness in $\widehat{\pi }_{n}$ on $r(\widehat{\theta }%
_{n}) $ is concentrated in the full rank matrix $r_{\pi }^{\ast }(\widehat{%
\theta }_{n})$ because the upper right corner of $r_{\theta }^{A}(\widehat{%
\theta }_{n})$ is $0.$ The effect of randomness in $\widehat{\psi }_{n}$ on $%
r(\widehat{\theta }_{n})$ is incorporated in both $r_{\psi }^{\ast }(%
\widehat{\theta }_{n})$ and $r_{\psi }^{0}(\widehat{\theta }_{n}).$

Using the rotation by $A(\widehat{\theta }_{n}),$ the Wald statistic in (\ref%
{Defn Wald}) can be written as%
\begin{equation}
W_{n}=n(r(\widehat{\theta }_{n}\hspace{-0.01in})-v)^{\prime }\hspace{-0.02in}%
A(\widehat{\theta }_{n}\hspace{-0.01in})^{\prime }(r_{\theta }^{A}(\widehat{%
\theta }_{n}\hspace{-0.01in})\hspace{-0.01in}B^{-1\hspace{-0.01in}}(\widehat{%
\beta }_{n})\widehat{\Sigma }_{n}B^{-1}\hspace{-0.01in}(\widehat{\beta }%
_{n})r_{\theta }^{A}(\widehat{\theta }_{n}\hspace{-0.01in})^{\prime })^{-1}%
\hspace{-0.02in}A(\widehat{\theta }_{n}\hspace{-0.01in})(r(\widehat{\theta }%
_{n}\hspace{-0.01in})-v),
\end{equation}%
where the first $d_{r}-d_{\pi }^{\ast }$ rows of $A(\widehat{\theta }_{n})r(%
\widehat{\theta }_{n})$ only depend on the randomness in $\widehat{\psi }%
_{n},$ not $\widehat{\pi }_{n},$ asymptotically by the choice of $A(\widehat{%
\theta }_{n}).$

Define a $d_{r}\times d_{\theta }$ matrix 
\begin{equation}
r_{\theta }^{\ast }(\theta )=\left[ 
\begin{array}{cc}
r_{\psi }^{\ast }(\theta ) & 0 \\ 
0 & r_{\pi }^{\ast }(\theta )%
\end{array}%
\right] .
\end{equation}

The matrix $r_{\theta }^{\ast }(\theta ),$ rather than $r_{\theta
}^{A}(\theta ),$ appears in the asymptotic distribution below. The reason is
as follows. Because $\widehat{\psi }_{n}$ converges faster than $\widehat{%
\pi }_{n}$ under $\{\gamma _{n}\}\in \Gamma (\gamma _{0},0,b),$ as shown in
Theorems \ref{Thm dist'n of estimator b=finite} and \ref{Thm dist'n of
estimator b=inf}, the effect of randomness in $\widehat{\pi }_{n}$ is an
order of magnitude larger than that in $\widehat{\psi }_{n}.$ As a result,
the limit of $r_{\psi }^{0}(\widehat{\theta }_{n})$ in (\ref{r_A matrix})
does not show up in the asymptotic distributions of the Wald and $t$
statistics. On the other hand, the limit of $r_{\psi }^{\ast }(\widehat{%
\theta }_{n})$ does appear in the asymptotic distribution because it is the
effect of randomness in $\widehat{\psi }_{n}$ separated from that in $%
\widehat{\pi }_{n}.$

When $r_{\pi }(\theta )$ has full row rank, i.e., $d_{\pi }^{\ast }=d_{r},$
for all $\theta \in \Theta _{\delta },$ we have $A(\theta )=I_{d_{r}},$ $%
r_{\theta }^{A}(\theta )=r_{\theta }(\theta ),$ and $r_{\theta }^{\ast
}(\theta )=[0:r_{\pi }(\theta )].$ In this case, rotation is not needed to
concentrate the randomness in $\widehat{\pi }_{n}.$ Also, when $d_{r}=1,$ we
have $A(\theta )=1,$ so no rotation is employed.

Define 
\begin{equation}
\eta _{n}(\theta )=\left\{ 
\begin{array}{cc}
n^{1/2}A_{1}(\theta )(r(\psi _{n},\pi )-r(\psi _{n},\pi _{n})) & \text{if }%
d_{\pi }^{\ast }<d_{r} \\ 
0 & \text{if }d_{\pi }^{\ast }=d_{r}.%
\end{array}%
\right.  \label{Defn eta}
\end{equation}

\subsection{\hspace{-0.23in}\textbf{.}\hspace{0.18in}Function $\mathbf{r(%
\protect\theta )}$ of Interest}

\hspace{0.25in}The function of interest, $r(\theta ),$ satisfies the
following assumptions.$\medskip $

\noindent \textbf{Assumption R1. }(i) $r(\theta )$ is continuously
differentiable on $\Theta .$

\noindent (ii) $r_{\theta }(\theta )$ is full row rank $d_{r}$ $\forall
\theta \in \Theta .$

\noindent (iii) $rank(r_{\pi }(\theta ))=d_{\pi }^{\ast }$ for some constant 
$d_{\pi }^{\ast }\leq \min (d_{r},d_{\pi })$ $\forall \theta \in \Theta
_{\delta }=\{\theta \in \Theta :||\beta ||<\delta \}$ for some $\delta
>0.\medskip $

\noindent \textbf{Assumption R2. }$\eta _{n}(\widehat{\theta }%
_{n})\rightarrow _{p}0$ under $\{\gamma _{n}\}\in \Gamma (\gamma _{0},0,b)$ $%
\forall b\in (R\cup \left\{ \pm \infty \right\} )^{d_{\beta }}.\medskip $

Three different sufficient conditions for the high-level Assumption R2 are
given by Assumptions R2$^{\ast }$(i)-(iii) below. Any one of them is
sufficient for Assumption R2 (under the conditions in Lemma \ref{Lemma
Sufficient R2} below).$\medskip $

\noindent \textbf{Assumption R2}$^{\ast }$\textbf{.} (i) $d_{\pi }^{\ast
}=d_{r}.$

\noindent (ii) $d_{r}=1.$

\noindent (iii) The column space of $r_{\pi }(\theta )$ is the same $\forall
\theta \in \Theta _{\delta }$ for some $\delta >0.\medskip $

Assumption R2$^{\ast }$(i) requires that the restrictions only involve $\pi
. $ Alternatively, Assumption R2$^{\ast }$(ii) requires that only one
restriction appears. Alternatively, R2$^{\ast }$(iii) is satisfied when $%
r_{\pi }(\theta )=a(\theta )R_{\pi },$ where $a(\theta ):\Theta _{\delta
}\rightarrow R,$ $a(\theta )\neq 0,$ and $R_{\pi }\in R^{d_{r}\times d_{\pi
}}.$ A special case is when $r_{\pi }(\theta )$ is constant due to the
restrictions being linear.$\medskip $

\noindent \textbf{Assumption }R$_{\text{\textbf{L}}}$\textbf{. }$r(\theta
)=R\theta ,$ where $R\in R^{d_{r}\times d_{\theta }}$ has full row rank $%
d_{r}.\medskip $

Assumption R$_{\text{L}}$ is a sufficient condition for Assumptions R1 and
R2.$\medskip $

\begin{lemma}
\hspace{-0.08in}\textbf{.} \label{Lemma Sufficient R2}Assumptions \emph{R2}$%
^{\ast }$\emph{(i)} and \emph{R2}$^{\ast }$\emph{(ii)} each \emph{(}%
separately\emph{)} implies Assumption \emph{R2}. Assumption \emph{R2}$^{\ast
}$\emph{(iii)} combined with Assumption \emph{GMM1 (}or Assumptions \emph{A}
and \emph{B3(i)-(ii)} of \emph{AC1) }implies Assumption \emph{R2.}
\end{lemma}

\begin{lemma}
\hspace{-0.08in}\textbf{.} \label{Lemma Sufficient Linear}Assumption \emph{R}%
$_{\text{\emph{L}}}$ implies Assumptions \emph{R1} and \emph{R2.}
\end{lemma}

\subsection{\hspace{-0.23in}\textbf{.}\hspace{0.18in}Variance Matrix
Estimators\label{Var Matrix Subsec}}

\hspace{0.25in}The estimators of the components of the asymptotic variance
matrix are assumed to satisfy the following assumptions. Two forms are given
for Assumption V1 that follows. The first applies when $\beta $ is a scalar
and the second applies when $\beta $ is a vector. The reason for the
difference is that the normalizing matrix $B(\beta )$ is different in these
two cases.

When $\beta $ is a scalar, let $J(\theta ;\gamma _{0})$ and $V(\theta
;\gamma _{0})$ for $\theta \in \Theta $ be some non-stochastic $d_{\theta
}\times d_{\theta }$ matrix-valued functions such that $J(\theta _{0};\gamma
_{0})=J(\gamma _{0})$ and $V(\theta _{0};\gamma _{0})=V(\gamma _{0}),$ where 
$J(\gamma _{0})$ and $V(\gamma _{0})$ are as in (\ref{J and V Matrices}) (or
as in Assumptions D2 and D3 of AC1). Let 
\begin{equation}
\Sigma (\theta ;\gamma _{0})=J^{-1}(\theta ;\gamma _{0})V(\theta ;\gamma
_{0})J^{-1}(\theta ;\gamma _{0})\text{ and }\Sigma (\pi ;\gamma _{0})=\Sigma
(\psi _{0},\pi ;\gamma _{0}).  \label{Sigma defn scalar beta}
\end{equation}%
Let $\Sigma _{\beta \beta }(\pi ;\gamma _{0})$ denote the upper left (1,1)
element of $\Sigma (\pi ;\gamma _{0}).$

Assumption V1 below applies when $\beta $ is a scalar.\medskip

\noindent \textbf{Assumption V1 (scalar }$\mathbf{\beta }$\textbf{). }(i) $%
\widehat{J}_{n}=\widehat{J}_{n}(\widehat{\theta }_{n})$ and $\widehat{V}_{n}=%
\widehat{V}_{n}(\widehat{\theta }_{n})$ for some (stochastic) $d_{\theta
}\times d_{\theta }$ matrix-valued functions $\widehat{J}_{n}(\theta )$ and $%
\widehat{V}_{n}(\theta )$ on $\Theta $ that satisfy $\sup_{\theta \in \Theta
}||\widehat{J}_{n}(\theta )-J(\theta ;\gamma _{0})||\rightarrow _{p}0$ and $%
\sup_{\theta \in \Theta }||\widehat{V}_{n}(\theta )-V(\theta ;\gamma
_{0})||\rightarrow _{p}0$ under $\{\gamma _{n}\}\in \Gamma (\gamma _{0},0,b)$
with $|b|<\infty .$

\noindent (ii) $J(\theta ;\gamma _{0})$ and $V(\theta ;\gamma _{0})$ are
continuous in $\theta $ on $\Theta $ $\forall \gamma _{0}\in \Gamma $ with $%
\beta _{0}=0.$

\noindent (iii) $\lambda _{\min }(\Sigma (\pi ;\gamma _{0}))>0$ and $\lambda
_{\max }(\Sigma (\pi ;\gamma _{0}))<\infty $ $\forall \pi \in \Pi ,$ $%
\forall \gamma _{0}\in \Gamma $ with $\beta _{0}=0.$\medskip

When $\beta $ is a vector, i.e., $d_{\beta }>1,$ we reparameterize $\beta $
as $(||\beta ||,\omega ),$ where $\omega =\beta /||\beta ||$ if $\beta \neq
0 $ and by definition $\omega =1_{d_{\beta }}/||1_{d_{\beta }}||$ with $%
1_{d_{\beta }}=(1,...,1)\in R^{d_{\beta }}$ if $\beta =0.$ Correspondingly, $%
\theta $ is reparameterized as $\theta ^{+}=(||\beta ||,\omega ,\zeta ,\pi
). $ Let $\Theta ^{+}=\{\theta ^{+}:$ $\theta ^{+}=(||\beta ||,\beta
/||\beta ||,\zeta ,\pi ),$ $\theta \in \Theta \}.$ Let $\widehat{\theta }%
_{n}^{+}$ and $\theta _{0}^{+}$ be the counterparts of $\widehat{\theta }%
_{n} $ and $\theta _{0}$ after reparametrization.

When $\beta $ is a vector, let $J(\theta ^{+};\gamma _{0})$ and $V(\theta
^{+};\gamma _{0})$ denote some non-stochastic $d_{\theta }\times d_{\theta }$
matrix-valued functions such that $J(\theta _{0}^{+};\gamma _{0})=J(\gamma
_{0})$ and $V(\theta _{0}^{+};\gamma _{0})=V(\gamma _{0}).$ Let 
\begin{eqnarray}
\Sigma (\theta ^{+};\gamma _{0})\hspace{-0.08in} &=&\hspace{-0.08in}%
J^{-1}(\theta ^{+};\gamma _{0})V(\theta ^{+};\gamma _{0})J^{-1}(\theta
^{+};\gamma _{0})\text{ and}  \notag \\
\Sigma (\pi ,\omega ;\gamma _{0})\hspace{-0.08in} &=&\hspace{-0.08in}\Sigma
(||\beta _{0}||,\omega ,\zeta _{0},\pi ;\gamma _{0}).
\label{Sigma defn vector beta}
\end{eqnarray}%
Let $\Sigma _{\beta \beta }(\pi ,\omega ;\gamma _{0})$ denote the upper left 
$d_{\beta }\times d_{\beta }$ sub-matrix of $\Sigma (\pi ,\omega ;\gamma
_{0}).$

Assumption V1 below applies when $\beta $ is a vector.\medskip

\noindent \textbf{Assumption V1 (vector }$\mathbf{\beta }$)\textbf{. }(i) $%
\widehat{J}_{n}=\widehat{J}_{n}(\widehat{\theta }_{n}^{+})$ and $\widehat{V}%
_{n}=\widehat{V}_{n}(\widehat{\theta }_{n}^{+})$ for some (stochastic) $%
d_{\theta }\times d_{\theta }$ matrix-valued functions $\widehat{J}%
_{n}(\theta ^{+})$ and $\widehat{V}_{n}(\theta ^{+})$ on $\Theta ^{+}$ that
satisfy $\sup_{\theta ^{+}\in \Theta ^{+}}||\widehat{J}_{n}(\theta
^{+})-J(\theta ^{+};\gamma _{0})||\rightarrow _{p}0$ and $\sup_{\theta
^{+}\in \Theta ^{+}}||\widehat{V}_{n}(\theta ^{+})-V(\theta ^{+};\gamma
_{0})||\rightarrow _{p}0$ under $\{\gamma _{n}\}\in \Gamma (\gamma _{0},0,b)$
with $||b||<\infty .\footnote{%
The functions $J(\theta ^{+};\gamma _{0})$ and $V(\theta ^{+};\gamma _{0})$
do not depend on $\omega _{0},$ only $\gamma _{0}.$}$

\noindent (ii) $J(\theta ^{+};\gamma _{0})$ and $V(\theta ^{+};\gamma _{0})$
are continuous in $\theta ^{+}$ on $\Theta ^{+}$ $\forall \gamma _{0}\in
\Gamma $ with $\beta _{0}=0.$

\noindent (iii) $\lambda _{\min }(\Sigma (\pi ,\omega ;\gamma _{0}))>0$ and $%
\lambda _{\max }(\Sigma (\pi ,\omega ;\gamma _{0}))<\infty $ $\forall \pi
\in \Pi ,$ $\forall \omega \in R^{d_{\beta }}$ with $||\omega ||=1,$ $%
\forall \gamma _{0}\in \Gamma $ with $\beta _{0}=0.$

\noindent (iv) $P(\tau _{\beta }(\pi ^{\ast }(\gamma _{0},b),\gamma
_{0},b)=0)=0$ $\forall \gamma _{0}\in \Gamma $ with $\beta _{0}=0$ and $%
\forall b$ with $||b||<\infty .$\medskip

The following assumption applies with both scalar and vector $\beta .$%
\medskip

\noindent \textbf{Assumption V2. }Under $\Gamma (0,\infty ,\omega _{0}),$ $%
\widehat{J}_{n}\rightarrow _{p}J(\gamma _{0})$ and $\widehat{V}%
_{n}\rightarrow _{p}V(\gamma _{0}).$\medskip

\noindent \textbf{Example 1 (cont.). }In this example, $\beta $ is a scalar.
The estimators of $J(\gamma _{0})$ and $V(\gamma _{0})$ are%
\begin{equation}
\widehat{J}_{n}=\widehat{J}_{n}(\widehat{\theta }_{n})\text{ and }\widehat{V}%
_{n}=\widehat{V}_{n}(\widehat{\theta }_{n}),
\end{equation}%
respectively, where%
\begin{eqnarray}
\widehat{J}_{n}(\theta )\hspace{-0.08in} &=&\hspace{-0.08in}\widehat{J}%
_{g,n}(\theta )^{\prime }\mathcal{W}_{n}\widehat{J}_{g,n}\left( \theta
\right) ,  \notag \\
\widehat{V}_{n}(\theta )\hspace{-0.08in} &=&\hspace{-0.08in}\widehat{J}%
_{g,n}(\theta )^{\prime }\mathcal{W}_{n}\widehat{V}_{g,n}\left( \theta
\right) \mathcal{W}_{n}\widehat{J}_{g,n}\left( \theta \right) ,  \notag \\
\widehat{J}_{g,n}(\theta )^{\prime }\hspace{-0.08in} &=&\hspace{-0.08in}%
n^{-1}\sum_{i=1}^{n}Z_{i}d_{i}(\pi )^{\prime },\text{ and }\widehat{V}%
_{g,n}\left( \theta \right) =n^{-1}\sum_{i=1}^{n}U_{i}^{2}(\theta
)Z_{i}Z_{i}^{\prime }.
\end{eqnarray}%
The key quantities in Assumption V1 (scalar $\beta $) are 
\begin{eqnarray}
&&J(\theta ;\gamma _{0})\overset{}{=}J_{g}(\theta ;\gamma _{0})^{\prime }%
\mathcal{W}(\gamma _{0})J_{g}(\theta ;\gamma _{0})\text{ and}  \notag \\
&&V(\theta ;\gamma _{0})\overset{}{=}J_{g}(\theta ;\gamma _{0})^{\prime }%
\mathcal{W}(\gamma _{0})V_{g}(\theta ;\gamma _{0})\mathcal{W}(\gamma
_{0})J_{g}(\theta ;\gamma _{0}),\text{ where}  \notag \\
&&J_{g}(\theta ;\gamma _{0})\overset{}{=}-E_{\phi _{0}}Z_{i}d_{i}(\pi
)^{\prime },\text{ }\mathcal{W}(\gamma _{0})\overset{}{=}(E_{\phi
_{0}}Z_{i}Z_{i}^{\prime })^{-1},\text{ and}  \label{gmm J and V} \\
&&V_{g}(\theta ;\gamma _{0})\overset{}{=}E_{\phi
_{0}}U_{i}^{2}Z_{i}Z_{i}^{\prime }+2E_{\phi _{0}}[\beta _{0}h(X_{1,i},\pi
_{0})-\beta h(X_{1,i},\pi )+X_{2,i}(\zeta _{0}-\zeta )]Z_{i}Z_{i}^{\prime } 
\notag \\
&&\hspace{0.78in}+E_{\phi _{0}}[\beta _{0}h(X_{1,i},\pi _{0})-\beta
h(X_{1,i},\pi )+X_{2,i}^{\prime }(\zeta _{0}-\zeta )]^{2}Z_{i}Z_{i}^{\prime
}.  \notag
\end{eqnarray}

Assumption V1(i) holds by the uniform LLN given in Lemma \ref{Lemma uniform
convergence} in Supplemental Appendix D using the moment conditions in (\ref%
{gmm phi space}), Assumption GMM1(ii), and the continuous mapping theorem.
Assumption V1(ii) holds by the continuity of $h(x,\pi )$ and $h_{\pi }(x,\pi
)$ in $\pi $ and the conditions in (\ref{gmm phi space}).

To verify Assumption V1(iii), note that%
\begin{eqnarray}
\Sigma (\pi ;\gamma _{0})\hspace{-0.08in} &=&\hspace{-0.08in}J^{-1}(\psi
_{0},\pi ;\gamma _{0})V(\psi _{0},\pi ;\gamma _{0})J^{-1}(\psi _{0},\pi
;\gamma _{0}),\text{ where}  \notag \\
J_{g}(\psi _{0},\pi ;\gamma _{0})\hspace{-0.08in} &=&\hspace{-0.08in}%
-E_{\phi _{0}}Z_{i}d_{i}(\pi )^{\prime }\text{ and }V_{g}(\psi _{0},\pi
;\gamma _{0})=E_{\phi _{0}}U_{i}^{2}Z_{i}Z_{i}^{\prime }
\end{eqnarray}%
when $\beta _{0}=0.$ We have: $\Sigma (\pi ;\gamma _{0})$ is positive
definite (pd) and finite $\forall \pi \in \Pi $ because both $J(\psi
_{0},\pi ;\gamma _{0})$ and $V(\psi _{0},\pi ;\gamma _{0})$ are pd and
finite, which in turn holds because (i) $\mathcal{W}(\gamma _{0})$ is pd and
finite by Assumption GMM1(vii), (ii) $J_{g}(\psi _{0},\pi ;\gamma _{0})\in
R^{k\times d_{\theta }}$ has full rank by (\ref{gmm phi space}), and (iii) $%
V_{g}(\psi _{0},\pi ;\gamma _{0})$ is pd and finite by (\ref{gmm phi space}%
). This completes the verification of Assumption V1.

Assumptions V1(i) and V1(ii) hold not only under $\{\gamma _{n}\}\in \Gamma
(\gamma _{0},0,b),$ but also under $\{\gamma _{n}\}\in \Gamma (\gamma
_{0},\infty ,\omega _{0})$ in this example. This and $\widehat{\theta }%
_{n}\rightarrow _{p}\theta _{0}$ under $\{\gamma _{n}\}\in \Gamma (\gamma
_{0},\infty ,\omega _{0}),$ which holds by Theorem \ref{Thm dist'n of
estimator b=inf} (because Assumptions GMM1-GMM5, B1, and B2 have been
verified above), imply that Assumption V2 holds. This completes the
verification of Assumption V2. $\square $

\subsection{\hspace{-0.23in}\textbf{.}\hspace{0.18in}Asymptotic Null
Distribution of the Wald Statistic\label{Subsec Asy Distn Wald Stat}}

\hspace{0.25in}The asymptotic null distribution of the Wald statistic under $%
H_{0}$ depends on the following quantities. The limit distribution of $%
\widehat{\omega }_{n}(\pi )=\widehat{\beta }_{n}(\pi )/||\widehat{\beta }%
_{n}(\pi )||$ under $\Gamma (\gamma _{0},0,b)$ with $||b||<\infty $ is given
by%
\begin{equation}
\omega ^{\ast }(\pi ;\gamma _{0},b)=\frac{\tau _{\beta }(\pi ;\gamma _{0},b)%
}{||\tau _{\beta }(\pi ;\gamma _{0},b)||}\text{ for }\pi \in \Pi ,
\label{Omegastar defn}
\end{equation}%
where $\tau _{\beta }(\pi ;\gamma _{0},b)$ is defined in (\ref{Asy Distn of
Betahat(pi)}). Let $\overline{B}(\pi ;\gamma _{0},b)$ be a $d_{r}\times
d_{r} $ matrix-valued function of $\tau _{\beta }(\pi ;\gamma _{0},b)$
defined as%
\begin{equation}
\overline{B}(\pi ;\gamma _{0},b)=\left[ 
\begin{array}{cc}
I_{(d_{r}-d_{\pi }^{\ast })} & 0 \\ 
0 & \iota (\tau _{\beta }(\pi ;\gamma _{0},b))I_{d_{\pi }^{\ast }}%
\end{array}%
\right]
\end{equation}%
where $\iota (\beta )=\beta $ when $\beta $ is a scalar and $\iota (\beta
)=||\beta ||$ when $\beta $ is a vector.

Let%
\begin{eqnarray}
r_{\theta }^{\ast }(\pi )\hspace{-0.08in} &=&\hspace{-0.08in}r_{\theta
}^{\ast }(\psi _{0},\pi ),\text{ }r_{\psi }^{\ast }(\pi )=r_{\psi }^{\ast
}(\psi _{0},\pi )\text{ and}  \notag \\
\overline{\Sigma }(\pi ;\gamma _{0},b)\hspace{-0.08in} &=&\hspace{-0.08in}%
\left\{ 
\begin{array}{ll}
\Sigma (\pi ;\gamma _{0}) & \text{if }\beta \text{ is a scalar} \\ 
\Sigma (\pi ,\omega ^{\ast }(\pi ;\gamma _{0},b);\gamma _{0}) & \text{if }%
\beta \text{ is a vector,}%
\end{array}%
\right.  \label{Sigmabar defn}
\end{eqnarray}%
where $\Sigma (\pi ;\gamma _{0})$ and $\Sigma (\pi ,\omega ;\gamma _{0})$
are defined in (\ref{Sigma defn scalar beta}) and (\ref{Sigma defn vector
beta}), respectively.

Define a stochastic process $\{\lambda (\pi ;\gamma _{0},b):\pi \in \Pi \}$
by%
\begin{eqnarray}
&&\hspace{-0.08in}\lambda (\pi ;\gamma _{0},b)\hspace{-0.08in}  \notag \\
&=&\hspace{-0.08in}\tau ^{A}(\pi ;\gamma _{0},b)^{\prime }\overline{B}(\pi
;\gamma _{0},b)(r_{\theta }^{\ast }(\pi )\overline{\Sigma }(\pi ;\gamma
_{0},b)r_{\theta }^{\ast }(\pi )^{\prime })^{-1}\overline{B}(\pi ;\gamma
_{0},b)\tau ^{A}(\pi ;\gamma _{0},b),\text{ where}  \notag \\
&&\hspace{-0.25in}\tau ^{A}(\pi ;\gamma _{0},b)\overset{}{=}\left( 
\begin{array}{c}
r_{\psi }^{\ast }(\pi )\tau (\pi ;\gamma _{0},b) \\ 
A_{2}(\psi _{0},\pi )(r(\psi _{0},\pi )-r(\psi _{0},\pi _{0}))%
\end{array}%
\right) \overset{}{\in }R^{d_{r}}.
\end{eqnarray}

With linear restrictions, the stochastic process $\lambda (\pi ;\gamma
_{0},b)$ can be simplified. Under Assumption R$_{\text{L}}$, $r_{\theta
}(\theta )=R$ does not depend on $\theta ,$ and, hence, $A(\theta )$ and $%
r_{\theta }^{\ast }(\theta )$ do not depend on $\theta .$ Define $R^{\ast
}=r_{\theta }^{\ast }(\theta )$ under Assumption R$_{\text{L}}$.
Specifically, 
\begin{equation}
R^{A}=AR=\left[ 
\begin{array}{cc}
R_{\psi }^{\ast } & 0 \\ 
R_{\psi }^{0} & R_{\pi }^{\ast }%
\end{array}%
\right] \text{ and }R^{\ast }=\left[ 
\begin{array}{cc}
R_{\psi }^{\ast } & 0 \\ 
0 & R_{\pi }^{\ast }%
\end{array}%
\right] ,
\end{equation}%
where $R_{\psi }^{\ast }\in R^{(d_{r}-d_{\pi }^{\ast })\times d_{\psi }}$
and $R_{\pi }^{\ast }\in R^{d_{\pi }^{\ast }\times d_{\pi }}.$

Define a stochastic process $\{\lambda _{L}(\pi ;\gamma _{0},b):\pi \in \Pi
\}$ by%
\begin{eqnarray}
&&\hspace{-0.08in}\lambda _{L}(\pi ;\gamma _{0},b)  \notag \\
&=&\hspace{-0.08in}\overline{\tau }(\pi ;\gamma _{0},b)^{\prime }R^{\ast
\prime }\overline{B}(\pi ;\gamma _{0},b)(R^{\ast }\overline{\Sigma }(\pi
;\gamma _{0},b)R^{\ast \prime })^{-1}\overline{B}(\pi ;\gamma _{0},b)R^{\ast
}\overline{\tau }(\pi ;\gamma _{0},b),\text{ where}  \notag \\
&&\hspace{-0.25in}\overline{\tau }(\pi ;\gamma _{0},b)\overset{}{=}(\tau
(\pi ;\gamma _{0},b)^{\prime },(\pi -\pi _{0})^{\prime })^{\prime }\overset{}%
{\in }R^{d_{\theta }}.  \label{Omega Process}
\end{eqnarray}%
Under the linear restriction of Assumption R$_{\text{L}}$, $\lambda _{L}(\pi
;\gamma _{0},b)=\lambda (\pi ;\gamma _{0},b)$ and the asymptotic
distribution of the Wald statistic can be simplified by replacing the
stochastic process $\{\lambda (\pi ;\gamma _{0},b):\pi \in \Pi \}$ with $%
\{\lambda _{L}(\pi ;\gamma _{0},b):\pi \in \Pi \}$ in the asymptotic results
below.

The following theorem establishes the asymptotic null distribution of the
Wald statistic for nonlinear restrictions that satisfy Assumption R2. (The
null holds by the definition $W_{n}=W_{n}(v_{n})$ in (\ref{Null Defn of Wald
and t Stats}).)

\begin{theorem}
\hspace{-0.08in}\textbf{.} \label{Theorem Wald Nonlinear}Suppose Assumptions 
\emph{B1-B2, R1-R2, }and \emph{V1-V2 }hold. In addition, suppose Assumptions 
\emph{GMM1-GMM5 }hold \emph{(}or Assumptions \emph{A, B3, C1-C8, }and \emph{%
D1-D3} of \emph{AC1 }hold\emph{).}

\noindent \emph{(a) }Under $\{\gamma _{n}\}\in \Gamma (\gamma _{0},0,b)$
with $||b||<\infty ,$ $W_{n}\rightarrow _{d}\lambda (\pi ^{\ast }(\gamma
_{0},b);\gamma _{0},b).$

\noindent \emph{(b) }Under $\{\gamma _{n}\}\in \Gamma (\gamma _{0},\infty
,\omega _{0}),$ $W_{n}\rightarrow _{d}\chi _{d_{r}}^{2}.$
\end{theorem}

A special case of Theorem \ref{Theorem Wald Nonlinear} is the following
result for linear restrictions.

\begin{corollary}
\hspace{-0.08in}\textbf{.} \label{Cor Wald Linear}Suppose Assumptions \emph{%
B1-B2, R}$_{\text{\emph{L}}},$ and \emph{V1-V2} hold. In addition, suppose
Assumptions \emph{GMM1-GMM5 }hold \emph{(}or Assumptions \emph{A, B1-B3,
C1-C8, }and \emph{D1-D3 }of \emph{AC1 }hold\emph{).}

\noindent \emph{(a) }Under $\{\gamma _{n}\}\in \Gamma (\gamma _{0},0,b)$
with $||b||<\infty ,$ $W_{n}\rightarrow _{d}\lambda _{L}(\pi ^{\ast }(\gamma
_{0},b);\gamma _{0},b).$

\noindent \emph{(b) }Under $\{\gamma _{n}\}\in \Gamma (\gamma _{0},\infty
,\omega _{0}),$ $W_{n}\rightarrow _{d}\chi _{d_{r}}^{2}.$
\end{corollary}

Specific forms of the stochastic process $\lambda (\pi ;\gamma _{0},b)$ are
provided in the following examples. In Examples r1-r4, $r(\theta )$ is
linear in $\theta $ and Corollary \ref{Cor Wald Linear} applies. In Example
r5, $r(\theta )$ is nonlinear in $\theta $ and Assumption R2 is
verified.\medskip

\noindent \textbf{Example r1.}\label{exp 1} When $r(\theta )=\psi ,$ $%
R=R^{\ast }=[I_{d_{\psi }}:0],$ and $\lambda _{L}(\pi ;\gamma _{0},b)=\tau
(\pi ;\gamma _{0},b)^{\prime }\overline{\Sigma }_{\psi \psi }^{-1}(\pi
;\allowbreak \gamma _{0},b)\tau (\pi ;\gamma _{0},b),$ where $\overline{%
\Sigma }_{\psi \psi }(\pi ;\gamma _{0},b)$ is the upper left $d_{\psi
}\times d_{\psi }$ block of $\overline{\Sigma }(\pi ;\gamma _{0},b).$\medskip

\noindent \textbf{Example r2.}\label{exp 2} When $r(\theta )=\pi ,$ $%
R=R^{\ast }=[0:I_{d_{\pi }}],$ and $\lambda _{L}(\pi ;\gamma _{0},b)=||\tau
_{\beta }(\pi ;\gamma _{0},b)||^{2}(\pi -\pi _{0})^{\prime }\allowbreak 
\overline{\Sigma }_{\pi \pi }^{-1}(\pi ;\gamma _{0},b)(\pi -\pi _{0}),$
where $\overline{\Sigma }_{\pi \pi }(\pi ;\gamma _{0},b)$ is the lower right 
$d_{\pi }\times d_{\pi }$ block of $\overline{\Sigma }(\pi ;\gamma _{0},b).$%
\medskip

\noindent \textbf{Example r3.}\label{exp 3} When $d_{\psi }=d_{\pi }$ and $%
r(\theta )=\psi +\pi ,$ $R=[I_{d_{\psi }}:I_{d_{\pi }}],$ $R^{\ast
}=[0_{d_{\psi }}:I_{d_{\pi }}],$ and $\lambda _{L}(\pi ;\gamma
_{0},b)=||\tau _{\beta }(\pi ;\gamma _{0},b)||^{2}(\pi -\pi _{0})^{\prime }%
\overline{\Sigma }_{\pi \pi }^{-1}(\pi ;\gamma _{0},b)(\pi -\pi _{0}).$ Note
that $\lambda _{L}(\pi ;\gamma _{0},b)$ is the same in this example as in
Example r2. This occurs because $d_{\pi }^{\ast }=d_{r}$ so that the
randomness in $\widehat{\psi }_{n}$ is completely dominated by that in $%
\widehat{\pi }_{n}.$ Although $R$ is different in Examples r2 and r3, $%
R^{\ast }$ is the same in both examples.\medskip

\noindent \textbf{Example r4.}\label{exp 4} When $r(\theta )=\theta ,$ $%
R=R^{\ast }=I_{d_{\theta }},$ and $\lambda _{L}(\pi ;\gamma _{0},b)=%
\overline{\tau }(\pi ;\gamma _{0},b)^{\prime }\overline{B}(\pi ;\gamma
_{0},b)\allowbreak \overline{\Sigma }^{-1}(\pi ;\gamma _{0},b)\overline{B}%
(\pi ;\gamma _{0},b)\overline{\tau }(\pi ;\gamma _{0},b).$\medskip

\noindent \textbf{Example r5.}\label{exp 5} When $\theta =(\beta ,\pi
)^{\prime },$ $r(\theta )=(\beta ,\pi ^{2})^{\prime },$ and $\beta $ and $%
\pi $ are scalars, we have 
\begin{equation}
r_{\theta }(\theta )=r_{\theta }^{\ast }(\theta )=\left[ 
\begin{array}{cc}
1 & 0 \\ 
0 & 2\pi%
\end{array}%
\right] ,\text{ and }A(\theta )=I_{2}.
\end{equation}%
Assumption R2$^{\ast }$(iii) holds because $A_{2}(\theta )$ does not depend
on $\theta .$ This implies that Assumption R2 holds. The stochastic process $%
\{\tau ^{A}(\pi ;\gamma _{0},b):\pi \in \Pi \}$ can be simplified to $\tau
^{A}(\pi ;\gamma _{0},b)=(\tau (\pi ;\gamma _{0},b),\pi ^{2}-\pi _{0}^{2}).$%
\medskip

Next we show that Assumption R2 is not superfluous. In certain cases, the
Wald statistic diverges to infinity in probability under $H_{0}.$

\begin{theorem}
\hspace{-0.08in}\textbf{.} \label{Theorem Divergence}Suppose Assumptions 
\emph{B1-B2, R1, }and \emph{V1 }hold. In addition, suppose Assumptions \emph{%
GMM1-GMM4 }hold \emph{(}or Assumptions \emph{A, B1-B3, }and \emph{C1-C8 }of 
\emph{AC1 }hold\emph{).} Under $\{\gamma _{n}\}\in \Gamma (\gamma _{0},0,b),$
$W_{n}\rightarrow _{p}\infty $ if $||\eta _{n}(\widehat{\theta }%
_{n})||\rightarrow _{p}\infty .$
\end{theorem}

\noindent \textbf{Comment. }This theorem provides a high-level condition
under which the Wald statistic diverges to infinity in probability under the
null. This result holds for sequences $\{\gamma _{n}\}$ in both the weak and
semi-strong identification categories. The Wald statistic, which uses $%
r_{\theta }(\widehat{\theta }_{n})$ in the covariance matrix estimation, is
designed for the standard case in which $\widehat{\theta }_{n}$ converges to 
$\theta _{n}$ at rate $n^{-1/2}.$ When $\widehat{\pi }_{n}$ is inconsistent
or converges to $\pi _{n}$ slower than $n^{-1/2},$ the estimator of the
covariance matrix does not necessarily provide a proper normalization for
the Wald statistic to have a non-degenerate limit.$\medskip $

\noindent \textbf{Example r6.}\label{exp 6} We now demonstrate that
restrictions exist for which Assumption R2 fails to hold. Suppose $\theta
=(\beta ,\pi )^{\prime },$ $r(\theta )=((\beta +1)\pi ,\pi ^{2})^{\prime },$
and $\beta $ and $\pi $ are both scalars. Then, we have%
\begin{eqnarray}
r_{\theta }(\theta )\hspace{-0.08in} &=&\hspace{-0.08in}\left[ 
\begin{array}{cc}
\pi  & \beta +1 \\ 
0 & 2\pi 
\end{array}%
\right] ,\text{ }A_{1}(\theta )=\frac{1}{||(-2\pi ,\beta +1)||}(-2\pi ,\beta
+1),\text{ and}  \notag \\
\eta _{n}(\theta )\hspace{-0.08in} &=&\hspace{-0.08in}-\frac{n^{1/2}}{%
||(-2\pi ,\beta +1)||}[-2\pi (\beta _{n}+1)(\pi -\pi _{n})+(\beta +1)(\pi
^{2}-\pi _{n}^{2})].
\end{eqnarray}%
Consider a sequence $\{\gamma _{n}\}\in \Gamma (\gamma _{0},0,b).$ Suppose
Assumptions B1, B2, and GMM1-GMM5 hold. If $|b|<\infty ,$ assume $P(\pi
^{\ast }(\gamma _{0},b)=0)=0$ (which typically holds when $\Pi $ contains a
nondegenerate interval). Some calculations show that under $\{\gamma _{n}\},$
we have $\eta _{n}(\widehat{\theta }_{n})=||(-2\pi _{0},1)||^{-1}\allowbreak
\times \lbrack n^{1/2}\beta _{n}(\widehat{\pi }_{n}-\pi
_{n})]^{2}(n^{1/4}\beta _{n})^{-2}(1+o(1))+O_{p}(1).$\footnote{%
This holds because $\eta _{n}(\theta )=-\frac{n^{1/2}}{||(-2\pi ,\beta +1)||}%
[-2\pi (\beta _{n}+1)(\pi -\pi _{n})+\allowbreak (\beta _{n}+1)(\pi ^{2}-\pi
_{n}^{2})+\allowbreak (\beta -\beta _{n})(\pi ^{2}-\pi _{n}^{2})]\allowbreak
=\frac{n^{1/2}}{||(-2\pi ,\beta +1)||}[(\beta _{n}+1)(\pi -\pi
_{n})^{2}-\allowbreak (\beta -\beta _{n})(\pi ^{2}-\pi _{n}^{2})].$ Hence, $%
\eta _{n}(\widehat{\theta }_{n})||(-2\widehat{\pi }_{n},\widehat{\beta }%
_{n}+1)||\allowbreak =n^{1/2}(\widehat{\pi }_{n}-\pi
_{n})^{2}(1+o(1))-\allowbreak n^{1/2}(\widehat{\beta }_{n}-\beta _{n})(%
\widehat{\pi }_{n}^{2}-\pi _{n}^{2})\allowbreak =[n^{1/2}\beta _{n}(\widehat{%
\pi }_{n}-\pi _{n})]^{2}(n^{1/4}\beta _{n})^{-2}\allowbreak (1+o(1))+O_{p}(1)
$ using Theorem \ref{Thm dist'n of estimator b=finite}(a) or \ref{Thm dist'n
of estimator b=inf}(a). (The $O_{p}(1)$ term is $o_{p}(1)$ if $|b|=\infty .$%
) Because $||(-2\widehat{\pi }_{n},\widehat{\beta }_{n}+1)||\rightarrow
_{p}\allowbreak ||(-2\pi _{0},1)||<\infty ,$ the claim follows.} In
consequence, if $n^{1/4}\beta _{n}\rightarrow 0,$ then $\eta _{n}(\widehat{%
\theta }_{n})\rightarrow _{p}\infty $ and Theorem \ref{Theorem Divergence}
applies.\footnote{%
When $|b|=\infty ,$ this holds because $n^{1/2}\beta _{n}(\widehat{\pi }%
_{n}-\pi _{n})$ has an asymptotic normal distribution by Theorem \ref{Thm
dist'n of estimator b=inf}(a). When $|b|<\infty ,$ this holds because $%
[n^{1/2}\beta _{n}(\widehat{\pi }_{n}-\pi _{n})]^{2}(n^{1/4}\beta
_{n})^{-2}=n^{1/2}(\widehat{\pi }_{n}-\pi _{n})^{2},$ $\widehat{\pi }%
_{n}\rightarrow _{d}\pi ^{\ast }(\gamma _{0},b)$ by Theorem \ref{Thm dist'n
of estimator b=finite}(a), and $P(\pi ^{\ast }(\gamma _{0},b)=0)=0.$}

Sequences for which $n^{1/2}\beta _{n}\rightarrow \infty $ and $n^{1/4}\beta
_{n}\rightarrow 0$ are in the semi-strong identification category. Hence,
this example shows that even for sequences in the semi-strong identification
category, in which case both $\widehat{\beta }_{n}$ and $\widehat{\pi }_{n}$
are consistent and asymptotically normal, the Wald test can diverge to
infinity for nonlinear restrictions due to the different rates of
convergence of $\widehat{\beta }_{n}$ and $\widehat{\pi }_{n}.$

\subsection{\hspace{-0.23in}\textbf{.}\hspace{0.18in}Asymptotic Distribution
of the Wald Statistic\newline
Under the Alternative\label{Wald under Alt Subsec}}

\hspace{0.25in}Next, we provide the asymptotic distributions of the Wald
test under alternative hypotheses, which yield power results for the Wald
test and false coverage probabilities for Wald CS's. Suppose the conditions
of Theorem \ref{Theorem Wald Nonlinear} hold. The following results are
obtained by altering of the proof of Theorem \ref{Theorem Wald Nonlinear}.
Suppose the sequence of null hypothesis values of $r(\theta )$ are $%
\{v_{n,0}^{null}:n\geq 1\}.\footnote{%
By allowing $v_{n,0}^{null}$ to depend on $n,$ we obtain results for
drifting null values. For example, if $r(\theta )=\beta ,$ this provides
results when the null and local alternative values of $\beta $ are $n^{-1/2}$%
-local to zero. This is useful for obtaining asymptotic false coverage
probabilities of CS's for $\beta $ when the true value of $\beta $ is close
to zero. In this case, the relevant null values also are close to zero, in a 
$n^{-1/2}$-local to zero sense.}$ We consider the case where the true
parameters $\{\gamma _{n}\}$ satisfy $r(\theta _{n})\neq v_{n,0}^{null}.$

First, consider the alternative hypothesis distributions $\{\gamma _{n}\}\in
\Gamma (\gamma _{0},0,b)$ with $b\in R^{d_{\beta }}.$ Suppose the sequence
of true values $\{\theta _{n}\}$ satisfies $n^{1/2}(r(\theta
_{n})-v_{n,0}^{null})\rightarrow d$ for some $d\in R^{d_{r}}.$ Then, the
asymptotic distribution of $W_{n}(v_{n,0}^{null})$ is given by the
expression in Theorem \ref{Theorem Wald Nonlinear}(a), but with $\tau
^{A}(\pi ;\gamma _{0},b)$ in the definition of $\lambda (\pi ;\gamma _{0},b)$
replaced by $\tau ^{A\ast }(\pi ;\gamma _{0},b)=\tau ^{A}(\pi ;\gamma
_{0},b)+(A_{1}(\psi _{0},\pi )d,0_{d_{\pi }^{\ast }}).$ Alternatively,
suppose the sequence of true values satisfies $r(\theta
_{n})-v_{n,0}^{null}\rightarrow d_{0}\in R^{d_{r}}$ and $d_{0}\neq 0.$ When $%
A_{1}(\theta )\neq 0$ $\forall \theta \in \Theta ,$ $W_{n}(v_{n,0}^{null})%
\rightarrow _{p}\infty .$ When $A_{1}(\theta )=0$ $\forall \theta \in \Theta
,$ the asymptotic distribution of $W_{n}(v_{n,0}^{null})$ is given by the
expression in Theorem \ref{Theorem Wald Nonlinear}(a), but with $\tau
^{A}(\pi ;\gamma _{0},b)$ in the definition of $\lambda (\pi ;\gamma _{0},b)$
replaced by $\tau ^{A\ast \ast }(\pi ;\gamma _{0},b)=\tau ^{A}(\pi ;\gamma
_{0},b)+(0_{d_{r}-d_{\pi }^{\ast }},A_{2}(\psi _{0},\pi )d_{0}).$

Next, consider the alternative hypothesis distributions $\{\gamma _{n}\}\in
\Gamma (\gamma _{0},\infty ,\omega _{0})$ with $\beta _{0}\neq 0.$ When $%
n^{1/2}(r(\theta _{n})-v_{n,0}^{null})\rightarrow d$ for some $d\in
R^{d_{r}},$ $W_{n}(v_{n,0}^{null})$ converges in distribution to a
non-central $\chi _{d_{r}}^{2}$distribution with noncentrality parameter $%
\delta ^{2}=d^{\prime }(r_{\theta }(\theta _{0})B^{-1}(\beta
_{0})\allowbreak \Sigma (\gamma _{0})B^{-1}(\beta _{0})r_{\theta }(\theta
_{0})^{\prime })^{-1}d.$ Alternatively, when $r(\theta
_{n})-v_{n,0}^{null}\rightarrow d_{0}$ for some $d_{0}\in R^{d_{r}}$ with $%
d_{0}\neq 0,$ $W_{n}\rightarrow _{p}\infty .$

Lastly, consider the alternative hypothesis distributions $\{\gamma
_{n}\}\in \Gamma (\gamma _{0},\infty ,\omega _{0})$ with $\beta _{0}=0.$
Suppose the restrictions satisfy $r(\theta )=(r_{1}(\psi ),r_{2}(\theta ))$
for $r_{2}(\theta )\in R^{d_{\pi }^{\ast }}$ with $d_{\pi }^{\ast }\geq 0$
and the $d_{\pi }^{\ast }\times d_{\pi }$ matrix $(\partial /\partial \pi
^{\prime })r_{2}(\theta )$ has full rank $d_{\pi }^{\ast }.\footnote{%
Under these conditions on $r(\theta ),$ one can take $A(\theta )=I_{d_{r}}.$}
$ Let $v_{n,0}^{null}=(v_{n,0,1}^{null},v_{n,0,2}^{null})$ for $%
v_{n,0,2}^{null}\in R^{d_{\pi }^{\ast }}.$ When%
\begin{equation}
n^{1/2}(r_{1}(\theta _{n})-v_{n,0,1}^{null})\rightarrow d_{1}\in
R^{d_{r}-d_{\pi }^{\ast }}\text{ and }n^{1/2}\iota (\beta _{n})(r_{2}(\theta
_{n})-v_{n,0,2}^{null})\rightarrow d_{2}\in R^{d_{\pi }^{\ast }},
\label{Local deviations}
\end{equation}%
the asymptotic distribution of $W_{n}(v_{n,0}^{null})$ is a non-central $%
\chi _{d_{r}}^{2}$ distribution with non-centrality parameter $\delta
^{2}=d^{\prime }(r_{\theta }^{\ast }(\theta _{0})\Sigma (\gamma
_{0})r_{\theta }^{\ast }(\theta _{0})^{\prime })^{-1}d,$ where $%
d=(d_{1},d_{2})\in R^{d_{r}}.$ Note that the local alternatives in (\ref%
{Local deviations}) are $n^{-1/2}$-alternatives for the $r_{1}(\psi )$
restrictions, but are more distant $n^{-1/2}\iota (\beta _{n})^{-1}$%
-alternatives for the $r_{2}(\theta )$ restrictions due to the slower $%
n^{1/2}\iota (\beta _{n})$-rate of convergence of $\widehat{\pi }_{n}$ in
the present context. Alternatively, when $r(\theta
_{n})-v_{n,0}^{null}\rightarrow d_{0}$ for some $d_{0}\in R^{d_{r}}$ with $%
d_{0}\neq 0,$ $W_{n}\rightarrow _{p}\infty .$

\subsection{\hspace{-0.23in}\textbf{.}\hspace{0.18in}Asymptotic Size of
Standard Wald Confidence Sets}

\hspace{0.25in}Here, we determine the asymptotic size of a standard CS for $%
r(\theta )\in R^{d_{r}}$ obtained by inverting a Wald statistic, i.e., 
\begin{equation}
CS_{W,n}=\{v:W_{n}(v)\leq \chi _{d_{r},1-\alpha }^{2}\},  \label{wald CS}
\end{equation}%
where the Wald statistic $W_{n}(v)$ is as in (\ref{Defn Wald}), $\chi
_{d_{r},1-\alpha }^{2}$ is the $1-\alpha $ quantile of a chi-square
distribution with $d_{r}$ degree of freedom, and $1-\alpha $ is the nominal
size of the CS.

The asymptotic size of the CS above is determined using the asymptotic
distribution of $W_{n}=W_{n}(r(\theta _{n}))$ under drifting sequences of
true parameters, as given in Theorems \ref{Theorem Wald Nonlinear} and \ref%
{Theorem Divergence}. For $||b||<\infty ,$ define 
\begin{eqnarray}
h\hspace{-0.08in} &=&\hspace{-0.08in}(b,\gamma _{0}),\text{ }H=\{h=(b,\gamma
_{0}):||b||<\infty ,\gamma _{0}\in \Gamma \text{ with }\beta _{0}=0\},\text{
and}  \notag \\
W(h)\hspace{-0.08in} &=&\hspace{-0.08in}\lambda (\pi ^{\ast }(\gamma
_{0},b);\gamma _{0},b).  \label{Defn of W(h) and T(h)}
\end{eqnarray}%
As defined, $W(h)$ is the asymptotic distribution of $W_{n}$ under $\{\gamma
_{n}\}\in \Gamma (\gamma _{0},0,b)$ for $||b||<\infty $ determined in
Theorem \ref{Theorem Wald Nonlinear}(a).

Let $c_{W,1-\alpha }(h)$ denote the $1-\alpha $ quantile of $W(h)$ for $h\in
H.$

As in (\ref{AsySz Confid Set defn}), $AsySz$ denotes the asymptotic size of
a CS of nominal level $1-\alpha .$ The asymptotic size results use the
following distribution function (df) continuity assumption, which typically
is not restrictive.$\medskip $

\noindent \textbf{Assumption V4.} The df of $W(h)$ is continuous at $\chi
_{d_{r},1-\alpha }^{2}$ and $\sup_{h\in H}c_{W,1-\alpha }(h)$ $\forall h\in
H.$

\begin{theorem}
\hspace{-0.08in}\textbf{.} \label{Theorem Wald Asy Sz}Suppose Assumptions 
\emph{B1-B2, R1-R2, V1-V2, }and\emph{\ V4 hold. }In addition, suppose
Assumptions \emph{GMM1-GMM5 }hold \emph{(}or Assumptions \emph{A,} \emph{%
B1-B3,} \emph{C1-C8,} and \emph{D1-D3} of \emph{AC1 }hold\emph{).} Then, the
standard nominal $1-\alpha $ Wald CS satisfies%
\begin{equation*}
AsySz=\min \{\inf_{h\in H}P(W(h)\leq \chi _{d_{r},1-\alpha }^{2}),\text{ }%
1-\alpha \}.
\end{equation*}
\end{theorem}

\noindent \textbf{Comment. }Under Assumption R$_{\text{L}}$ (i.e., linearity
of $r(\theta )$), Theorem \ref{Theorem Wald Asy Sz} holds with $W(h)$
replaced by the equivalent, but simpler, quantity $W_{L}(h)=\lambda _{L}(\pi
^{\ast }(\gamma _{0},b);\gamma _{0},b)$ for $h=(b,\gamma _{0}).$ This holds
by Corollary \ref{Cor Wald Linear}(a).\medskip

Theorem \ref{Theorem Divergence} shows that the Wald statistic $W_{n}$
diverges to infinity in some circumstances, e.g., see Example r6 in Section %
\ref{Subsec Asy Distn Wald Stat} above. In such cases, the standard Wald CS
has asymptotic size equal to 0.

\begin{corollary}
\hspace{-0.08in}\textbf{.} \label{Corollary Wald 2}Suppose Assumptions \emph{%
B1-B2, R1,} and \emph{V1} hold. In addition, suppose Assumptions \emph{%
GMM1-GMM5 }hold \emph{(}or Assumptions \emph{A,} \emph{B1-B3,} \emph{C1-C8,}
and \emph{D1-D3} of \emph{AC1 }hold\emph{).} If $||\eta _{n}(\widehat{\theta 
}_{n})||\rightarrow _{p}\infty $ under $\{\gamma _{n}\}\in \Gamma (\gamma
_{0},0,b)$ for some $\gamma _{0}\in \Gamma $ and $||b||<\infty ,$ the
standard nominal $1-\alpha $ Wald CS has $AsySz=0.$
\end{corollary}

\subsection{\hspace{-0.23in}\textbf{.}\hspace{0.18in}Robust Wald Confidence
Sets\label{Robust Wald CS Subsec}}

\hspace{0.25in}Next, we construct Wald CS's that have correct asymptotic
size. These CS's are robust to the strength of identification. The CS's for $%
r(\theta )$ are constructed by inverting a robust Wald test that combines
the Wald test statistic with a robust critical value that differs from the
usual $\chi _{d_{r}}^{2}$-quantile, which is designed for the
strong-identification case. The first robust CS uses the least favorable
(LF) critical value. The second robust CS, called a type 2 robust CS, is
introduced in AC1. It uses a data-dependent critical value. It is smaller
than the LF robust CS under strong identification and, hence, is preferable.

\subsubsection{\hspace{-0.19in}\textbf{.}\hspace{0.18in}Least Favorable
Critical Value\textbf{\label{LF Subsubsec}}}

\hspace{0.25in}The LF critical value is%
\begin{equation}
c_{W,1-\alpha }^{LF}=\max \{\sup_{h\in H}c_{W,1-\alpha }(h),\chi
_{d_{r},1-\alpha }^{2}\}.  \label{LF defn}
\end{equation}

The LF critical value can be improved (i.e., made smaller) by exploiting the
knowledge of the null hypothesis value of $r(\theta ).$ For instance, if the
null hypothesis specifies the value of $\pi $ to be $3,$ then the supremum
in (\ref{LF defn}) does not need to be taken over all $h\in H,$ only over
the $h$ values for which $\pi =3.$ We call such a critical value a
null-imposed (NI) LF critical value. Using a NI-LF critical value increases
the computational burden because a different critical value is employed for
each null hypothesis value.\footnote{%
To be precise, let $H(v)=\{h=(b,\gamma _{0})\in H:||b||<\infty ,r(\theta
_{0})=v\},$ where $\gamma _{0}=(\theta _{0},\phi _{0}).$ By definition, $%
H(v) $ is the subset is $H$ that is consistent with the null hypothesis $%
H_{0}:r(\theta _{0})=v,$ where $\theta _{0}$ denotes the true value. The
NI-LF critical value, denoted $c_{W,1-\alpha }^{LF}(v),$ is defined by
replacing $H$ by $H(v)$ in (\ref{LF defn}) when the null hypothesis value is 
$r(\theta _{0})=v.$ Note that $v$ takes values in the set $V_{r}=\{v_{0}:$ $%
r(\theta _{0})=v_{0}$ for some $h=(b,\gamma _{0})\in H\}.$}$^{,}$\footnote{%
When $r(\theta )=\beta $ and the null hypothesis imposes that $\beta =v,$
the parameter $b$ can be imposed to equal $n^{1/2}v.$ In this case, $%
H(v)=H_{n}(v)=\{h=(b,\gamma _{0})\in H:b=n^{1/2}v\}.$ The asymptotic size
results given below for NI-LF CS's and NI robust CS's hold in this case.}

When part of $\gamma $ is unknown under $H_{0}$ but can be consistently
estimated, then a \emph{plug-in} LF (or plug-in NI-LF) critical value can be
used that has correct size asymptotically and is smaller than the LF (or
NI-LF) critical value. The plug-in critical value replaces elements of $%
\gamma $ with consistent estimators in the formulae in (\ref{LF defn}) and
the supremum over $H$ is reduced to a supremum over the resulting subset of $%
H,$ denoted $\widehat{H}_{n},$ for which the consistent estimators appear in
each vector $\gamma .$\footnote{%
For example, if $\zeta $ is consistently estimated by $\widehat{\zeta }_{n},$
then $H$ is replaced by $\widehat{H}_{n}=\{h=(b,\gamma )\in H:\gamma =(\beta
,\widehat{\zeta }_{n},\pi ,\phi )\}.$ If a plug-in NI-LF critical value is
employed, $H(v)$ is replaced by $H(v)\cap \widehat{H}_{n},$ where $H(v)$ is
defined in a footnote above. The parameter $b$ is not consistently
estimable, so it cannot be replaced by a consistent estimator.}

\subsubsection{\hspace{-0.19in}\textbf{.}\hspace{0.18in}Type 2 Robust
Critical Value\label{Type 2 robust CI Subsubsec}}

\hspace{0.25in}Next, we define the type 2 robust critical value. It improves
on the LF critical value. It employs an identification category selection
(ICS) procedure that uses the data to determine whether $b$ is finite.%
\footnote{%
When $\beta $ is specified by the null hypothesis, it is not necesary to use
an ICS procedure. Instead, we recommend using a (possibly plug-in) NI-LF
critical value, see the footnote above.} The ICS procedure chooses between
the identification categories $\mathcal{IC}_{0}:||b||<\infty $ and $\mathcal{%
IC}_{1}:||b||=\infty .$ The identification-category selection statistic is%
\begin{equation}
A_{n}=\left( n\widehat{\beta }_{n}^{\prime }\widehat{\Sigma }_{\beta \beta
,n}^{-1}\widehat{\beta }_{n}/d_{\beta }\right) ^{1/2},
\label{Defn of A_n for robust CS}
\end{equation}%
where $\widehat{\Sigma }_{\beta \beta ,n}$ is the upper left $d_{\beta
}\times d_{\beta }$ block of $\widehat{\Sigma }_{n},$ which is defined in (%
\ref{Variance Matrix Defns}).

The type 2 robust critical value provides a continuous transition from a
weak-identification critical value to a strong-identification critical value
using a transition function $s(x).$ Let $s(x)$ be a continuous function on $%
[0,\infty )$ that satisfies: (i) $0\leq s(x)\leq 1,$ (ii) $s(x)$ is
non-increasing in $x,$ (iii) $s(0)=1,$ and (iv) $s(x)\rightarrow 0$ as $%
x\rightarrow \infty .$ Examples of transition functions include (i) $%
s(x)=\exp (-c\cdot x)$ for some $c>0$ and (ii) $s(x)=(1+c\cdot x)^{-1}$ for
some $c>0.\footnote{%
If $c_{W,1-\alpha }^{LF}=\infty ,$ $s(x)$ should be taken to equal $0$ for $x
$ sufficiently large, where $\infty \times 0$ equals $0$ in (\ref{type 2
robust cv}). Then, the critical value $\widehat{c}_{W,1-\alpha ,n}$ is
infinite if $A_{n}$ is small and is finite if $A_{n}$ is sufficiently large.}
$ For example, in the nonlinear regression\ model with endogeneity, we use
the function $s(x)=\exp (-2x).$

The type 2 robust critical value is%
\begin{eqnarray}
\widehat{c}_{W,1-\alpha ,n}\hspace{-0.08in} &=&\hspace{-0.08in}\left\{ 
\begin{tabular}{ll}
$c_{B}$ & $\text{if }A_{n}\leq \kappa $ \\ 
$c_{S}+[c_{B}-c_{S}]\cdot s(A_{n}-\kappa )$ & $\text{if }A_{n}>\kappa ,$
where%
\end{tabular}%
\right.  \notag \\
c_{B}\hspace{-0.08in} &=&\hspace{-0.08in}c_{W,1-\alpha }^{LF}+\Delta _{1},%
\text{ }c_{S}=\chi _{d_{r},1-\alpha }^{2}+\Delta _{2},
\label{type 2 robust cv}
\end{eqnarray}%
and $\Delta _{1}\geq 0$ and $\Delta _{2}\geq 0$ are asymptotic
size-correction factors that are defined below. Here, \textquotedblleft $B$%
\textquotedblright\ denotes Big, and \textquotedblleft $S$%
\textquotedblright\ denotes Small. When $A_{n}\leq \kappa ,$ $\widehat{c}%
_{W,1-\alpha ,n}$ equals the LF critical value $c_{W,1-\alpha }^{LF}$ plus a
size-correction factor $\Delta _{1}.$ When $A_{n}>\kappa ,$ $\widehat{c}%
_{W,1-\alpha ,n}$ is a linear combination of $c_{W,1-\alpha }^{LF}+\Delta
_{1}$ and $\chi _{d_{r},1-\alpha }^{2}+\Delta _{2},$ where $\Delta _{2}$ is
another size-correction factor. The weight given to the standard critical
value $\chi _{d_{r},1-\alpha }^{2}$ increases with the strength of
identification, as measured by $A_{n}-\kappa .$

The ICS statistic $A_{n}$ satisfies $A_{n}\rightarrow _{d}A(h)$ under $\{
\gamma _{n}\} \in \Gamma (\gamma _{0},0,b)$ with $||b||<\infty ,$ where $%
A(h) $ is defined by%
\begin{equation}
A(h)=\left( \tau _{\beta }(\pi ^{\ast };\gamma _{0},b)^{\prime }\Sigma
_{\beta \beta }^{-1}(\pi ^{\ast };\gamma _{0})\tau _{\beta }(\pi ^{\ast
};\gamma _{0},b)/d_{\beta }\right) ^{1/2},  \label{A(h) defn}
\end{equation}%
where $\pi ^{\ast }$ abbreviates $\pi ^{\ast }(\gamma _{0},b),$ $\tau
_{\beta }(\pi ;\gamma _{0},b)$ is defined in (\ref{Asy Distn of Betahat(pi)}%
), and $\Sigma _{\beta \beta }(\pi ;\gamma _{0})$ is the upper left (1,1)
element of $\Sigma (\psi _{0},\pi ;\gamma _{0})$ for $\Sigma (\theta ;\gamma
_{0})=J^{-1}(\theta ;\gamma _{0})V(\theta ;\gamma _{0})J^{-1}(\theta ;\gamma
_{0}).$\footnote{%
The convergence in distribution follows from Theorem \ref{Thm dist'n of
estimator b=finite}(a) and Assumption V1.}$^{,}$\footnote{%
In the vector $\beta $ case, $\Sigma _{\beta \beta }^{-1}(\pi ^{\ast
};\gamma _{0})$ is replaced in (\ref{A(h) defn}) by a slightly different
expresssion, see footnote 51 of AC1. When the type 2 robust critical value
is considered in the vector $\beta $ case, $h$ is defined to include $\omega
_{0}=\lim_{n\rightarrow \infty }\beta _{n}/||\beta _{n}||\in R^{d_{\beta }}$
as an element, i.e., $h=(b,\gamma _{0},\omega _{0})$ and $H=\{h=(b,\gamma
_{0},\omega _{0}):||b||<\infty ,\gamma _{0}\in \Gamma $ with $\beta
_{0}=0,||\omega _{0}||=1\}$ because the true value $\omega _{0}$ affects the
asymptotic distribution of $A_{n}.$}$^{,}$\footnote{%
Alternatively to the ICS statistic $A_{n},$ one can use a NI-ICS statistic $%
A_{n}(v),$ which employs the restricted estimator $\widetilde{\beta }_{n}(v)$
of $\beta $ in place of $\widehat{\beta }_{n}$ and a different weight
matrix. See AC1 for details.
\par
{}}

Under $\gamma _{n}\in \Gamma (\gamma _{0},0,b)$ with $||b||<\infty ,$ the
asymptotic null rejection probability of a test based on the statistic $%
W_{n} $ and the robust critical value $\widehat{c}_{W,1-\alpha ,n}$ is equal
to%
\begin{eqnarray}
&&\hspace{-0.2in}NRP(\Delta _{1},\Delta _{2};h)\overset{}{=}P(W(h)\overset{}{%
>}c_{B}\text{ \& }A(h)\overset{}{\leq }\kappa )+P(W(h)\overset{}{>}c_{A}(h)%
\text{ \& }A(h)\overset{}{>}\kappa )  \notag \\
&&\hspace{-0.2in}\hspace{1.21in}\overset{}{=}P(W(h)\overset{}{>}c_{B})+P(W(h)%
\overset{}{\in }(c_{A}(h),c_{B}]\text{ \& }A(h)\overset{}{>}\kappa ),\text{
where}  \notag \\
&&\hspace{-0.2in}c_{A}(h)\overset{}{=}c_{S}+(c_{B}-c_{S})\cdot s(A(h)-\kappa
).  \label{NRP defn}
\end{eqnarray}

The constants $\Delta _{1}$ and $\Delta _{2}$ are chosen such that $%
NRP(\Delta _{1},\Delta _{2};h)\leq \alpha $ $\forall h\in H.$ In particular,
we define $\Delta _{1}=\sup_{h\in H_{1}}\Delta _{1}(h),$ where $\Delta
_{1}(h)\geq 0$ solves $NRP(\Delta _{1}(h),0;h)=\alpha $ (or $\Delta
_{1}(h)=0 $ if $NRP(0,0;h)<\alpha ),$ $H_{1}=\{(b,\gamma _{0}):(b,\gamma
_{0})\in H$ \& $||b||\leq ||b_{\max }||+D\},$ $b_{\max }$ is defined such
that $c_{W,1-\alpha }(h)$ is maximized over $h\in H$ at $h_{\max }=(b_{\max
},\gamma _{\max })\in H$ for some $\gamma _{\max }\in \Gamma ,$ and $D$ is a
non-negative constant, such as $1.$ We define $\Delta _{2}=\sup_{h\in
H}\Delta _{2}(h),$ where $\Delta _{2}(h)$ solves $NRP(\Delta _{1},\Delta
_{2}(h);h)=\alpha $ (or $\Delta _{2}(h)=0$ if $NRP(\Delta _{1},0;\allowbreak
h)<\alpha ).\footnote{%
When $NRP(0,0;h)>\alpha ,$ a unique solution $\Delta _{1}(h)$ typically
exists because $NRP(\Delta _{1},0;h)$ is always non-increasing in $\Delta
_{1}$ and is typically strictly decreasing and continuous in $\Delta _{1}.$
If no exact solution to $NRP(\Delta _{1}(h),0;h)=\alpha $ exists, then $%
\Delta _{1}(h)$ is taken to be any value for which $NRP(\Delta
_{1}(h),0;h)\leq \alpha $ and $\Delta _{1}(h)\geq 0$ is as small as
possible. Analogous comments apply to the equation $NRP(\Delta _{1},\Delta
_{2}(h);h)=\alpha $ and the definition of $\Delta _{2}(h).$}^{,}\footnote{%
When the LF critical value is achieved at $||b||=\infty ,$ i.e., $\chi
_{d_{r},1-\alpha }^{2}\geq \sup_{h\in H}c_{QLR,1-\alpha }(h),$ the standard
asymptotic critical value $\chi _{d_{r},1-\alpha }^{2}$ yields a test or CI
with correct asymptotic size and constants $\Delta _{1}$ and $\Delta _{2}$
are not needed. Hence, here we consider the case where $||b_{\max }||<\infty
.$ If $\sup_{h\in H}c_{QLR,1-\alpha }(h)$ is not attained at any point $%
h_{\max },$ then $b_{\max }$ can be taken to be any point such that $%
c_{QLR,1-\alpha }(h_{\max })$ is arbitrarily close to $\sup_{h\in
H}c_{QLR,1-\alpha }(h)$ for some $h_{\max }=(b_{\max },\gamma _{\max })\in
H. $}$ As defined, $\Delta _{1}$ and $\Delta _{2}$ can be computed
sequentially, which eases computation.

Given the definitions of $\Delta _{1}$ and $\Delta _{2},$ the asymptotic
rejection probability is always less than or equal to the nominal level $%
\alpha $ and it is close to $\alpha $ when $h$ is close to $h_{\max }$ (due
to the adjustment by $\Delta _{1})$ and when $||b||$ is large (due to the
adjustment by $\Delta _{2}).$

The type 2 robust critical value can be improved by employing NI and/or
plug-in versions of it, denoted by $\widehat{c}_{W,1-\alpha ,n}(v).$ These
are defined by replacing $c_{W,1-\alpha }^{LF}$ in (\ref{type 2 robust cv})
by the NI-LF or plug-in NI-LF critical value and making $c_{B},$ $\Delta
_{1},$ and $\Delta _{2}$ depend on the null value $v,$ denoted $c_{B}(v),$ $%
\Delta _{1}(v),$ and $\Delta _{2}(v).$ We recommend using these versions
whenever possible because they lead to smaller CS's.

For any given value of $\kappa ,$ the type 2 robust CS has correct
asymptotic size due to the choice of $\Delta _{1}$ and $\Delta _{2}.$ In
consequence, a good choice of $\kappa $ depends on the false coverage
probabilities (FCP's) of the robust CS. (An FCP of a CS for $r(\theta )$ is
the probability that the CS includes a value different from the true value $%
r(\theta ).)$ The numerical work in this paper and in AC1 and AC2 shows that
if a reasonable value of $\kappa $ is chosen, such as $\kappa =1.5$ or $2.0,$
the FCP's of type 2 robust CS's are not sensitive to deviations from this
value of $\kappa .$ This is because the size-correction constants $\Delta
_{1}$ and $\Delta _{2}$ have to adjust as $\kappa $ is changed in order to
maintain correct asymptotic size. The adjustments of $\Delta _{1}$ and $%
\Delta _{2}$ offset the effect of changing $\kappa .$

One can select $\kappa $ in a simple way, i.e., by taking $\kappa =1.5$ or $%
2.0,$ or one can select $\kappa $ in a more sophisticated way that
explicitly depends on FCP's. Both methods yield similar results for the
cases that we have considered.

The more sophisticated method of choosing $\kappa $ is to minimize the
average FCP of the robust CS over a chosen set of $\kappa $ values denoted
by $\mathcal{K}.$ First, for given $h\in H,$ one chooses a null value $%
v_{H_{0}}(h)$ that differs from the true value $v_{0}=r(\theta _{0})$ (where 
$h=(b,\gamma _{0})$ and $\gamma _{0}=(\theta _{0},\phi _{0})).$ The null
value $v_{H_{0}}(h)$ is selected such that the robust CS based on a
reasonable choice of $\kappa ,$ such as $\kappa =1.5$ or $2,$ has a FCP that
is in a range of interest, such as close to $0.50.\footnote{%
When $b$ is close to $0,$ the FCP may be larger than $0.50$ for all
admissible $v$ due to weak identification. In such cases, $v_{H_{0}}(h)$ is
taken to be the admissible value that minimizes the FCP for the selected
value of $\kappa $ that is being used to obtain $v_{H_{0}}(h).$}$ Second,
one computes the FCP of the value $v_{H_{0}}(h)$ for each robust CS with $%
\kappa \in \mathcal{K}.$ Third, one repeats steps one and two for each $h\in 
\mathcal{H},$ where $\mathcal{H}$ is a representative subset of $H.\footnote{%
When $r(\theta )=\pi ,$ we do not include $h$ values in $\mathcal{H}$ for
which $b=0$ because when $b=0$ there is no information about $\pi $ and it
is not necessarily desirable to have a small FCP.}$ The optimal choice of $%
\kappa $ is the value that minimizes over $\mathcal{K}$ the average over $%
h\in \mathcal{H}$ of the FCP's at $v_{H_{0}}(h).$

In summary, the steps used to construct a type 2 robust Wald (or $t$) test
are as follows: (1) Estimate the model using the standard GMM estimator,
yielding $\widehat{\beta }_{n}$ and the covariance matrix $\widehat{\Sigma }%
_{\beta \beta ,n}.$ (2) Compute the Wald statistic using the formula in (\ref%
{Defn Wald}). (3) Construct the ICS statistic $A_{n}$ defined in (\ref{Defn
of A_n for robust CS}). (4) Simulate the LF critical value $c_{W,1-\alpha
}^{LF}$ and the size correction factors $\Delta _{1}$ and $\Delta _{2}$
based on the asymptotic formulae in (\ref{Defn of W(h) and T(h)}), (\ref%
{A(h) defn}), and (\ref{NRP defn}) and the description below (\ref{NRP defn}%
), for a given value of $\kappa .$ (5). Compute the type 2 robust critical
value $\widehat{c}_{W,1-\alpha ,n}$ defined in (\ref{type 2 robust cv}),
employing the NI and/or plug-in versions when applicable. (6). Choose $%
\kappa $ by minimizing the FCP of the type 2 robust CI. The last step can be
avoided when the type 2 robust CI constructed is not very sensitive to the
choice of $\kappa ,$ which is typically the case found in our simulation
studies. For a type 2 robust CI for a particular parameter, one takes the CI
to consist of all null values of the parameter for which the type 2 robust
test fails to reject the null hypothesis. This can be computed by grid
search or some more sophisticated method, such as a multi-step grid search
where the fineness of the grid varies across the steps.

\subsubsection{\hspace{-0.19in}\textbf{.}\hspace{0.18in}Asymptotic Size of
Robust Wald CS's\label{Asy Size Robust Wald CS Subsubsec}}

\hspace{0.25in}In this section, we show that the LF and data-dependent
robust CS's defined above have correct asymptotic size. The asymptotic size
results rely on the following df continuity conditions, which are not
restrictive in most examples.$\medskip $

\noindent \textbf{Assumption LF.} (i) The df of $W(h)$ is continuous at $%
c_{W,1-\alpha }(h)$ $\forall h\in H.$

\noindent (ii) If $c_{W,1-\alpha }^{LF}>\chi _{d_{r},1-\alpha }^{2},$ $%
c_{W,1-\alpha }^{LF}$ is attained at some $h_{\max }\in H.\medskip $

\noindent \textbf{Assumption NI-LF.} (i) The df of $W(h)$ is continuous at $%
c_{W,1-\alpha }(h)$ $\forall h\in H(v),$ $\forall v\in V_{r}.$

\noindent (ii) For some $v\in V_{r},$ $c_{W,1-\alpha }^{LF}(v)=\chi
_{d_{r},1-\alpha }^{2}$ or $c_{W,1-\alpha }^{LF}(v)$ is attained at some $%
h_{\max }\in H.\medskip $

For $h\in H,$ define%
\begin{equation}
\widehat{c}_{W,1-\alpha }(h)=\left\{ 
\begin{tabular}{ll}
$c_{B}$ & $\text{if }A(h)\leq \kappa $ \\ 
$c_{S}+[c_{B}-c_{S}]\cdot s(A(h)-\kappa )$ & $\text{if }A(h)>\kappa .$%
\end{tabular}%
\right.  \label{Defn of chat(h)}
\end{equation}%
As defined, $\widehat{c}_{W,1-\alpha }(h)$ equals $\widehat{c}_{W,1-\alpha
,n}$ with $A(h)$ in place of $A_{n}.$ The asymptotic distribution of $%
\widehat{c}_{W,1-\alpha ,n}$ under $\{\gamma _{n}\}\in \Gamma (\gamma
_{0},0,b)$ for $||b||<\infty $ is the distribution of $\widehat{c}%
_{W,1-\alpha }(h).$

Define $\widehat{c}_{W,1-\alpha }(h,v)$ analogously to $\widehat{c}%
_{W,1-\alpha }(h),$ but with $c_{W,1-\alpha }^{LF},$ $\Delta _{1},$ and $%
\Delta _{2}$ replaced by $c_{W,1-\alpha }^{LF}(v),$ $\Delta _{1}(v),$ and $%
\Delta _{2}(v),$ respectively, for $v\in V_{r}.$ The asymptotic distribution
of $\widehat{c}_{W,1-\alpha ,n}(v)$ under $\{\gamma _{n}\}\in \Gamma (\gamma
_{0},0,b)$ for $||b||<\infty $ is the distribution of $\widehat{c}%
_{W,1-\alpha }(h,v).\medskip $

\noindent \textbf{Assumption Rob2. }(i) $P(W(h)=\widehat{c}_{W,1-\alpha
}(h))=0$ $\forall h\in H.$

\noindent (ii) If $\Delta _{2}>0,$ $NRP(\Delta _{1},\Delta _{2};h^{\ast })%
\overset{}{=}\alpha $ for some point $h^{\ast }\in H,$ where $\Delta _{1}$
and $\Delta _{2}$ are defined following (\ref{NRP defn}).$\medskip $

\noindent \textbf{Assumption NI-Rob2. }(i) $P(W(h)=\widehat{c}_{W,1-\alpha
}(h,v))=0$ $\forall h\in H(v),$ $\forall v\in V_{r}.$

\noindent (ii) For some $v\in V_{r},$ $\Delta _{2}(v)=0$ or $NRP(\Delta
_{1}(v),\Delta _{2}(v);h^{\ast })=\alpha $ for some point $h^{\ast }\in
H(v), $ where $\Delta _{1}(v)$ and $\Delta _{2}(v)$ are defined following (%
\ref{NRP defn}).

\begin{theorem}
\hspace{-0.08in}\textbf{.} \label{Theorem Robust Size Wald CS}Suppose
Assumptions \emph{B1-B2, R1-R2, }and \emph{V1-V2 hold. }In addition, suppose
Assumptions \emph{GMM1-GMM5 }hold \emph{(}or Assumptions \emph{A,} \emph{%
B1-B3,} \emph{C1-C8,} and \emph{D1-D3} of \emph{AC1 }hold\emph{). }Then, the
nominal $1-\alpha $ robust Wald CS has $AsySz=1-\alpha $ when based on the
following critical values\emph{:} \emph{(a) LF,} \emph{(b) NI-LF, (c) }type 
\emph{2} robust, and \emph{(d) }type \emph{2} \emph{NI} robust, provided the
following additional Assumptions hold, respectively\emph{:} \emph{(a) LF,
(b) NI-LF,} \emph{(c) Rob2, }and \emph{(d) NI-Rob2.}
\end{theorem}

\noindent \textbf{Comments. 1. }Plug-in versions of the robust Wald CS's
considered in Theorem \ref{Theorem Robust Size Wald CS} also have
asymptotically correct size under continuity assumptions on $c_{W,1-\alpha
}(h)$ that typically are not restrictive. For brevity, we do not provide
formal results here.

\noindent \textbf{2. }If part (ii) of Assumption LF, NI-LF, Rob2, or NI-Rob2
does not hold, then the corresponding part of Theorem \ref{Theorem Robust
Size Wald CS} still holds, but with $AsySz\geq 1-\alpha .$

\noindent \textbf{3. }A third type of robust critical value, referred to as
type 1, is considered in AC1. Critical values of this type can be employed
with Wald statistics. The resulting type 1 robust CS's out-perform LF robust
CS's in terms of FCP's, but are inferior to type 2 robust CS's. However,
they are easier to compute than type 2 robust CS's.

\section{ \hspace{-0.34in}\textbf{.}\hspace{0.2in}QLR Confidence Sets and
Tests\label{QLR Tests Sec}}

\setcounter{equation}{0}\hspace{0.25in}In this section, we introduce CS's
based on the quasi-likelihood ratio (QLR) statistic. For brevity,
theoretical results for the QLR procedures are given in AC1. However, we
define QLR procedures here because numerical results are reported for them
in the numerical results section.

We consider CS's for a function $r(\theta )$ $(\in R^{d_{r}})$ of $\theta $
obtained by inverting QLR tests. The function $r(\theta )$ is assumed to be
smooth and to be of the form%
\begin{equation}
r(\theta )=\left[ 
\begin{array}{c}
r_{1}(\psi ) \\ 
r_{2}(\pi )%
\end{array}%
\right] ,  \label{Form of r(theta)}
\end{equation}%
where $r_{1}(\psi )\in R^{d_{r_{1}}},$ $d_{r_{1}}\geq 0$ is the number of
restrictions on $\psi ,$ $r_{2}(\pi )\in R^{d_{r_{2}}},$ $d_{r_{2}}\geq 0$
is the number of restrictions on $\pi ,$ and $d_{r}=d_{r_{1}}+d_{r_{2}}.$

For $v\in r(\Theta ),$ we define a restricted estimator $\widetilde{\theta }%
_{n}(v)$ of $\theta $ subject to the restriction that $r(\theta )=v.$ By
definition, 
\begin{equation}
\widetilde{\theta }_{n}(v)\in \Theta ,\text{ }r(\widetilde{\theta }%
_{n}(v))=v,\text{ and }Q_{n}(\widetilde{\theta }_{n}(v))=\inf_{\theta \in
\Theta :r(\theta )=v}Q_{n}(\theta )+o(n^{-1}).
\end{equation}

For testing $H_{0}:r(\theta )=v,$ the QLR test statistic is%
\begin{equation}
QLR_{n}(v)=2n(Q_{n}(\widetilde{\theta }_{n}(v))-Q_{n}(\widehat{\theta }%
_{n}))/\widehat{s}_{n},  \label{Defn QLR Test Statistic}
\end{equation}%
where $\widehat{s}_{n}$ is a real-valued scaling factor that is employed in
some cases to yield a QLR statistic that has an asymptotic $\chi
_{d_{r}}^{2} $ null distribution under strong identification. See AC1 for
details.

Let $c_{n,1-\alpha }(v)$ denote a nominal level $1-\alpha $ critical value
to be used with the QLR test statistic. It may be stochastic or
non-stochastic. The usual choice, based on the asymptotic distribution of
the QLR statistic under standard regularity conditions, is the $1-\alpha $
quantile of the $\chi _{d_{r}}^{2}$ distribution: $c_{n,1-\alpha }(v)=\chi
_{d_{r},1-\alpha }^{2}.$

A critical value that delivers a robust QLR CS for $r(\theta )$ that has
correct asymptotic size can be constructed using the same approach as in
Section \ref{Asy Size Robust Wald CS Subsubsec}. Details are in AC1.

Given a critical value $c_{n,1-\alpha }(v),$ the nominal level $1-\alpha $
QLR CS for $r(\theta )$ is 
\begin{equation}
CS_{r,n}^{QLR}=\{v\in r(\Theta ):QLR_{n}(v)\leq c_{n,1-\alpha }(v)\}.
\label{Defn of QLR CS}
\end{equation}

\section{ \hspace{-0.34in}\textbf{.}\hspace{0.2in}Numerical Results:
Nonlinear Regression Model with Endogeneity\label{Numerical Results Sec}}

\hspace{0.25in}\setcounter{equation}{0}In this section, we provide
asymptotic and finite-sample simulation results for the nonlinear regression
model with endogeneity.

The model we consider consists of a structural equation with two right-hand
side endogenous variables $X_{1}$ and $X_{2},$ where $X_{1}$ is a nonlinear
regressor and $X_{2}$ is a linear regressor, and two reduced-form equations
for $X_{1}$ and $X_{2},$ respectively:%
\begin{eqnarray}
Y_{i}\hspace{-0.08in} &=&\hspace{-0.08in}\zeta _{1}+\beta \cdot
h(X_{1,i},\pi )+\zeta _{2}X_{2,i}+U_{i},  \notag \\
X_{1,i}\hspace{-0.08in} &=&\hspace{-0.08in}\lambda _{1}+\lambda
_{2}Z_{1,i}+V_{1,i},  \notag \\
X_{2,i}\hspace{-0.08in} &=&\hspace{-0.08in}\lambda _{3}+\lambda
_{4}Z_{2,i}+\lambda _{5}Z_{3,i}+V_{2,i},
\end{eqnarray}%
where $Y_{i},X_{1,i},X_{2,i}\in R$ are endogenous variables, $%
Z_{1,i},Z_{2,i},Z_{3,i}\in R$ are excluded exogenous variables, $h(x,\pi
)=(|x|^{\pi }-1)/\pi $, and $\theta =(\beta ,\zeta _{1},\zeta _{2},\pi
)^{\prime }\in R^{4}$ is the unknown parameter.\footnote{%
The absolute value of $x$ is employed in $h(x,\pi )$ to guarantee $h(x,\pi
)\in R$ when $\pi $ is not an integer. With the data generating process
specified below, $X_{1,i}$ is positive with probability close to $1.$ Hence, 
$h(X_{1,i},\pi )$ is approximately the Box-Cox transformation of $X_{1,i}.$}
The data generating process (DGP) satisfies $(\zeta _{1},\zeta
_{2})=(-2,2),(\lambda _{1},\lambda _{2})=(3,1),$ $(\lambda _{3},\lambda
_{4},\lambda _{5})=(0,1,1),$ $%
\{(Z_{1,i},Z_{2,i},Z_{3,i},U_{i},V_{1,i},V_{2,i}):i=1,...,n\}$ are i.i.d., $%
(Z_{1,i},Z_{2,i},Z_{3,i})$ and $(U_{i},V_{1,i},V_{2,i})$ are independent, $%
(Z_{1,i},Z_{2,i},Z_{3,i})\sim N(0,I_{3}),$ $U_{i}\sim N(0,0.25),$ $%
V_{k,i}\sim N(0,1)$ and $\mathrm{Corr}(U_{i},V_{k,i})=0.5$ for $k=1$ and $2,$
and $\mathrm{Corr}(V_{1,i},V_{2,i})=0.5.$

The IV's for the GMM estimator of $\theta $ are $%
Z_{i}=(1,Z_{1,i},Z_{1,i}^{2},Z_{2,i},Z_{3,i})^{\prime }\in R^{5}.$ Thus,
five moment conditions are used to estimate four parameters.

The true parameter space for $\pi $ is $[1.5,$ $3.5]$ and the optimization
space for $\pi $ is $[1,4].$ The finite-sample results are for $n=500.$ The
number of simulation repetitions is 20,000.\footnote{%
The discrete values of $b$ for which computations are made run from $0$ to $%
30$, with a grid of $0.2$ for $b$ between $0$ and $10,$ a grid of $1$ for $b$
between $10$ and $20$, and a grid of $2$ for $b$ between $20$ and $30.$}

Figures 1 and 2 provide the asymptotic and finite-sample densities of the
GMM estimators of $\beta $ and $\pi $ when the true $\pi $ value is $\pi
_{0}=1.5$. Each Figure gives the densities for $b=0,$ $4,$ $10,$ and $30,$
where $b$ indexes the magnitude of $\beta $. Specifically, for the
finite-sample results, $b=n^{1/2}\beta .$ Figures S-1 and S-2 in
Supplemental Appendix E provide analogous results for $\pi _{0}=3.0.$

Figure 1 shows that the ML estimator of $\beta $ has a distribution that is
very far from a normal distribution in the unidentified and
weakly-identified cases. The figure shows a build-up of mass at $0$ in the
unidentified case and a bi-modal distribution in the weakly-identified case.
Figure 2 shows that there is a build-up of mass at the boundaries of the
optimization space for the estimator of $\pi $ in the unidentified and
weakly-identified cases. Figures 1 and 2 indicate that the asymptotic
approximations developed here work very well.

% \FRAME{ftbpFU}{3.557in}{1.7038in}{0pt}{\Qcb{Figure 1. Asymptotic and
% Finite-Sample ($n=500$) Densities of the Estimator of $\protect\beta $ in
% the Nonlinear Regression Model with Endogeneity when $\protect\pi _{0}=1.5.$}%
% }{}{gmm_beta_dens_15.eps}{\special{language "Scientific Word";type
% "GRAPHIC";maintain-aspect-ratio TRUE;display "USEDEF";valid_file "F";width
% 3.557in;height 1.7038in;depth 0pt;original-width 6.9851in;original-height
% 2.9447in;cropleft "0.0861";croptop "1";cropright "0.9281";cropbottom
% "0.0512";filename 'graphics/GMM_beta_dens_15.eps';file-properties "XNPEU";}}

% \FRAME{ftbpFU}{3.557in}{1.7038in}{0pt}{\Qcb{Figure 2. Asymptotic and
% Finite-Sample ($n=500$) Densities of the Estimator of $\protect\pi $ in the
% Nonlinear Regression Model with Endogeneity when $\protect\pi _{0}=1.5.$}}{}{%
% gmm_pi_dens_15.eps}{\special{language "Scientific Word";type
% "GRAPHIC";maintain-aspect-ratio TRUE;display "USEDEF";valid_file "F";width
% 3.557in;height 1.7038in;depth 0pt;original-width 6.9851in;original-height
% 2.9447in;cropleft "0.0861";croptop "1";cropright "0.9281";cropbottom
% "0.0512";filename 'graphics/GMM_pi_dens_15.eps';file-properties "XNPEU";}}

Figures S-3 to S-6 in Supplemental Appendix E provide the asymptotic and
finite-sample ($n=500$) densities of the $t$ and QLR statistics for $\beta $
and $\pi $ when $\pi _{0}=1.5.$ These Figures show that in the case of weak
identification the $t$ and QLR statistics are not well approximated by
standard normal and $\chi _{1}^{2}$ distributions. However, the asymptotic
approximations developed here work very well.

% \FRAME{ftbpFU}{3.4823in}{3.4168in}{0pt}{\Qcb{Figure 3. Asymptotic 0.95
% Quantiles of the $|t|$ and QLR Statistics for Tests Concerning $\protect%
% \beta $ and $\protect\pi $ in the Nonlinear Regression Model with
% Endogeneity.}}{}{gmm_quant.eps}{\special{language "Scientific Word";type
% "GRAPHIC";maintain-aspect-ratio TRUE;display "USEDEF";valid_file "F";width
% 3.4823in;height 3.4168in;depth 0pt;original-width 7.4858in;original-height
% 6.7464in;cropleft "0.0875";croptop "0.9567";cropright "0.9268";cropbottom
% "0.0432";filename 'graphics/GMM_Quant.eps';file-properties "XNPEU";}}

% \FRAME{ftbpFU}{3.462in}{3.3974in}{0pt}{\Qcb{Figure 4. Coverage Probabilities
% of Standard $|t|$ and QLR CI's for $\protect\beta $ and $\protect\pi $ in
% the Nonlinear Regression Model with Endogeneity when $\protect\pi _{0}=1.5.$}%
% }{}{gmm_std_cp_15.eps}{\special{language "Scientific Word";type
% "GRAPHIC";maintain-aspect-ratio TRUE;display "USEDEF";valid_file "F";width
% 3.462in;height 3.3974in;depth 0pt;original-width 7.4858in;original-height
% 6.7464in;cropleft "0.0876";croptop "0.9567";cropright "0.9268";cropbottom
% "0.0432";filename 'graphics/GMM_Std_CP_15.eps';file-properties "XNPEU";}}

Figure 3 provides graphs of the $0.95$ asymptotic quantiles of the $|t|$ and
QLR statistics concerning $\beta $ and $\pi $ as a function of $b$ for $\pi
_{0}=1.5,$ $2.0,$ $3.0,$ and $3.5.$ For the $|t|$ statistic concerning $%
\beta ,$ for small to medium $b$ values, the graphs exceed the $0.95$
quantile under strong identification (given by the horizontal black line).
This implies that tests and CI's that employ the $|t|$ statistic for $\beta $
and the standard critical value (based on the normal distribution) have
incorrect size. For the QLR statistic for $\beta $, the graphs slightly
exceed the $0.95$ quantile under strong identification when $b$ is $0$ or
almost $0$ and fall below the $0.95$ quantile under strong identification
for other small to medium $b$ values. The graphs in Figure 3(b) imply that
tests and CI's that employ the QLR statistic for $\beta $ and the standard
critical value (based on the $\chi _{1}^{2}$ distribution) have small size
distortions due to the under-coverage for $b$ values close to $0.$ Given the
heights of the graphs in Figure 3(c) and 3(d), tests and CI's that employ
the $|t|$ statistic for $\pi $ have correct asymptotic size when $\pi
_{0}=1.5$ and $2.0$ and have slight size distortion when $\pi _{0}=3.0$ and $%
3.5,$ whereas those that employ the QLR statistic for $\pi $ always have
correct asymptotic size.

Figure 4 reports the asymptotic and finite-sample CP's of nominal $0.95$
standard $|t|$ and QLR CI's for $\beta $ and $\pi $ when $\pi _{0}=1.5.$ For
example, the smallest asymptotic and finite-sample CP's (over $b$) are
around $0.68$ and $0.93$ for the $|t|$ and QLR CI's for $\beta ,$
respectively. There is no size distortion for the $|t|$ and QLR CI's for $%
\pi .$ Note that the asymptotic CP's provide a good approximation to the
finite-sample CP's. Figure S-7 in Supplemental Appendix E provides analogous
results for $\pi _{0}=3.0.$

% \FRAME{ftbpFU}{3.4832in}{3.4177in}{0pt}{\Qcb{Figure 5. Coverage
% Probabilities of Robust $|t|$ and QLR CI's for $\protect\beta $ and $\protect%
% \pi $ in the Nonlinear Regression Model with Endogeneity when $\protect\pi %
% _{0}=1.5.$ No smooth transition is employed.}}{}{gmm_rob_cp_15.eps}{\special%
% {language "Scientific Word";type "GRAPHIC";maintain-aspect-ratio
% TRUE;display "USEDEF";valid_file "F";width 3.4832in;height 3.4177in;depth
% 0pt;original-width 7.4858in;original-height 6.7464in;cropleft
% "0.0876";croptop "0.9567";cropright "0.9268";cropbottom "0.0432";filename
% 'graphics/GMM_Rob_CP_15.eps';file-properties "XNPEU";}}

Next, we consider CI's that are robust to weak identification. For the
robust CI for $\beta ,$ we impose the null value of $b=n^{1/2}\beta _{0},$
where $\beta _{0}$ is the true value of $\beta $ under the null. With the
knowledge of $b$ under the null, no identification-category-selection
procedure is needed. Imposing the null value of $b$ also results in a
smaller LF critical value. As indicated in Figure 3(a), the NI-LF critical
values for the $|t|$ CI for $\beta $ is attained at $\pi _{0}=1.5$ for all $%
b $ values. In consequence, the robust $|t|$ CI for $\beta $ is
asymptotically similar when $\pi _{0}=1.5,$ as shown in Figure 5(a). Figure
5(a) also reports the finite-sample ($n=500$) CP's of the robust $|t|$ CI
for $\beta .$ The smallest and largest finite-sample CP's are around 0.91
and 0.97, as opposed to 0.68 and 1.00 for the standard $|t|$ CI. Figure 5(b)
shows that the robust QLR CI for $\beta $ tends to over-cover for a range of
small to medium b values, but the asymptotic size is correct. Figures S-8(a)
and S-8(b) in Supplemental Appendix E provide analogous results for $\pi
_{0}=3.0.$ The robust CI's for $\beta $ are not asymptotically similar when $%
\pi _{0}=3.0,$ but they have correct asymptotic size and the asymptotic and
finite-sample CP's are close for all $b$ values.

The robust CI's for $\pi $ are constructed with the null value $\pi _{0}$
imposed. When $\pi _{0}=1.5,$ the robust $|t|$ and QLR CI's are the same as
the standard $|t|$ and QLR CI's, respectively, because the NI-LF critical
values equal the standard critical values in both cases. In consequence,
Figures 5(c) and 5(d) are the same as Figures 4(c) and 4(d), respectively.
The robust $|t|$ and QLR CI's for $\pi $ when $\pi _{0}=3.0$ are reported in
Figures S-8(c) and S-8(d) in Supplemental Appendix E. In this case, the
NI-LF critical value for the robust $|t|$ CI for $\pi $ is slightly larger
than the standard critical value, as shown in Figure 3(c). We apply the
smooth transition in (\ref{Defn of chat(h)}) to obtain critical values for
the robust $|t|$ CI for $\pi ,$ where the transition function is $s(x)=\exp
(-2x)$ and the constants are $\kappa =1.5$ and $D=1.$ The choices of $s(x)$
and $D$ were determined via some experimentation to be good choices in terms
of yielding CP's that are relatively close to the nominal size $0.95$ across
different values of $b.$ A wide range of $\kappa $ values yield similar
results (because the constants $\Delta _{1}$ and $\Delta _{2}$ adjust to
maintain correct asymptotic size as $\kappa $ is changed). Figures S-7(c)
and S-8(c) show that, when $\pi _{0}=3.0,$ the standard $|t|$ CI for $\pi $
suffers from size distortion but the robust $|t|$ CI for $\pi $ has correct
asymptotic size. When $\pi _{0}=3.0,$ the robust QLR CI for $\pi $ is the
same the standard QLR CI for $\pi ,$ as shown in Figures S-7(d) and S-8(d).

Besides $b$ and $\pi _{0},$ the construction of a robust CI also requires
the $\zeta $ value in order to obtain the LF (or NI-LF) critical value
through simulation. In this model, $\zeta =(\zeta _{1},\zeta _{2})^{\prime
}. $ Because $\zeta $ can be consistently estimated, we recommend plugging
in the estimator $\widehat{\zeta }_{n}$ in place of $\zeta _{0}$ in
practice. To ease the computational burden required to simulate the CP's,
the finite-sample CP's of the robust CI's reported in Figures 5 and S-8 are
constructed using the true value $\zeta _{0},$ rather than the estimated
value $\widehat{\zeta }_{n}.$\footnote{%
With a single sample, the computational burden is the same whether the true
value $\zeta _{0}$ or the estimated value $\widehat{\zeta }_{n}$ is
employed. However, in a simulation study, it is much faster to simulate the
critical values for a range of true values of $b$ and $\pi _{0}$ and the
single true value of $\zeta _{0}$ one time and then use them in each of the
simulation repetitions, rather than to simulate a new critical value for
each simulation repetition, which is required if $\widehat{\zeta }_{n}$ is
employed.} However, the difference between the robust CI's constructed with $%
\widehat{\zeta }_{n}$ and $\zeta _{0}$ typically is relatively minor. A
comparison is reported in Table S-1 of AC2 in the context of a smooth
transition autoregressive model.

\newpage

\begin{center}
{\LARGE R}{\Large EFERENCES}
\end{center}

\begin{description}
\item Amemiya, T. (1974) Multivariate regression and simultaneous-equation
models when the dependent variables are truncated normal.\ \emph{Econometrica%
} 42, 999--1012.

\item Andrews, D.W.K. (2002) Generalized method of moments estimation when a
parameter is on a boundary.\ \emph{Journal of Business and Economic
Statistics} 20, 530--544.

\item Andrews, D.W.K. \& X. Cheng (2011a) Maximum likelihood estimation and
uniform inference with sporadic identification failure.\ Cowles Foundation
Discussion Paper No. 1824, Yale University.

\item Andrews, D.W.K. \& X. Cheng (2011b) Supplemental appendices for
\textquotedblleft Generalized method of moments estimation and uniform
subvector inference with possible identification failure.\textquotedblright\
Cowles Foundation Discussion Paper No. 1828, Yale University.

\item Andrews, D.W.K. \& X. Cheng (2012a) Estimation and inference with
weak, semi-strong, and strong identification.\ \emph{Econometrica }80,
forthcoming.

\item Andrews, D.W.K. \& X. Cheng (2012b) Supplemental material for
\textquotedblleft Estimation and inference with weak, semi-strong, and
strong identification.\textquotedblright\ Econometric Society website.

\item Andrews, D.W.K., X. Cheng, \& P. Guggenberger (2009) Generic results
for establishing the asymptotic size of confidence sets and tests.\ Cowles
Foundation Discussion Paper No. 1813, Yale University.

\item Andrews, D.W.K. \& P. Guggenberger (2009) Validity of Subsampling and
`Plug-in Asymptotic' Inference for Parameters Defined by Moment
Inequalities. \emph{Econometric Theory}\textit{\ }25, 669--709.

\item Andrews, D.W.K. \& P. Guggenberger (2010) Asymptotic size and a
problem with subsampling and with the $m$ out of $n$ bootstrap.\ \emph{%
Econometric Theory}\textit{\ }26, 426--468.

\item Andrews, I. \& A. Mikusheva (2011) Maximum likelihood inference in
weakly identified DSGE models.\ Unpublished manuscript, Department of
Economics, MIT.

\item Andrews, I. \& A. Mikusheva (2012) A Geometric Approach to Weakly
Identified Econometric Models.\ Unpublished manuscript, Department of
Economics, MIT.

\item Antoine, B. \& E. Renault (2009) Efficient GMM with nearly weak
instruments.\ \emph{Econometrics Journal }12, S135--S171.

\item Antoine, B. \& E. Renault (2010) Efficient inference with poor
instruments, a general framework. In D. Giles \& A. Ullah (eds.), \emph{%
Handbook of Empirical Economics and Finance. }Taylor and Francis.

\item Areosa, W.D., M. McAleer, and M.C. Medeiros (2011) Moment-Based
Estimation of Smooth Transition Regression Models with Endogenous Variables. 
\emph{Journal of Econometrics }165. 100--111.

\item Caner, M. (2010) Testing, estimation in GMM and CUE with nearly weak
identification.\ \emph{Econometric Reviews} 29, 330--363.

\item Cheng, X. (2008) Robust confidence intervals in nonlinear regression
under weak identification.\ Unpublished working paper, Department of
Economics, Yale University.

\item Choi, I. \& P.C.B. Phillips (1992) Asymptotic and finite sample
distribution theory for IV estimators and tests in partially identified
structural equations.\ \emph{Journal of Econometrics} 51, 113--150.

\item Davies, R. B. (1977) Hypothesis testing when a nuisance parameter is
present only under the alternative. \emph{Biometrika} 64, 247--254.

\item Dufour, J.-M. (1997) Impossibility theorems in econometrics with
applications to structural and dynamic models.\ \emph{Econometrica} 65,
1365--1387.

\item Guggenberger, P., F. Kleibergen, S. Mavroeidis, \& L. Chen (2013) On
the asymptotic sizes of subset Anderson-Rubin and Lagrange multiplier tests
in linear instrumental variables regression. \emph{Econometrica,}
forthcoming.

\item Hansen, L.P. (1982) Large sample properties of generalized method of
moments estimation.\ \emph{Econometrica} 50, 1029--1054.

\item Heckman, J.J. (1978) Dummy endogenous variables in a simultaneous
equation system.\ \emph{Econometrica} 46, 931--959.

\item Kleibergen, F. (2002) Pivotal statistics for testing structural
parameters in instrumental variables regression.\ \emph{Econometrica} 70,
1781--1803.

\item Kleibergen, F. (2005) Testing parameters in GMM without assuming that
they are identified.\ \emph{Econometrica} 73, 1103--1123.Lee, L.F. (1981)
Simultaneous equations models with discrete endogenous variables. In C.F.
Manski \& D. McFadden (eds.), \emph{Structural Analysis of Discrete Data and
Econometric Applications}. MIT Press.

\item Lee, L.-F. \& A. Chesher (1986) Specification testing when score test
statistics are identically zero.\ \emph{Journal of Econometrics }31,
121--149.

\item Ma, J. \& C.R. Nelson (2008) Valid inference for a class of models
where standard inference performs poorly; including nonlinear regression,
ARMA, GARCH, and unobserved components. Unpublished manuscript, Department
of Economics, U. of Washington.

\item Moreira, M.J. (2003) A conditional likelihood ratio test for
structural models.\ \emph{Econometrica} 71, 1027--1048.

\item Nelson, C.R. \& R. Startz (1990) Some further results on the exact
small sample properties of the instrumental variables estimator.\ \emph{%
Econometrica} 58, 967--976.

\item Nelson, C.R. \& R. Startz (2007) The zero-information-limit condition
and spurious inference in weakly identified models.\ \emph{Journal of
Econometrics} 138, 47--62.

\item Nelson, F. \& L. Olson (1978) Specification and estimation of a
simultaneous-equation model with limited dependent variables.\ \emph{%
International Economic Review }19, 695--709.

\item Pakes, A., \& D. Pollard (1989) Simulation and the asymptotics of
optimization estimators.\ \emph{Econometrica} 57, 1027--1057.

\item Park, J.Y. \& P.C.B. Phillips (1988) Statistical inference in
regressions with integrated processes: part 1.\ \emph{Econometric Theory }4,
468--497.

\item Phillips, P.C.B. (1989) Partially identified econometric models. \emph{%
Econometric Theory} 5, 181--240.

\item Rotnitzky, A., D.R. Cox, M. Bottai, \& J. Robins (2000)
Likelihood-based inference with singular information matrix.\ \emph{%
Bernoulli }6, 243--284.

\item Qu, Z. (2011) Inference and specification testing in DSGE models with
possible weak identification. Unpublished working paper, Department of
Economics, Boston University.

\item Rivers, D. \& Q.H. Vuong (1988) Limited information estimators and
exogeneity tests for simultaneous probit models.\ \emph{Journal of
Econometrics }39, 347--366.

\item Sargan, J.D. (1983) Identification and lack of identification. \emph{%
Econometrica} 51, 1605--1633.

\item Schorfheide, F. (2011) Estimation and evaluation of DSGE models:
progress and challenges. NBER Working Paper 16781.

\item Shi, X. \& P.C.B. Phillips (2011) Nonlinear cointegrating regression
under weak identification.\ \emph{Econometric Theory} 28, 1--39.

\item Smith, R.J. \& R.W. Blundell (1986) An exogeneity test for a
simultaneous equation tobit model with an application to labor supply.\ 
\emph{Econometrica} 54, 679--685.

\item Staiger, D. \& J.H. Stock (1997) Instrumental variables regression
with weak instruments.\ \emph{Econometrica} 65, 557--586.

\item Stock, J.H. \& J.H. Wright (2000) GMM with weak instruments.\ \emph{%
Econometrica }68, 1055--1096.\newpage 
\end{description}

\begin{center}
%TCIMACRO{\TeXButton{empty page}{\thispagestyle{empty}}}%
%BeginExpansion
\thispagestyle{empty}%
%EndExpansion
$\vspace*{1.95cm}$

{\LARGE Supplemental Appendices}$\bigskip $

{\Large for}$\bigskip $

{\LARGE GMM Estimation and\medskip }

{\LARGE Uniform Subvector Inference\medskip }

{\LARGE with Possible Identification Failure}$\vspace*{1.95cm}$

{\large Donald W. K. Andrews}

{\large Cowles Foundation for Research in Economics}

{\large Yale University}$\bigskip $

{\large Xu Cheng}

{\large Department of Economics}

{\large University of Pennsylvania}$\vspace*{0.75cm}$

{\large First Draft: August, 2007}

{\large Revised: \today}\bigskip
\end{center}

\newpage

\section{ \hspace{-0.34in}\textbf{.}\hspace{0.2in}Outline}

%TCIMACRO{\TeXButton{pageno}{\setcounter{page}{1}}}%
%BeginExpansion
\setcounter{page}{1}%
%EndExpansion
\hspace{0.25in}This Supplement includes five Supplemental Appendices
(denoted A-E) to the paper \textquotedblleft GMM Estimation and Uniform
Subvector Inference with Possible Identification Failure,\textquotedblright\
denoted hereafter by AC3. Supplemental Appendix A verifies the assumptions
of AC3 for the probit model with endogeneity. Supplemental Appendix B
provides proofs of the GMM estimation results given in Section \ref%
{Estimation Results Sec} of AC3. It also provides some results for minimum
distance estimators. Supplemental Appendix C provides proofs of the Wald
test and CS results given in Section \ref{Wald Tests Sec} of AC3.
Supplemental Appendix D gives some results that are used in the verification
of the assumptions for the two examples of AC3. Supplemental Appendix E
provides additional numerical results to those provided in AC3 for the
nonlinear regression model with endogeneity.

\section{ \hspace{-0.34in}\textbf{.}\hspace{0.2in}Supplemental Appendix A:
Probit Model with Endogeneity: Verification of Assumptions\label{Example 2
Verif of As.s Sec}}

\setcounter{equation}{0}\hspace{0.25in}In this Supplemental Appendix, we
verify Assumptions GMM1-GMM5 and V1-V2 for the probit model with endogeneity
and possibly weak instruments. Assumptions B1 and B2 hold immediately in
this model given the definitions of $\Theta ,$ $\Theta ^{\ast },$ and $\Phi
^{\ast }(\theta )$ in Section \ref{Simul Probit Ex Sec} of AC3.

\subsection{\hspace{-0.23in}\textbf{.}\hspace{0.18in}Verification of
Assumption GMM1}

\hspace{0.25in}Assumption GMM1(i) holds by (\ref{Probit-SmplCrit}) and (\ref%
{Probit-Weight}) because $Z_{i}^{\prime }\beta \pi $ does not depend on $\pi 
$ when $\beta =0.$

The quantity $g_{0}(\theta ;\gamma )$ that appears in Assumptions
GMM1(ii)-(v) is 
\begin{eqnarray}
g_{0}(\theta ;\gamma _{0})\hspace{-0.08in} &=&\hspace{-0.08in}E_{\gamma
_{0}}e_{i}(\theta )\otimes \overline{Z}_{i}=E_{\gamma _{0}}e_{0,i}(\theta
)\otimes \overline{Z}_{i},\text{ where}  \notag \\
e_{0,i}(\theta )\hspace{-0.08in} &=&\hspace{-0.08in}\binom{w_{1,i}(\theta
)(L_{i}(\theta _{0})-L_{i}(\theta ))}{Z_{i}^{\prime }(\beta _{0}-\beta
)-X_{i}^{\prime }(\zeta _{2,0}-\zeta _{2})}\in R^{2}.
\end{eqnarray}%
The first uniform convergence condition in Assumption GMM1(ii) follows from
the ULLN given in Lemma \ref{Lemma uniform convergence} in Supplemental
Appendix D because $E_{\gamma _{0}}(y_{i}|X_{i},Z_{i})=L_{i}(\theta _{0})$
when the true value is $\gamma _{0}=(\theta _{0},\phi _{0}).$

When $\mathcal{W}_{n}(\theta )$ is the identity matrix, $\mathcal{W}(\theta
;\gamma _{0})$ in Assumption GMM1(ii) also is the identity matrix. When $%
\mathcal{W}_{n}(\theta )$ is the optimal weight matrix defined in (\ref%
{Probit-Weight}), Assumption GMM1(ii) holds with 
\begin{eqnarray}
\mathcal{W}(\theta ;\gamma _{0})\hspace{-0.08in} &=&\hspace{-0.08in}%
E_{\gamma _{0}}\left( e_{i}(\theta )e_{i}(\theta )^{\prime }\right) \otimes (%
\overline{Z}_{i}\overline{Z}_{i}^{\prime })=E_{\gamma _{0}}(\mathcal{W}%
_{e,i}(\theta ;\gamma _{0})\otimes (\overline{Z}_{i}\overline{Z}_{i}^{\prime
}),\text{ where}  \notag \\
\mathcal{W}_{e,i}(\theta ;\gamma _{0})\hspace{-0.08in} &=&\hspace{-0.08in}%
E_{\gamma _{0}}\left( e_{i}(\theta )e_{i}(\theta )^{\prime }|\overline{Z}%
_{i}\right) =\left( 
\begin{array}{cc}
\mathcal{W}_{11,i}(\theta ) & \mathcal{W}_{12,i}(\theta ) \\ 
\mathcal{W}_{12,i}(\theta ) & \mathcal{W}_{22,i}(\theta )%
\end{array}%
\right)  \label{Probit_WeightMatrix}
\end{eqnarray}%
and $\mathcal{W}_{11,i}(\theta ),\mathcal{W}_{12,i}(\theta ),$ and $\mathcal{%
W}_{22,i}(\theta )$ are defined in (\ref{Probit_W11})-(\ref{Probit_W22})
below.\footnote{%
Note that $\mathcal{W}_{11,i}(\theta ),\mathcal{W}_{12,i}(\theta ),$ and $%
\mathcal{W}_{22,i}(\theta )$ all depend on $\gamma _{0}.$ We omit $\gamma
_{0}$ from these terms for notational simplicity.} The convergence condition
in Assumption GMM1(ii) holds for the optimal weight matrix $\mathcal{W}%
_{n}(\theta )$ by the ULLN given in Lemma \ref{Lemma uniform convergence} in
Supplemental Appendix C.

Now we derive the elements of $\mathcal{W}_{e,i}(\theta ;\gamma _{0})$ in (%
\ref{Probit_WeightMatrix}). Note that 
\begin{equation}
P_{\gamma _{0}}(y_{i}=1|\overline{Z}_{i})=L_{i}(\theta _{0})\text{ and }%
P_{\gamma _{0}}(y_{i}=0|\overline{Z}_{i})=1-L_{i}(\theta _{0}).
\end{equation}%
The upper left element of $\mathcal{W}_{e,i}(\theta ;\gamma _{0})$ is%
\begin{equation}
\mathcal{W}_{11,i}(\theta )=E_{\gamma _{0}}(w_{1,i}(\theta
)^{2}(y_{i}-L_{i}(\theta ))^{2}|\overline{Z}_{i})=w_{1,i}(\theta
)^{2}(L_{i}(\theta _{0})-2L_{i}(\theta _{0})L_{i}(\theta )+L_{i}(\theta
)^{2}).  \label{Probit_W11}
\end{equation}%
The lower-right element of $\mathcal{W}_{e,i}(\theta ;\gamma _{0})$ is%
\begin{equation}
\mathcal{W}_{22,i}(\theta )=E_{\gamma _{0}}((Y_{i}-Z_{i}^{\prime }\beta
-X_{i}^{\prime }\zeta _{2})^{2}|\overline{Z}_{i})=\sigma
_{v}^{2}+(Z_{i}^{\prime }(\beta _{0}-\beta )+X_{i}^{\prime }(\zeta
_{2,0}-\zeta _{2}))^{2}.  \label{Probit_W22}
\end{equation}

To calculate the off-diagonal term of $\mathcal{W}_{e,i}(\theta ;\gamma
_{0}),$ note that%
\begin{eqnarray}
E_{\gamma _{0}}(V_{i}|\overline{Z}_{i},y_{i}\hspace{-0.08in} &=&\hspace{%
-0.08in}1)=E_{\gamma _{0}}(V_{i}|\overline{Z}_{i},U_{i}>-(Z_{i}^{\prime
}\beta _{0}\pi _{0}+X_{i}^{\prime }\zeta _{1,0}))=\sigma _{v}\rho \frac{%
L_{i}^{\prime }(\theta _{0})}{L_{i}(\theta _{0})}\text{ and}  \notag \\
E_{\gamma _{0}}(V_{i}|\overline{Z}_{i},y_{i}\hspace{-0.08in} &=&\hspace{%
-0.08in}0)=E_{\gamma _{0}}(V_{i}|\overline{Z}_{i},-U_{i}>Z_{i}^{\prime
}\beta _{0}\pi _{0}+X_{i}^{\prime }\zeta _{1,0})=-\sigma _{v}\rho \frac{%
L_{i}^{\prime }(\theta _{0})}{1-L_{i}(\theta _{0})}\text{.}
\end{eqnarray}%
The off-diagonal term of $\mathcal{W}_{e,i}(\theta ;\gamma _{0})$ is%
\begin{eqnarray}
&&\mathcal{W}_{12,i}(\theta )  \notag \\
\hspace{-0.08in} &=&\hspace{-0.08in}E_{\gamma _{0}}(w_{1,i}(\theta
)(y_{i}-L_{i}(\theta ))(Y_{i}-Z_{i}^{\prime }\beta -X_{i}^{\prime }\zeta
_{2})|\overline{Z}_{i})  \notag \\
\hspace{-0.08in} &=&\hspace{-0.08in}w_{1,i}(\theta
)\sum_{k=0,1}(k-L_{i}(\theta ))\left[ E_{\gamma _{0}}(V_{i}|\overline{Z}%
_{i},y_{i}=k)+Z_{i}^{\prime }(\beta _{0}-\beta )+X_{i}^{\prime }(\zeta
_{2,0}-\zeta _{2})\right] P_{\gamma _{0}}(y_{i}=k|\overline{Z}_{i})  \notag
\\
\hspace{-0.08in} &=&\hspace{-0.08in}w_{1,i}(\theta )\left[ (1-L_{i}(\theta
))\sigma _{v}\rho \frac{L_{i}^{\prime }(\theta _{0})}{L_{i}(\theta _{0})}%
L_{i}(\theta _{0})+L_{i}(\theta )\sigma _{v}\rho \frac{L_{i}^{\prime
}(\theta _{0})}{1-L_{i}(\theta _{0})}(1-L_{i}(\theta _{0})\right] +  \notag
\\
&&w_{1,i}(\theta )\left[ (1-L_{i}(\theta ))L_{i}(\theta _{0})-L_{i}(\theta
)(1-L_{i}(\theta _{0}))\right] \left[ Z_{i}^{\prime }(\beta _{0}-\beta
)+X_{i}^{\prime }(\zeta _{2,0}-\zeta _{2})\right]   \notag \\
\hspace{-0.08in} &=&\hspace{-0.08in}w_{1,i}(\theta )\left[ \sigma _{v}\rho
L_{i}^{\prime }(\theta _{0})+\left( L_{i}(\theta _{0})-L_{i}(\theta )\right)
\left( Z_{i}^{\prime }(\beta _{0}-\beta )+X_{i}^{\prime }(\zeta _{2,0}-\zeta
_{2})\right) \right] .  \label{Probit_W12}
\end{eqnarray}

Now we verify Assumptions GMM1(iii) and GMM1(iv). We write $g_{0}(\theta
;\gamma _{0})=$\linebreak $(g_{1,0}(\theta ;\gamma _{0})^{\prime
},g_{2,0}(\theta ;\gamma _{0})^{\prime })^{\prime }$ for $g_{j,0}(\theta
;\gamma _{0})\in R^{d_{X}+d_{Z}}$ for $j=1,2.$ We have 
\begin{equation*}
g_{2,0}(\theta ;\gamma _{0})\xi =\xi ^{\prime }E_{\gamma _{0}}\overline{Z}%
_{i}\overline{Z}_{i}^{\prime }\xi >0\text{ for }\xi =((\beta _{0}-\beta
)^{\prime },(\zeta _{2,0}-\zeta _{2}))^{\prime },
\end{equation*}%
where the inequality holds because $E_{\gamma _{0}}\overline{Z}_{i}\overline{%
Z}_{i}^{\prime }$ is positive definite since $P_{\phi _{0}}(\overline{Z}%
_{i}^{\prime }c=0)<1$ for any $c\neq 0$ by (\ref{Simul Probit Phi Space}).
Hence, $g_{2,0}(\theta ;\gamma _{0})=0$ if and only if $\beta =\beta _{0}$
and $\zeta _{2}=\zeta _{2,0}.$ Now, for $\theta $ with $\beta =\beta _{0}$
and $\zeta _{2}=\zeta _{2,0},$ 
\begin{equation}
g_{1,0}(\theta ;\gamma _{0})=E_{\gamma _{0}}w_{1,i}(\theta )(L_{i}(\theta
_{0})-L_{i}(\theta ))\overline{Z}_{i}\text{ and }L_{i}(\theta
)=L(Z_{i}^{\prime }\beta _{0}\pi +X_{i}^{\prime }\zeta _{1}).
\end{equation}%
If $\beta _{0}\neq 0,$ the conditions $g_{1,0}(\theta ;\gamma _{0})=0$ are
more restrictive than the populations first-order conditions for the
standard probit ML estimator for a probit model with regression function $%
Z_{i}^{\prime }\beta _{0}\pi +X_{i}^{\prime }\zeta _{1}$ (because the latter
has the multiplicative factor $(Z_{i}^{\prime }\beta _{0},X_{i}^{\prime
})^{\prime },$ rather than $\overline{Z}_{i}$). The latter have a unique
solution at the true parameter vector because, as is well known, the
population log likelihood function of the probit model is strictly concave.
Hence, $g_{1,0}(\theta ;\gamma _{0})=0$ only if $\pi =\pi _{0}$ and $\zeta
_{1}=\zeta _{1,0}$ and Assumption GMM1(iv) holds. If $\beta _{0}=0,$ then
the same argument holds but with the regression function being $%
X_{i}^{\prime }\zeta _{1},$ rather than $Z_{i}^{\prime }\beta _{0}\pi
+X_{i}^{\prime }\zeta _{1}.$ In this case, $g_{1,0}(\theta ;\gamma _{0})=0$
only if $\zeta _{1}=\zeta _{1,0}$ and Assumption GMM1(iii) holds.

The partial derivatives $g_{\psi }(\theta ;\gamma _{0})$ and $g_{\theta
}(\theta ;\gamma _{0})$ in Assumptions GMM1(v) and GMM1\linebreak (viii) are 
\begin{eqnarray}
g_{\psi }(\theta ;\gamma _{0})\hspace{-0.08in} &=&\hspace{-0.08in}E_{\phi
_{0}}\binom{\overline{Z}_{i}a_{i}(\theta )d_{1\psi ,i}(\pi )^{\prime }}{%
\overline{Z}_{i}d_{2\psi ,i}^{\prime }}\text{ and }g_{\theta }(\theta
;\gamma _{0})=E_{\phi _{0}}\binom{\overline{Z}_{i}a_{i}(\theta
)d_{1,i}(\theta )^{\prime }}{\overline{Z}_{i}d_{2,i}^{\prime }},\text{ where}
\notag \\
d_{1\psi ,i}(\pi )\hspace{-0.08in} &=&\hspace{-0.08in}(\pi
Z_{i},X_{i},0_{d_{X}})\in R^{d_{Z}+2d_{X}},\text{ }d_{2\psi
,i}=(Z_{i},0_{d_{X}},X_{i})\in R^{d_{Z}+2d_{X}},  \notag \\
d_{1,i}(\theta )\hspace{-0.08in} &=&\hspace{-0.08in}(d_{1\psi ,i}(\pi
),Z_{i}^{\prime }\beta )\in R^{d_{Z}+2d_{X}+1},\text{ }d_{2,i}=(d_{2\psi
,i},0)\in R^{d_{Z}+2d_{X}+1},\text{ and}  \label{Probit_1st derivative} \\
a_{i}(\theta )\hspace{-0.08in} &=&\hspace{-0.08in}\frac{L_{i}^{\prime
}(\theta )^{2}+L_{i}^{\prime \prime }(\theta )(L_{i}(\theta )-L_{i}(\theta
_{0}))}{L_{i}(\theta )(1-L_{i}(\theta ))}-\frac{L_{i}^{\prime }(\theta
)^{2}(L_{i}(\theta )-L_{i}(\theta _{0}))(1-2L_{i}(\theta ))}{L_{i}(\theta
)^{2}(1-L_{i}(\theta ))^{2}}.  \notag
\end{eqnarray}%
Assumptions GMM1(v) and GMM1(vi) hold by the continuity of $w_{1,i}(\theta )$
and $L_{i}(\theta )$ in $\theta $ and the moment conditions in (\ref{Simul
Probit Phi Space}).

Next, we verify Assumption GMM1(vii). To show $\lambda _{\min }(\mathcal{W}%
(\psi _{0},\pi ;\gamma _{0}))>0,$ $\forall \pi \in \Pi ,$ $\forall \gamma
_{0}\in \Gamma ,$ we show that for any $c=(c_{1}^{\prime },c_{2}^{\prime
})^{\prime }$ with $||c||>0,$ $c^{\prime }\mathcal{W}(\psi _{0},\pi ;\gamma
_{0})c>0,$ where $c_{j}\in R^{d_{X}+d_{Z}}$ for $j=1,2.$ Let%
\begin{equation}
U_{i}^{\ast }(\theta )=w_{1,i}(\theta )(U_{i}+L_{i}(\theta
_{0})-L_{i}(\theta )).  \label{GMM1(vii) 1}
\end{equation}%
For $\theta \in (\psi _{0},\pi ),$ we have%
\begin{eqnarray}
c^{\prime }\mathcal{W}(\psi _{0},\pi ;\gamma _{0})c\hspace{-0.08in} &=&%
\hspace{-0.08in}c^{\prime }\left[ E_{\gamma _{0}}\left( 
\begin{array}{c}
U_{i}^{\ast }(\theta ) \\ 
V_{i}%
\end{array}%
\right) \left( 
\begin{array}{c}
U_{i}^{\ast }(\theta ) \\ 
V_{i}%
\end{array}%
\right) ^{\prime }\otimes \overline{Z}_{i}\overline{Z}_{i}^{\prime }\right] c
\notag \\
&=&\hspace{-0.08in}E_{\gamma _{0}}E_{\gamma _{0}}((U_{i}^{\ast }(\theta
)c_{1}^{\prime }\overline{Z}_{i}+V_{i}c_{2}^{\prime }\overline{Z}_{i})^{2}|%
\overline{Z}_{i})  \notag \\
&\geq &\hspace{-0.08in}E_{\gamma _{0}}E_{\gamma _{0}}((U_{i}w_{1,i}(\theta
)c_{1}^{\prime }\overline{Z}_{i}+V_{i}c_{2}^{\prime }\overline{Z}_{i})^{2}|%
\overline{Z}_{i}),  \label{c'Wc}
\end{eqnarray}%
where the inequality holds because $E_{\gamma _{0}}(w_{1,i}(\theta
)(L_{i}(\theta _{0})-L_{i}(\theta ))c_{1}^{\prime }\overline{Z}%
_{i}V_{i}c_{2}^{\prime }\overline{Z}_{i}|\overline{Z}_{i})=0$ a.s. since $%
E_{\gamma _{0}}(V_{i}|\overline{Z}_{i})=0$ a.s. and $E_{\gamma
_{0}}((w_{1,i}(\theta )(L_{i}(\theta _{0})-L_{i}(\theta ))c_{1}^{\prime }%
\overline{Z}_{i})^{2}|\overline{Z}_{i})\geq 0$ a.s. The rhs of (\ref{c'Wc})
equals zero only if $E_{\gamma _{0}}((U_{i}w_{1,i}(\theta )c_{1}^{\prime }%
\overline{Z}_{i}+\allowbreak V_{i}c_{2}^{\prime }\overline{Z}_{i})^{2}|%
\overline{Z}_{i})=0$ a.s. But,%
\begin{equation}
E_{\gamma _{0}}((U_{i}w_{1,i}(\theta )c_{1}^{\prime }\overline{Z}%
_{i}+V_{i}c_{2}^{\prime }\overline{Z}_{i})^{2}|\overline{Z}_{i})>0
\label{GMM1(vii) 3}
\end{equation}%
for all $\overline{Z}_{i}$ for which $c_{j}^{\prime }\overline{Z}_{i}\neq 0$
for $j=1$ and $j=2$ because $w_{1,i}(\theta )>0$ a.s., $(U_{i},V_{i})$ is
independent of $\overline{Z}_{i},$ and $|Cov(U_{i},V_{i})|=|\rho |<1.$ By (%
\ref{Simul Probit Phi Space}), $P_{\gamma _{0}}(c_{j}^{\prime }\overline{Z}%
_{i}\neq 0$ for $j=1$ and $j=2)>0.$ Hence, we conclude that $c^{\prime }%
\mathcal{W}(\psi _{0},\pi ;\gamma _{0})c>0.$

In addition, $\lambda _{\max }(\mathcal{W}(\psi _{0},\pi ;\gamma
_{0}))<\infty $ because $||\mathcal{W}(\psi _{0},\pi ;\gamma
_{0})||=||E_{\phi _{0}}[\mathcal{W}_{e,i}(\theta ;\gamma _{0})\otimes (%
\overline{Z}_{i}\overline{Z}_{i}^{\prime })]||<\infty $ using (\ref%
{Probit_W11})-(\ref{Probit_W22}) and $E_{\phi _{0}}(||\overline{Z}%
_{i}||^{4+\varepsilon }+\overline{w}_{1,i}^{4+\varepsilon })<\infty $ for
some $\varepsilon >0$ by (\ref{Simul Probit Phi Space}), where $||\cdot ||$
denotes the Frobenious norm. Thus, Assumption GMM1(vii) holds.

Assumption GMM1(viii) holds because $\mathcal{W}(\psi _{0},\pi ;\gamma _{0})$
is non-singular $\forall \pi \in \Pi $ and $g_{\psi }(\psi _{0},\pi ;\gamma
_{0})$ has full column rank because $P_{\phi _{0}}(\overline{Z}_{i}^{\prime
}c=0)<1$ for all $c\neq 0.$

Assumption GMM1(ix) holds automatically by the Assumptions on the parameter
space.

Assumption GMM1(x) holds because $\Psi (\pi )$ does not depend on $\pi $ in
this example.

\subsection{\hspace{-0.23in}\textbf{.}\hspace{0.18in}Verification of
Assumption GMM2}

\hspace{0.25in}We verify Assumption GMM2 using the sufficient condition
Assumption GMM2$^{\ast }.$ Assumption GMM2$^{\ast }$(i) holds because $%
e_{i}(\theta )$ is continuously differentiable in $\theta .$ Assumption GMM2$%
^{\ast }$(ii) holds by the ULLN given in Lemma \ref{Lemma uniform
convergence} in Supplemental Appendix C. Assumption GMM2$^{\ast }$(iii)
holds by the uniform LLN given in Lemma \ref{Lemma uniform convergence} in
Supplemental Appendix D using $||\beta ||/||\beta _{n}||=1+o(1)$ for $\theta
\in \Theta _{n}(\delta _{n})$ and $||\beta _{n}||\neq 0$ for $n$ large for $%
\{\gamma _{n}\}\in \Gamma (\gamma _{0},\infty ,\omega _{0}).$

\subsection{\hspace{-0.23in}\textbf{.}\hspace{0.18in}Verification of
Assumption GMM3}

\hspace{0.25in}Assumption GMM3(i) holds with 
\begin{equation}
g(W_{i},\theta )=e_{i}(\theta )\otimes \overline{Z}_{i}.
\end{equation}%
Assumption GMM3(ii) holds because $E_{\gamma ^{\ast }}g(W_{i},\psi ^{\ast
},\pi )=E_{\gamma ^{\ast }}e_{0,i}(\psi ^{\ast },\pi )\otimes \overline{Z}%
_{i}=0$ when $\beta ^{\ast }=0.$

Assumption GMM3(iii) hold by the CLT for triangular arrays of row-wise
i.i.d. random variables given in Lemma \ref{Lemma CLT, array} of
Supplemental Appendix D. The variance matrix is%
\begin{eqnarray}
\Omega _{g}(\gamma _{0})\hspace{-0.08in} &=&\hspace{-0.08in}E_{\gamma
_{0}}\left( e_{i}(\theta _{0})e_{i}(\theta _{0})^{\prime }\right) \otimes (%
\overline{Z}_{i}\overline{Z}_{i}^{\prime })=\mathcal{W}(\theta _{0};\gamma
_{0})  \notag \\
&=&\hspace{-0.08in}E_{\gamma _{0}}\left( 
\begin{array}{cc}
w_{1,i}(\theta _{0})L_{i}^{\prime }(\theta _{0}) & w_{1,i}(\theta
_{0})L_{i}^{\prime }(\theta _{0})\rho \sigma _{v} \\ 
w_{1,i}(\theta _{0})L_{i}^{\prime }(\theta _{0})\rho \sigma _{v} & \sigma
_{v}^{2}%
\end{array}%
\right) \otimes \left( \overline{Z}_{i}\overline{Z}_{i}^{\prime }\right) ,
\label{Probit_Omega}
\end{eqnarray}%
where the second and third equalities follow from (\ref{Probit_WeightMatrix}%
) and (\ref{Probit_W11})-(\ref{Probit_W22}) with $\theta =\theta _{0}$ and $%
w_{1,i}(\theta _{0})(L_{i}(\theta _{0})$\allowbreak $-L_{i}(\theta
_{0})^{2})\allowbreak =L_{i}^{\prime }(\theta _{0}).$

To verify Assumption GMM3(iv), first note that%
\begin{equation}
E_{\gamma ^{\ast }}g(W_{i},\theta )=E_{\gamma ^{\ast }}\binom{w_{1,i}(\theta
)(L_{i}(\theta ^{\ast })-L_{i}(\theta ))}{Z_{i}^{\prime }(\beta ^{\ast
}-\beta )-X_{i}^{\prime }(\zeta _{2}^{\ast }-\zeta _{2})}\otimes \overline{Z}%
_{i}.  \label{Expected g}
\end{equation}%
The derivative of $E_{\gamma ^{\ast }}g(W_{i},\theta )$ wrt $\beta ^{\ast }$
is%
\begin{equation}
K_{n,g}(\theta ;\gamma ^{\ast })=E_{^{\phi \ast }}\left( 
\begin{array}{c}
w_{1,i}(\theta )L_{i}^{\prime }(\theta ^{\ast })\pi ^{\ast }\overline{Z}%
_{i}Z_{i}^{\prime } \\ 
\overline{Z}_{i}Z_{i}^{\prime }%
\end{array}%
\right)  \label{Probit Kg}
\end{equation}%
$\forall (\theta ,\gamma ^{\ast })\in \Theta _{\delta }\times \Gamma _{0}$
and $\forall n\geq 1.$ This verifies Assumption GMM3(iv)(a). Assumptions
GMM3(iv)(b) and (c) hold with $K_{g}(\theta ;\gamma _{0})=K_{n,g}(\theta
;\gamma _{0}).$

To verify Assumption GMM3(v), note that $a_{i}(\psi _{0},\pi
)=w_{1,i}(\theta _{0})L_{i}^{\prime }(\theta _{0})$ when $\beta _{0}=0.$
Using (\ref{Probit_1st derivative}) and (\ref{Probit Kg}), this yields%
\begin{eqnarray}
g_{\psi }(\psi _{0},\pi ;\gamma _{0})\hspace{-0.08in} &=&\hspace{-0.08in}%
E_{\phi _{0}}M_{i}(\theta _{0})\left( 
\begin{array}{c}
d_{1\psi ,i}(\pi )^{\prime } \\ 
d_{2\psi ,i}^{\prime }%
\end{array}%
\right) ,\text{ }K_{g}(\psi _{0},\pi ;\gamma _{0})=E_{\phi _{0}}M_{i}(\theta
_{0})\left( 
\begin{array}{c}
\pi _{0}Z_{i}^{\prime } \\ 
Z_{i}^{\prime }%
\end{array}%
\right) ,\text{ where}  \notag \\
M_{i}(\theta _{0})\hspace{-0.08in} &=&\hspace{-0.08in}\left( 
\begin{array}{cc}
w_{1,i}(\theta _{0})L_{i}^{\prime }(\theta _{0})\overline{Z}_{i} & 0_{d_{Z}}
\\ 
0_{d_{Z}} & \overline{Z}_{i}%
\end{array}%
\right) .  \label{Probit_g_psi}
\end{eqnarray}%
Assumption GMM3(v) holds because (i) $M_{i}(\theta _{0})$ has full rank
a.s., (ii) $d_{2\psi ,i}S=Z_{i}^{\prime }$ for $S=(S_{1},S_{2},S_{3})\in
R^{d_{Z}\times d_{X}\times d_{X}}$ if and only if $S_{1}=1_{d_{Z}}$ and $%
S_{3}=0_{d_{X}},$ and (iii) $d_{1\psi ,i}(\pi )S=\pi _{0}Z_{i}$ for $%
S=(1_{d_{Z}},S_{2},0_{d_{X}})$ if and only if $S_{2}=0_{d_{X}}$ and $\pi
=\pi _{0}.$

Assumption GMM3(vi) holds by (\ref{Expected g}), (\ref{Probit_g_psi}), an
exchange of \textquotedblleft $E$\textquotedblright\ and \textquotedblleft $%
\partial ,$\textquotedblright\ the moment conditions in (\ref{Simul Probit
Phi Space}), and some calculations. The left-hand side does not depend on an
average over $n$ because the observations are identically distributed.

\subsection{\hspace{-0.23in}\textbf{.}\hspace{0.18in}Verification of
Assumption GMM4}

\hspace{0.25in}When $d_{Z}>1,$ we do not have a proof that Assumption GMM4
holds. In this case, we just assume that it does. However, when $d_{Z}=1,$
Assumption GMM4 can be verified by verifying Assumption GMM4$^{\ast }.$ In
this case, Assumption GMM4$^{\ast }$(i) holds automatically. Using (\ref%
{Probit_g_psi}), we obtain%
\begin{equation}
g_{\psi }^{\ast }(\psi _{0},\pi _{1},\pi _{2};\gamma _{0})=E_{\phi
_{0}}M_{i}(\theta _{0})\left( 
\begin{array}{c}
\pi _{1}Z_{i}^{\prime },\pi _{2}Z_{i}^{\prime },X_{i}^{\prime
},0_{d_{X}}^{\prime } \\ 
Z_{i}^{\prime },Z_{i}^{\prime },0_{d_{X}}^{\prime },X_{i}^{\prime }%
\end{array}%
\right) ,
\end{equation}%
where $M_{i}(\theta _{0})$ is of full column rank a.s. Assumption GMM4$%
^{\ast }$(ii) holds because $P_{\phi _{0}}(\overline{Z}_{i}^{\prime }c=0)<1$
for $c\neq 0$ and $\pi _{1}\neq \pi _{2}.$ Assumption GMM4$^{\ast }$(iii)
holds with $\Omega _{g}(\gamma _{0})=\mathcal{W}(\theta _{0};\gamma _{0})$
by (\ref{Probit_WeightMatrix}) and (\ref{Probit_Omega}) because $\mathcal{W}%
(\theta _{0};\gamma _{0})$ is positive definite by the verification of
Assumption GMM1(vii) in (\ref{GMM1(vii) 1})-(\ref{GMM1(vii) 3}).

\subsection{\hspace{-0.23in}\textbf{.}\hspace{0.18in}Verification of
Assumption GMM5}

\hspace{0.25in}The verification of Assumption GMM5(i) is analogous to that
of Assumption GMM3\linebreak (iii). The variance matrix $V_{g}(\gamma _{0})$
is equal to $\Omega _{g}(\gamma _{0})$ defined in (\ref{Probit_Omega}).

Assumption GMM5(ii) holds with $g_{\theta }(\theta ;\gamma _{0})$ in (\ref%
{Probit_1st derivative}) using $||\beta ||/||\beta _{n}||=1+o(1)$ for $%
\theta \in \Theta _{n}(\delta _{n}),$ $||\beta _{n}||\neq 0$ for $n$ large
for $\{\gamma _{n}\}\in \Gamma (\gamma _{0},\infty ,\omega _{0}),$ and the
moment conditions in (\ref{Simul Probit Phi Space}).

Assumption GMM5(iii) holds with 
\begin{equation}
J_{g}(\gamma _{0})=E_{\phi _{0}}M_{i}(\theta _{0})\binom{\pi
_{0}Z_{i}^{\prime },X_{i}^{\prime },0_{d_{X}}^{\prime },Z_{i}^{\prime
}\omega _{0}}{Z_{i}^{\prime },0_{d_{X}}^{\prime },X_{i}^{\prime },0}
\end{equation}%
using (\ref{Probit_1st derivative}) and (\ref{Probit_g_psi}) and $\beta
_{n}/||\beta _{n}||\rightarrow \omega _{0}.$ The matrix $J_{g}(\gamma _{0})$
has full column rank because $P_{\phi }(\overline{Z}_{i}^{\prime }c=0)<1$
for $c\neq 0.$

\subsection{\hspace{-0.23in}\textbf{.}\hspace{0.18in}Verification of
Assumptions V1 and V2 (Vector $\mathbf{\protect\beta }$)}

\hspace{0.25in}Here we verify Assumptions V1(i)-V1(iii) (vector $\beta $)
and V2. We do not verify Assumption V1(iv) (vector $\beta $). However, it
should hold because $\tau _{\beta }(\pi ;\gamma _{0},b)$ is a Gaussian
process.

We estimate $J(\gamma _{0})$ and $V(\gamma _{0})$ by $\widehat{J}_{n}=%
\widehat{J}_{n}(\widehat{\theta }_{n}^{+})$ and $\widehat{V}_{n}=\widehat{V}%
_{n}(\widehat{\theta }_{n}^{+}),$ respectively, where%
\begin{eqnarray}
\widehat{J}_{n}(\theta ^{+})\hspace{-0.08in} &=&\hspace{-0.08in}\widehat{J}%
_{g,n}(\theta ^{+})^{\prime }\mathcal{W}_{n}\widehat{J}_{g,n}(\theta ^{+}),%
\text{ }\widehat{V}_{n}(\theta ^{+})=\widehat{J}_{g,n}(\theta ^{+})^{\prime }%
\mathcal{W}_{n}\widehat{V}_{g,n}(\theta ^{+})\mathcal{W}_{n}\widehat{J}%
_{g,n}(\theta ^{+}),  \notag \\
\widehat{J}_{g,n}(\theta ^{+})\hspace{-0.08in} &=&\hspace{-0.08in}%
n^{-1}\sum_{i=1}^{n}M_{i}(\theta )\binom{\pi Z_{i}^{\prime },X_{i}^{\prime
},0_{d_{X}}^{\prime },Z_{i}^{\prime }\omega }{Z_{i}^{\prime
},0_{d_{X}}^{\prime },X_{i}^{\prime },0}\text{ and }  \notag \\
\widehat{V}_{g,n}(\theta ^{+})\hspace{-0.08in} &=&\hspace{-0.08in}%
n^{-1}\sum_{i=1}^{n}\left( e_{i}(\theta )e_{i}(\theta )^{\prime }\right)
\otimes \left( \overline{Z}_{i}\overline{Z}_{i}^{\prime }\right) .
\end{eqnarray}

Assumption V1(i) (vector $\beta $) holds with 
\begin{eqnarray}
J(\theta ^{+};\gamma _{0})\hspace{-0.08in} &=&\hspace{-0.08in}J_{g}(\theta
^{+};\gamma _{0})^{\prime }\mathcal{W}(\theta _{0};\gamma _{0})J_{g}(\theta
^{+};\gamma _{0})\text{ and}  \notag \\
V(\theta ^{+};\gamma _{0})\hspace{-0.08in} &=&\hspace{-0.08in}J_{g}(\theta
^{+};\gamma _{0})^{\prime }\mathcal{W}(\theta _{0};\gamma _{0})V_{g}(\theta
^{+};\gamma _{0})\mathcal{W}(\theta _{0};\gamma _{0})J_{g}(\theta
^{+};\gamma _{0}),
\end{eqnarray}%
where $J_{g}(\theta ^{+};\gamma _{0})$ and $V_{g}(\theta ^{+};\gamma _{0})$
are defined analogously to $\widehat{J}_{g}(\theta ^{+})$ and $\widehat{V}%
_{g}(\theta ^{+}),$ respectively, but with $n^{-1}\sum_{i=1}^{n}$ replaced
by $E_{\gamma _{0}}.$ The uniform convergence conditions of Assumption V1(i)
for $\widehat{J}_{n}(\theta ^{+})$ and $\widehat{V}_{n}(\theta ^{+})$ follow
from the uniform convergence of $\widehat{J}_{g,n}(\theta ^{+})$ and $%
\widehat{V}_{g,n}(\theta ^{+})$ and $\mathcal{W}_{n}\rightarrow _{p}\mathcal{%
W}(\theta _{0};\gamma _{0}).$ The former holds by the ULLN given in Lemma %
\ref{Lemma uniform convergence} in Supplemental Appendix C. When $\mathcal{W}%
_{n}$ is the identity matrix, the latter holds automatically. When $\mathcal{%
W}_{n}$ is the optimal weight matrix that involves a first step estimator $%
\overline{\theta }_{n}$ and $\overline{\theta }_{n}$ is based on the
identity weight matrix, the convergence in probability of $\mathcal{W}_{n}$
holds by Lemma \ref{Lemma Two step weight matrix}. The assumptions of Lemma %
\ref{Lemma Two step weight matrix} follow from Theorems \ref{Thm dist'n of
estimator b=finite}(a) and \ref{Thm dist'n of estimator b=inf}(a).

Assumption V1(ii) (vector $\beta $) holds by the continuity of $M_{i}(\theta
)$ and $e_{i}(\theta )$ in $\theta $ and the moment conditions in (\ref%
{Simul Probit Phi Space}).

Assumption V1(iii) (vector $\beta $) holds provided that $J(\theta
^{+};\gamma _{0})$ and $V(\theta ^{+};\gamma _{0})$ are both finite and
non-singular when $\beta _{0}=0.$ To this end, we need that $J_{g}(\theta
^{+};\gamma _{0}),$ $V_{g}(\theta ^{+};\gamma _{0}),$ and $\mathcal{W}%
(\theta ;\gamma _{0})$ are all finite and non-singular. This holds using the
forms of these matrices and $P_{\phi }(\overline{Z}_{i}^{\prime }c=0)<1$ for 
$c\neq 0$ by the arguments used in the verifications of Assumptions
GMM5(iii), GMM5(i), and GMM1(vii), respectively.

Assumption V2 follows from (i) the uniform convergence of $\widehat{J}%
_{g,n}(\theta ^{+})$ and $\widehat{V}_{g,n}(\theta ^{+})$, which holds by
the ULLN given in Lemma \ref{Lemma uniform convergence} in Supplemental
Appendix C, (ii) $\widehat{\theta }_{n}^{+}\rightarrow _{p}\theta _{0}^{+}$
under $\{\gamma _{n}\}\in \Gamma (\gamma _{0},\infty ,\omega _{0}),$ which
holds by Theorem \ref{Thm dist'n of estimator b=inf}(a) and $\widehat{\beta }%
_{n}/||\widehat{\beta }_{n}||\rightarrow \omega _{0}$ (see Lemma 9.4(b) of
Appendix B of AC1-SM), and (iii) $\mathcal{W}_{n}\rightarrow _{p}\mathcal{W}%
(\theta _{0};\gamma _{0}),$ which holds by Lemma \ref{Lemma Two step weight
matrix}.

\newpage

\section{ \hspace{-0.34in}\textbf{.}\hspace{0.2in}Supplemental Appendix B:
Proofs of GMM\newline
Estimation Results}

\subsection{\hspace{-0.23in}\textbf{.}\hspace{0.18in}Lemmas}

\setcounter{equation}{0}\hspace{0.25in}This Supplemental Appendix proves the
results in Theorems \ref{Thm dist'n of estimator b=finite} and \ref{Thm
dist'n of estimator b=inf} of AC3. The method of proof is to show that
Assumptions B1, B2, and GMM1-GMM5 imply the high-level assumptions in AC1,
viz., Assumptions A, B3, C1-C8, and D1-D3 of AC1. Given this, Theorems 3.1
and 3.2 of AC1 imply Theorems \ref{Thm dist'n of estimator b=finite} and \ref%
{Thm dist'n of estimator b=inf} because the results of these theorems are
the same, just the assumptions differ.

\begin{lemma}
\hspace{-0.08in}\textbf{.} \label{Lemma GMM A and B3}Suppose Assumption 
\emph{GMM1} holds. Then,

\noindent \emph{(a)} Assumption \emph{A} of \emph{AC1 }holds and

\noindent \emph{(b)} Assumption \emph{B3} of \emph{AC1 }holds with $Q(\theta
;\gamma _{0})=g_{0}(\theta ;\gamma _{0})^{\prime }\mathcal{W}(\theta ;\gamma
_{0})g_{0}(\theta ;\gamma _{0}).$
\end{lemma}

Under Assumptions GMM1 and GMM2, Assumption GMM3 is used to show that the
"C" assumptions of AC1 hold for the GMM estimator. As above, $\mathcal{W}%
(\psi _{0};\gamma _{0})$ abbreviates $\mathcal{W}(\psi _{0},\pi ;\gamma
_{0}) $ when $\beta _{0}=0.$

\begin{lemma}
\textbf{\hspace{-0.08in}.} \label{Lemma GMM suff condition C}Suppose
Assumptions \emph{GMM1-GMM3 }hold. Then, the following are true.

\noindent \emph{(a)} Assumption \emph{C1} of \emph{AC1} holds with $D_{\psi
}Q_{n}(\theta )=g_{\psi }(\psi _{0},\pi ;\gamma _{0})^{\prime }\mathcal{W}%
(\psi _{0};\gamma _{0})\overline{g}_{n}(\theta )$ and \newline
$D_{\psi \psi }Q_{n}(\theta )=g_{\psi }(\psi _{0},\pi ;\gamma _{0})^{\prime }%
\mathcal{W}(\psi _{0};\gamma _{0})g_{\psi }(\psi _{0},\pi ;\gamma _{0}).$

\noindent \emph{(b)} Assumption \emph{C2} of \emph{AC1 }holds with $%
m(W_{i},\theta )=g_{\psi }(\psi _{0},\pi ;\gamma _{0})^{\prime }\mathcal{W}%
(\psi _{0};\gamma _{0})g(W_{i},\theta ).$

\noindent \emph{(c)} Assumption \emph{C3} of \emph{AC1 }holds with $\Omega
(\pi _{1},\pi _{2};\gamma _{0})=g_{\psi }(\psi _{0},\pi _{1};\gamma
_{0})^{\prime }\mathcal{W}(\psi _{0};\gamma _{0})\Omega _{g}(\gamma _{0})%
\newline
\times \mathcal{W}(\psi _{0};\gamma _{0})g_{\psi }(\psi _{0},\pi _{2};\gamma
_{0}).$

\noindent \emph{(d)} Assumption \emph{C4} of \emph{AC1 }holds with $H(\pi
;\gamma _{0})=g_{\psi }(\psi _{0},\pi ;\gamma _{0})^{\prime }\mathcal{W}%
(\psi _{0};\gamma _{0})g_{\psi }(\psi _{0},\pi ;\gamma _{0})=$\newline
$D_{\psi \psi }Q_{n}(\theta ).$

\noindent \emph{(e)} Assumption \emph{C5} of \emph{AC1 }holds with $%
K_{n}(\theta ;\gamma ^{\ast })=g_{\psi }(\psi _{0},\pi ;\gamma _{0})^{\prime
}\mathcal{W}(\psi _{0};\gamma _{0})K_{n,g}(\theta ;\gamma ^{\ast })\in
R^{d_{\psi }\times d_{\beta }},$ and $K(\psi _{0},\pi ;\gamma _{0})=g_{\psi
}(\psi _{0},\pi ;\gamma _{0})^{\prime }\mathcal{W}(\psi _{0};\gamma
_{0})K_{g}(\psi _{0},\pi ;\gamma _{0}).$

\noindent \emph{(f) }Assumption \emph{C7} of \emph{AC1 }holds.

\noindent \emph{(g) }Assumption \emph{C8} of \emph{AC1 }holds.
\end{lemma}

\noindent \textbf{Comments.} \textbf{1.} To obtain Lemma \ref{Lemma GMM suff
condition C}(a), Assumption GMM3 is sufficient but not necessary. When $%
\overline{g}_{n}(\theta )$ is not a sample average, as occurs with the MD
estimator, Assumption MD can be used in conjunction with Assumptions GMM1
and GMM2 to obtain Lemma \ref{Lemma GMM suff condition C}(a). In this case,
Assumptions C2-C5 of AC1 can be verified directly without using Assumption
GMM3.

\textbf{2.} Lemma \ref{Lemma GMM suff condition C}(c)-(e) provide the
quantities that appear in Assumption C6 of AC1, which is the same as
Assumption GMM4.\medskip

\begin{lemma}
\textbf{\hspace{-0.08in}.} \label{Lemma GMM Sufficient D}Suppose Assumptions 
\emph{GMM1, }$\emph{GMM2,}$ and \emph{GMM5 }hold.

\noindent \emph{(a)} Assumption \emph{D1} of \emph{AC1 }holds with $%
DQ_{n}(\theta )=g_{\theta }(\theta _{0};\gamma _{0})^{\prime }\mathcal{W}%
(\theta _{0};\gamma _{0})\overline{g}_{n}(\theta )$ and \newline
$D^{2}Q_{n}(\theta )=g_{\theta }(\theta _{0};\gamma _{0})^{\prime }\mathcal{W%
}(\theta _{0};\gamma _{0})g_{\theta }(\theta _{0};\gamma _{0}).$

\noindent \emph{(b)} Assumption \emph{D2} of \emph{AC1 }holds with $J(\gamma
_{0})=J_{g}(\gamma _{0})^{\prime }\mathcal{W}(\theta _{0};\gamma
_{0})J_{g}\left( \gamma _{0}\right) .$

\noindent \emph{(c)} Assumption \emph{D3} of \emph{AC1 }holds with $V(\gamma
_{0})=J_{g}(\gamma _{0})^{\prime }\mathcal{W}(\theta _{0};\gamma
_{0})V_{g}\left( \gamma _{0}\right) \mathcal{W}(\theta _{0};\gamma
_{0})J_{g}\left( \gamma _{0}\right) .$
\end{lemma}

\subsection{\hspace{-0.23in}\textbf{.}\hspace{0.18in}Minimum Distance
Estimators}

\hspace{0.25in}For the MD estimator, Assumption MD can be used in place of
Assumption GMM3 to obtain Assumption C1 of AC1.

\begin{corollary}
\textbf{\hspace{-0.08in}.} \label{Corollary MD}Assumptions \emph{GMM1,} 
\emph{GMM2,} and \emph{MD} imply that Assumption \emph{C1} of \emph{AC1 }%
holds with $D_{\psi }Q_{n}(\theta )$ and $D_{\psi \psi }Q_{n}(\theta )$
defined as in Lemma \emph{\ref{Lemma GMM suff condition C}(a).}
\end{corollary}

In addition to the result of Corollary \ref{Corollary MD}, Lemmas \ref{Lemma
GMM A and B3} and \ref{Lemma GMM Sufficient D} show that Assumptions A, B3,
and D1-D3 of AC1 hold for the MD estimator under Assumptions GMM1, GMM2, and
GMM5. Hence, in order to obtain the results of Theorems 3.1 and 3.2 of AC1
for MD estimators and other results concerning CS's, one just needs to
verify Assumptions C2-C8 of AC1.

\subsection{\hspace{-0.23in}\textbf{.}\hspace{0.18in}Proofs of Lemmas\label%
{GMM MD Proofs}}

\noindent \textbf{Proof of Lemma \ref{Lemma GMM A and B3}.} Assumption A of
AC1 is implied by Assumption GMM1(i).

Assumption GMM1(ii) implies that Assumption B3(i) of AC1 holds with $%
Q(\theta ;\gamma _{0})=g_{0}(\theta ;\gamma _{0})^{\prime }\mathcal{W}%
(\theta ;\gamma _{0})g_{0}(\theta ;\gamma _{0}).$

Now we verify Assumptions B3(ii) and B3(iii) of AC1 by using Lemma 8.1 in
Appendix A of AC1-SM, which shows that Assumption B3$^{\ast }$ of AC1-SM is
sufficient for Assumptions B3(ii) and B3(iii) of AC1. Assumption B3$^{\ast }$%
(i) of AC1-SM holds by Assumptions GMM1(v) and GMM1(vi). Assumption B3$%
^{\ast }$(ii) of AC1-SM holds by Assumptions GMM1(iii) and GMM1(vii).
Assumption B3$^{\ast }$(iii) of AC1-SM holds by Assumptions GMM1(iv) and
GMM1(vii). Hence, Assumption B3 of AC1 holds. $\square $\bigskip

We prove Lemma \ref{Lemma GMM Sufficient D} first and then prove Corollary %
\ref{Corollary MD} and Lemma \ref{Lemma GMM suff condition C}.\medskip

\noindent \textbf{Proof of Lemma \ref{Lemma GMM Sufficient D}. }We start
with the proof of part (a). For notational simplicity, in this proof $%
g_{0}(\theta ;\gamma _{0}),$ $g_{\theta }(\theta ;\gamma _{0}),$ $g_{\psi
}(\theta ;\gamma _{0}),$ and $\mathcal{W}(\theta ;\gamma _{0})$ are
abbreviated to $g_{0}(\theta ),$ $g_{\theta }(\theta ),$ $g_{\psi }(\theta
), $ and $\mathcal{W}(\theta ),$ respectively.

We start with the case in which $\mathcal{W}_{n}(\theta )=I_{k}.$ When $%
DQ_{n}\left( \theta _{n}\right) $ and $D^{2}Q_{n}\left( \theta _{n}\right) $
take the form in Lemma \ref{Lemma GMM Sufficient D}(a), the remainder term
in Assumption D1 becomes%
\begin{equation}
R_{n}^{\ast }(\theta )=\left\Vert \overline{g}_{n}(\theta )\right\Vert
^{2}/2-\left\Vert \overline{g}_{n}(\theta _{n})\right\Vert ^{2}/2-\overline{g%
}_{n}(\theta _{n})^{\prime }g_{\theta }(\theta _{0})(\theta -\theta
_{n})-\left\Vert g_{\theta }(\theta _{0})(\theta -\theta _{n})\right\Vert
^{2}/2.  \label{R star GMM}
\end{equation}%
We approximate $R_{n}^{\ast }(\theta )$ by replacing $g_{\theta }(\theta
_{0})(\theta -\theta _{n})$ by $g_{0}(\theta )-g_{0}(\theta _{n})$ and get%
\begin{equation}
R_{n}^{\dag }(\theta )=\left\Vert \overline{g}_{n}(\theta )\right\Vert
^{2}/2-\left\Vert \overline{g}_{n}(\theta _{n})\right\Vert ^{2}/2-\overline{g%
}_{n}(\theta _{n})^{\prime }\left( g_{0}(\theta )-g_{0}(\theta _{n})\right)
-\left\Vert g_{0}(\theta )-g_{0}(\theta _{n})\right\Vert ^{2}/2.
\label{R star star}
\end{equation}%
Let $a,c,$ and $d$ be $k-$vectors for which $a=c+d.$ By the Cauchy-Schwarz
inequality, 
\begin{equation}
\left\vert \left\Vert a\right\Vert ^{2}-\left\Vert c\right\Vert
^{2}\right\vert =\left\vert \left\Vert d\right\Vert ^{2}+2c^{\prime
}d\right\vert \leq \left\Vert d\right\Vert ^{2}+2\left\Vert c\right\Vert
\left\Vert d\right\Vert .  \label{inequality}
\end{equation}%
Let $a=g_{0}(\theta )-g_{0}(\theta _{n})$ and $c=g_{\theta }(\theta
_{0})(\theta -\theta _{n}),$ then 
\begin{eqnarray}
d\hspace{-0.08in} &=&\hspace{-0.08in}a-c=g_{0}(\theta )-g_{0}(\theta
_{n})-g_{\theta }(\theta _{0})(\theta -\theta _{n})  \notag \\
\hspace{-0.08in} &=&\hspace{-0.08in}[(g_{\theta }(\theta _{n}^{\dag
})-g_{\theta }(\theta _{0}))B^{-1}(\beta _{n})]B(\beta _{n})(\theta -\theta
_{n})=o(\left\Vert B(\beta _{n})\left( \theta -\theta _{n}\right)
\right\Vert ),  \label{c small op}
\end{eqnarray}%
where the first two equalities hold by definition, the third equality
follows from element-by-element mean-value expansions, where $\theta
_{n}^{\dag }$ is between $\theta $ and $\theta _{n}$ (and $\theta _{n}^{\dag
}$ may depend on the row)$,$ and the last equality follows from Assumption
GMM5(ii). By Assumptions GMM5(ii) and GMM5(iii), 
\begin{equation}
c=g_{\theta }\left( \theta _{0}\right) \left( \theta -\theta _{n}\right) = 
\left[ g_{\theta }\left( \theta _{0}\right) B^{-1}\left( \beta _{n}\right) %
\right] B\left( \beta _{n}\right) \left( \theta -\theta _{n}\right) =O\left(
\left\Vert B\left( \beta _{n}\right) \left( \theta -\theta _{n}\right)
\right\Vert \right) .  \label{b term}
\end{equation}%
Hence, 
\begin{eqnarray}
&&\sup_{\theta \in \Theta _{n}\left( \delta _{n}\right) }\frac{n|R_{n}^{\dag
}(\theta )-R_{n}^{\ast }(\theta )|}{(1+n^{1/2}||B(\beta _{n})(\theta -\theta
_{n})||)^{2}}  \notag \\
\hspace{-0.18in} &=&\hspace{-0.08in}\frac{1}{2}\sup_{\theta \in \Theta
_{n}\left( \delta _{n}\right) }\frac{n\left\vert 2\overline{g}_{n}(\theta
_{n})^{\prime }d+\left\Vert g_{0}(\theta )-g_{0}(\theta _{n})\right\Vert
^{2}-\left\Vert g_{\theta }(\theta _{0})(\theta -\theta _{n})\right\Vert
^{2}\right\vert }{(1+n^{1/2}||B(\beta _{n})(\theta -\theta _{n})||)^{2}} \\
\hspace{-0.18in} &\leq &\hspace{-0.08in}\frac{1}{2}\sup_{\theta \in \Theta
_{n}\left( \delta _{n}\right) }n\left( 2\left\Vert \overline{g}_{n}(\theta
_{n})\right\Vert \left\Vert d\right\Vert \hspace{-0.02in}+\hspace{-0.02in}%
\left\Vert d\right\Vert ^{2}\hspace{-0.02in}+\hspace{-0.02in}2\left\Vert
c\right\Vert \left\Vert d\right\Vert \right) \hspace{-0.02in}%
/(1+n^{1/2}||B(\beta _{n})(\theta -\theta _{n})||)^{2}=o_{p}\left( 1\right) ,
\notag
\end{eqnarray}%
where the first equality follows from (\ref{R star GMM}) and (\ref{R star
star}), the inequality holds by (\ref{inequality}), and the second equality
uses (\ref{c small op}), (\ref{b term}), and $\overline{g}_{n}\left( \theta
_{n}\right) =O_{p}(n^{-1/2})$, where the latter holds by Assumption GMM5(i).
Thus, it suffices to show that Assumption D1(ii) holds with $R_{n}^{\ast
}(\theta )$ replaced by $R_{n}^{\dag }(\theta ).$

Note that%
\begin{eqnarray}
R_{n}^{\dag }(\theta )\hspace{-0.08in} &=&\hspace{-0.08in}\left\Vert 
\overline{g}_{n}(\theta )\right\Vert ^{2}/2-\left\Vert \overline{g}%
_{n}\left( \theta _{n}\right) +g_{0}\left( \theta \right) -g_{0}\left(
\theta _{n}\right) \right\Vert ^{2}/2  \notag \\
&=&\hspace{-0.08in}\left\Vert \widetilde{g}_{n}(\theta )-\widetilde{g}%
_{n}(\theta _{n})\right\Vert ^{2}/2+\left( g_{0}(\theta )-g_{0}(\theta _{n})+%
\overline{g}_{n}(\theta _{n})\right) ^{\prime }(\widetilde{g}_{n}(\theta )-%
\widetilde{g}_{n}(\theta _{n})),  \label{r star 1}
\end{eqnarray}%
where the first equality follows from (\ref{R star star}) and the second
equality uses $\left\Vert a\right\Vert ^{2}-\left\Vert c\right\Vert
^{2}=\left\Vert a-c\right\Vert ^{2}+2c^{\prime }(a-c)$ with $a=\overline{g}%
_{n}\left( \theta \right) ,$ $c=\overline{g}_{n}(\theta _{n})+g_{0}(\theta
)-g_{0}(\theta _{n}),$ and $a-c=\widetilde{g}_{n}(\theta )-\widetilde{g}%
_{n}(\theta _{n}).$

We have 
\begin{equation}
\eta _{n}=\sup_{\theta \in \Theta _{n}\left( \delta _{n}\right) }\frac{%
n^{1/2}\left\Vert \widetilde{g}_{n}(\theta )-\widetilde{g}_{n}(\theta
_{n})\right\Vert }{1+n^{1/2}\left\Vert B(\beta _{n})(\theta -\theta
_{n})\right\Vert }=o_{p}\left( 1\right) ,  \label{eta}
\end{equation}%
where the $o_{p}\left( 1\right) $ term holds by Assumption GMM2(ii)$.$ By (%
\ref{r star 1}), (\ref{eta}), and the triangle inequality,%
\begin{eqnarray}
&&\hspace{-0.08in}\sup_{\theta \in \Theta _{n}\left( \delta _{n}\right) }%
\frac{2n|R_{n}^{\dag }(\theta )|}{\left( 1+n^{1/2}\left\Vert B(\beta
_{n})(\theta -\theta _{n})\right\Vert \right) ^{2}}\hspace{-0.08in}  \notag
\\
&\leq &\hspace{-0.08in}\eta _{n}^{2}+2\sup_{\theta \in \Theta _{n}\left(
\delta _{n}\right) }\frac{n^{1/2}\left\Vert g_{0}(\theta )-g_{0}(\theta
_{n})\right\Vert +n^{1/2}||\overline{g}_{n}(\theta _{n})||}{%
1+n^{1/2}\left\Vert B(\beta _{n})(\theta -\theta _{n})\right\Vert }\eta _{n}
\notag \\
&=&\hspace{-0.08in}\eta _{n}^{2}+O_{p}(1)\eta _{n}=o_{p}(1),
\end{eqnarray}%
where the first equality holds because $\overline{g}_{n}\left( \theta
_{n}\right) =O_{p}(n^{-1/2})$ and $\left\Vert g_{0}\left( \theta \right)
-g_{0}\left( \theta _{n}\right) \right\Vert =O(||B(\beta _{n})$\allowbreak $%
(\theta -\theta _{n})||)$ uniformly on $\Theta _{n}\left( \delta _{n}\right)
.$ To see that the latter holds, element-by-element mean-value expansions
give%
\begin{equation}
g_{0}\left( \theta \right) -g_{0}\left( \theta _{n}\right) =(g_{\theta
}(\theta _{n}^{\dag })B^{-1}(\beta _{n}))B\left( \beta _{n}\right) \left(
\theta -\theta _{n}\right) =\left( J_{g}(\gamma _{0})+o\left( 1\right)
\right) B\left( \beta _{n}\right) \left( \theta -\theta _{n}\right) ,
\label{G diff}
\end{equation}%
where $\theta _{n}^{\dag }$ lies between $\theta $ and $\theta _{n}$ and the
last equality follows from Assumptions GMM5(ii) and GMM5(iii). This
completes the proof of Lemma \ref{Lemma GMM Sufficient D}(a) for the case in
which $\mathcal{W}_{n}\left( \theta \right) =I_{k}.$

Next, Lemma \ref{Lemma GMM Sufficient D}(a) is established for the case
where $\mathcal{W}_{n}(\theta )$ is as in Assumption GMM1. By Assumptions
GMM1(ii) and GMM1(vii), we know that $\mathcal{W}_{n}(\theta )$ is symmetric
and positive definite in a neighborhood of $\theta _{0}.$ Hence, both $%
\mathcal{W}(\theta )$ and $\mathcal{W}_{n}(\theta )$ have square roots,
denoted by $\mathcal{W}^{1/2}(\theta )$ and $\mathcal{W}_{n}^{1/2}(\theta ),$
respectively$.$ The idea is to use the same proof as above, but with $%
\overline{g}_{n}(\theta ),$ $g_{0}(\theta ),$ and $g_{\theta }(\theta _{0})$
replaced by $\mathcal{W}_{n}^{1/2}(\theta )\overline{g}_{n}(\theta ),$ $%
\mathcal{W}^{1/2}(\theta _{0})g_{0}(\theta ),$ and $\mathcal{W}^{1/2}(\theta
_{0})g_{\theta }(\theta _{0}).$ With these changes, $R_{n}^{\ast }(\theta )$
in (\ref{R star GMM}) becomes%
\begin{eqnarray}
&&R_{n}^{\ast \ast }(\theta )\overset{}{=}||\mathcal{W}_{n}^{1/2}(\theta )%
\overline{g}_{n}(\theta )||^{2}/2-||\mathcal{W}_{n}^{1/2}(\theta _{n})%
\overline{g}_{n}(\theta _{n})||^{2}/2-  \label{R star star replace} \\
&&\overline{g}_{n}(\theta _{n})^{\prime }\mathcal{W}_{n}^{1/2}(\theta
_{n})^{\prime }\mathcal{W}^{1/2}(\theta _{0})g_{\theta }(\theta _{0})(\theta
-\theta _{n})-||\mathcal{W}^{1/2}\left( \theta _{0}\right) g_{\theta
}(\theta _{0})(\theta -\theta _{n})||^{2}/2.  \notag
\end{eqnarray}%
To show the condition in Assumption D1(ii) holds for $R_{n}^{\ast \ast
}\left( \theta \right) ,$ the method used for the case $\mathcal{W}%
_{n}\left( \theta \right) =I_{k}$ works provided that Assumptions GMM2(ii)
and GMM5, which are used in the foregoing proof, hold with the same changes.
Assumption GMM5 obviously does with $V_{g}\left( \gamma _{0}\right) $ and $%
J_{g}\left( \gamma _{0}\right) $ adjusted to $\mathcal{W}^{1/2}\left( \theta
_{0}\right) V_{g}\left( \gamma _{0}\right) \mathcal{W}^{1/2}\left( \theta
_{0}\right) $ and $\mathcal{W}^{1/2}\left( \theta _{0}\right) J_{g}\left(
\gamma _{0}\right) ,$ respectively.

We now show Assumption GMM2(ii) also holds with the changes above. For $%
\theta \in \Theta _{n}(\delta _{n}),$%
\begin{eqnarray}
&&\hspace{-0.08in}||\mathcal{W}_{n}^{1/2}\left( \theta \right) \overline{g}%
_{n}\left( \theta \right) -\mathcal{W}^{1/2}\left( \theta _{0}\right)
g_{0}\left( \theta \right) -\mathcal{W}_{n}^{1/2}\left( \theta _{n}\right) 
\overline{g}_{n}\left( \theta _{n}\right) +\mathcal{W}^{1/2}\left( \theta
_{0}\right) g_{0}\left( \theta _{n}\right) ||  \notag \\
&\leq &\hspace{-0.08in}||\mathcal{W}^{1/2}\left( \theta _{0}\right)
||\left\Vert \widetilde{g}_{n}(\theta )-\widetilde{g}_{n}(\theta
_{n})\right\Vert +||\mathcal{W}_{n}^{1/2}\left( \theta \right) -\mathcal{W}%
^{1/2}\left( \theta _{0}\right) ||\left\Vert \overline{g}_{n}\left( \theta
\right) -\overline{g}_{n}\left( \theta _{n}\right) \right\Vert +  \notag \\
&&\hspace{-0.08in}||\mathcal{W}_{n}^{1/2}\left( \theta \right) -\mathcal{W}%
_{n}^{1/2}\left( \theta _{n}\right) ||\left\Vert \overline{g}_{n}\left(
\theta _{n}\right) \right\Vert  \notag \\
&\leq &\hspace{-0.08in}O\left( 1\right) \left\Vert \widetilde{g}_{n}(\theta
)-\widetilde{g}_{n}(\theta _{n})\right\Vert +o_{p}\left( 1\right)
(\left\Vert \widetilde{g}_{n}(\theta )-\widetilde{g}_{n}(\theta
_{n})\right\Vert +\left\Vert g_{0}\left( \theta \right) -g_{0}\left( \theta
_{n}\right) \right\Vert )+  \notag \\
&&\hspace{-0.08in}o_{p}\left( 1\right) \left\Vert \overline{g}_{n}\left(
\theta _{n}\right) \right\Vert  \label{GMM adjustment} \\
&=&\hspace{-0.08in}o_{p}(n^{-1/2}\sup_{\theta \in \Theta _{n}(\delta
_{n})}(1+n^{1/2}||B(\beta _{n})(\theta -\theta _{n})||))+O\left( \left\Vert
\left( B\left( \beta _{n}\right) \left( \theta -\theta _{n}\right) \right)
\right\Vert \right) =o_{p}(1),  \notag
\end{eqnarray}%
where the first inequality follows from adding and subtracting $\mathcal{W}%
^{1/2}\left( \theta _{0}\right) \overline{g}_{n}\left( \theta \right) ,$%
\linebreak $\mathcal{W}^{1/2}\left( \theta _{0}\right) \overline{g}%
_{n}\left( \theta _{n}\right) ,$ and $\mathcal{W}_{n}^{1/2}\left( \theta
\right) \overline{g}_{n}\left( \theta _{n}\right) $ and invoking the
triangle inequality, the second inequality holds by Assumptions GMM1(ii),
GMM1(vi), and GMM1(vii), the first equality holds by Assumption GMM2(ii), (%
\ref{G diff}), and $\overline{g}_{n}\left( \theta _{n}\right)
=O_{p}(n^{-1/2}),$ and the second equality holds by the definition of $%
\Theta _{n}(\delta _{n})$ and $B(\beta _{n})$. By (\ref{GMM adjustment}),
the condition in Assumption D1(ii) holds with $R_{n}^{\ast }(\theta )$
changed to $R_{n}^{\ast \ast }(\theta ).$

When the random derivative matrices take the form in Lemma \ref{Lemma GMM
Sufficient D}(a), the remainder term in Assumption D1(i) is%
\begin{eqnarray}
R_{n}^{\ast }(\theta )\hspace{-0.08in} &=&\hspace{-0.08in}||\mathcal{W}%
_{n}^{1/2}\hspace{-0.02in}\left( \theta \right) \overline{g}_{n}(\theta
)||^{2}/2-||\mathcal{W}_{n}^{1/2}\hspace{-0.02in}\left( \theta _{n}\right) 
\overline{g}_{n}(\theta _{n})||^{2}/2-\overline{g}_{n}(\theta _{n})^{\prime }%
\mathcal{W}\left( \theta _{0}\right) g_{\theta }(\theta _{0})^{\prime
}(\theta -\theta _{n})-  \notag \\
&&\hspace{-0.08in}||\mathcal{W}^{1/2}\left( \theta _{0}\right) g_{\theta
}(\theta _{0})(\theta -\theta _{n})||^{2}/2.
\end{eqnarray}%
We now show the difference between $R_{n}^{\ast }\left( \theta \right) $ and 
$R_{n}^{\ast \ast }\left( \theta \right) $ in (\ref{R star star replace}) is
small enough so that the condition in Assumption D1(ii) holds for $%
R_{n}^{\ast }\left( \theta \right) $ provided it holds for $R_{n}^{\ast \ast
}\left( \theta \right) .$ For $\theta \in \Theta _{n}(\delta _{n}),$%
\begin{eqnarray}
&&\hspace{-0.08in}|R_{n}^{\ast }\left( \theta \right) -R_{n}^{\ast \ast
}\left( \theta \right) |\overset{}{=}|\overline{g}_{n}(\theta _{n})^{\prime
}(\mathcal{W}_{n}^{1/2}\left( \theta _{n}\right) -\mathcal{W}^{1/2}\left(
\theta _{0}\right) )^{\prime }\mathcal{W}^{1/2}\left( \theta _{0}\right)
g_{\theta }(\theta _{0})(\theta -\theta _{n})|  \notag \\
&\leq &\hspace{-0.08in}\left\Vert \overline{g}_{n}\left( \theta _{n}\right)
\right\Vert \cdot ||\mathcal{W}_{n}^{1/2}\left( \theta _{n}\right) -\mathcal{%
W}^{1/2}\left( \theta _{0}\right) ||\cdot ||\mathcal{W}^{1/2}\left( \theta
_{0}\right) ||\cdot \left\Vert g_{\theta }(\theta _{0})B^{-1}\left( \beta
_{n}\right) \right\Vert \cdot  \notag \\
&&\hspace{-0.08in}\left\Vert B\left( \beta _{n}\right) \left( \theta -\theta
_{n}\right) \right\Vert  \notag \\
&=&\hspace{-0.08in}o_{p}(n^{-1/2}\left\Vert B\left( \beta _{n}\right) \left(
\theta -\theta _{n}\right) \right\Vert )=o_{p}(1),
\end{eqnarray}%
where the second last equality holds by Assumptions GMM1 and GMM5. This
completes the proof of part (a).

Part (b) follows from part (a) and Assumptions GMM5(ii) and GMM5(iii).

Part (c) follows from part (a) and Assumptions GMM5(i)-(iii). $\square $%
\bigskip

We now prove Corollary \ref{Corollary MD} and then use Corollary \ref%
{Corollary MD} to prove Lemma \ref{Lemma GMM suff condition C}.\medskip

\noindent \textbf{Proof of Corollary \ref{Corollary MD}. }The proof is
analogous to the proof of Lemma \ref{Lemma GMM Sufficient D}(a) with \ (i) $%
DQ_{n}\left( \theta _{n}\right) $ and $D^{2}Q_{n}\left( \theta _{n}\right) $
in Lemma \ref{Lemma GMM Sufficient D}(a) changed to $D_{\psi }Q_{n}\left(
\psi _{0,n},\pi \right) $ and $D_{\psi \psi }Q_{n}\left( \psi _{0,n},\pi
\right) $ in Lemma \ref{Lemma GMM suff condition C}(a)$,$ (ii) $R_{n}^{\ast
}(\theta )$ changed to $R_{n}(\psi ,\pi ),$ (iii) $\theta _{n}$ and $\theta
-\theta _{n}$ changed to $(\psi _{0,n},\pi )$ and $\psi -\psi _{0,n},$ (iv) $%
g_{\theta }\left( \cdot \right) $ changed to $g_{\psi }(\cdot ),$ where as
above $g_{\theta }(\cdot )$ and $g_{\psi }(\cdot )$ abbreviate $g_{\theta
}(\cdot ;\gamma _{0})$ and $g_{\psi }(\cdot ;\gamma _{0}),$ respectively$,$
(v) $B(\beta _{n})$ and $B^{-1}(\beta _{n})$ deleted throughout, (vi) $%
\theta _{n}^{\dag }$ changed to $(\psi _{0,n}^{\dag }(\pi ),\pi )$ with $%
\psi _{0,n}^{\dag }(\pi )$ between $\psi $ and $\psi _{0,n},$ (vii) $\theta
\in \Theta _{n}\left( \delta _{n}\right) $ changed to $\psi \in \Psi \left(
\pi \right) $ and $\left\Vert \psi -\psi _{0,n}\right\Vert \leq \delta _{n},$
and (viii) $O_{p}\left( 1\right) $ and $o_{p}\left( 1\right) $ changed to $%
O_{p\pi }(1)$ and $o_{p\pi }(1),$ where the uniformity over $\Pi $ usually
holds using the compactness of $\Pi ,$ and (ix) $\mathcal{W}\left( \theta
_{0}\right) $ changed to $\mathcal{W}(\psi _{0};\gamma _{0}).$ Note that
Assumptions GMM3(iii) and MD hold with $\pi _{n}$ replaced by $\pi $ $%
\forall \pi \in \Pi $ under Assumption GMM1(i). The assumptions that are
referenced in the proof also are changed accordingly. Specifically, the
proof goes through with Assumption GMM2(ii) changed to Assumption GMM2(i),
Assumption GMM5(i) changed to Assumption MD, Assumption GMM5(ii) changed to
the continuity of $g_{\psi }(\theta ,\pi )$ uniformly over $\Pi ,$ which is
implied by Assumption GMM1(vii) and the compactness of $\Pi ,$ and
Assumption GMM5(iii) changed to the continuity of $g_{\psi }(\theta ).$ (The
assumption that $J_{g}(\gamma _{0})$ has full column rank is not used in the
proof of Lemma \ref{Lemma GMM Sufficient D}(a).)

Assumption C1(iii) follows from the form of $D_{\psi }Q_{n}(\theta )$ and $%
D_{\psi \psi }Q_{n}(\theta )$ in Lemma \ref{Lemma GMM suff condition C} and
Assumption GMM1(i). $\square $\bigskip

\noindent \textbf{Proof of Lemma \ref{Lemma GMM suff condition C}. }First we
prove part (a). Under Assumption GMM3, we can show Assumption MD holds using
a proof that is similar to the proof of Lemma 9.1 in Appendix B of AC1-SM
with (i) $D_{\psi }Q_{n}\left( \psi _{0,n},\pi \right) $ changed to $%
\overline{g}_{n}(\psi _{0,n},\pi ),$ (ii) $m\left( W_{i},\theta \right) $
changed to $g\left( W_{i},\theta \right) ,$ (iii) Assumptions C2, C3, and C5
of AC1 changed to the corresponding conditions in Assumptions GMM3. By
Corollary \ref{Corollary MD}, Lemma \ref{Lemma GMM suff condition C}(a)
holds under Assumptions GMM1-GMM3.

Part (b) follows from part (a) and Assumptions GMM3(i) and GMM3(ii).

Part (c) follows from part (b) and Assumptions GMM1(i) and GMM3(iii).

Part (d) follows from part (a), $H(\pi ;\gamma _{0})=D_{\psi \psi
}Q_{n}(\psi _{0,n},\pi ),$ and Assumption GMM1\allowbreak (viii).

Part (e) follows from part (a) and Assumption GMM3(iv).

Now we verify part (f). Note that when $\beta _{0}=0$ as in Assumption C7, $%
K_{g}(\psi _{0},\pi ;\gamma _{0})$ does not depend on $\pi $ by Assumptions
GMM1(i) and GMM3(i). Given the form of $H(\pi ;\gamma _{0})$ and $K(\pi
;\gamma _{0})$ in parts (d) and (e), for any $\pi \in \Pi ,$%
\begin{eqnarray}
&&\omega _{0}^{\prime }K(\pi ;\gamma _{0})^{\prime }H^{-1}(\pi ;\gamma
_{0})K(\pi ;\gamma _{0})\omega _{0}\overset{}{=}Y^{\prime }X(\pi )(X(\pi
)^{\prime }X(\pi ))^{-1}X(\pi )^{\prime }Y\overset{}{\leq }Y^{\prime }Y,%
\text{ where}  \notag \\
&&X(\pi )\overset{}{=}\mathcal{W}^{1/2}(\psi _{0};\gamma _{0})g_{\psi }(\psi
_{0},\pi ;\gamma _{0})\text{, }Y\overset{}{=}\mathcal{W}^{1/2}(\psi
_{0};\gamma _{0})K_{g}(\psi _{0},\pi ;\gamma _{0})\omega _{0},
\label{C6(ii) eq}
\end{eqnarray}%
and $Y$ does not depend on $\pi .$ The inequality in (\ref{C6(ii) eq}) holds
because $X(\pi )(X(\pi )^{\prime }X(\pi ))^{-1}\allowbreak X(\pi )^{\prime }$
is a projection matrix. The inequality holds as an equality when $\mathcal{W}%
^{1/2}(\psi _{0};\gamma _{0})$\linebreak $\times K_{g}(\psi _{0},\pi ;\gamma
_{0})\omega _{0}=\mathcal{W}^{1/2}(\psi _{0};\gamma _{0})g_{\psi }(\psi
_{0},\pi ;\gamma _{0})S$ for some $S\in R^{d_{\psi }}.$ By Assumptions
GMM1(vii) and GMM3(v), the inequality in (\ref{C6(ii) eq}) holds as an
equality iff $\pi =\pi _{0}.$ This completes the verification of Assumption
C7.

To verify Assumption C8 as in part (g), we have%
\begin{eqnarray}
\frac{\partial }{\partial \psi ^{\prime }}E_{\gamma _{n}}D_{\psi }Q_{n}(\psi
_{n},\pi _{n})\hspace{-0.08in} &=&\hspace{-0.08in}g_{\psi }(\psi _{0},\pi
_{n};\gamma _{0})^{\prime }\mathcal{W}(\psi _{0};\gamma _{0})\frac{\partial 
}{\partial \psi ^{\prime }}E_{\gamma _{n}}\overline{g}_{n}(\theta _{n}) 
\notag \\
&=&\hspace{-0.08in}g_{\psi }(\psi _{0},\pi _{n};\gamma _{0})^{\prime }%
\mathcal{W}(\psi _{0};\gamma _{0})\left( n^{-1}\sum_{i=1}^{n}\frac{\partial 
}{\partial \psi ^{\prime }}E_{\gamma _{n}}g(W_{i},\theta _{n})\right)  \notag
\\
&\rightarrow &\hspace{-0.08in}g_{\psi }(\theta _{0};\gamma _{0})^{\prime }%
\mathcal{W}(\psi _{0};\gamma _{0})g_{\psi }(\theta _{0};\gamma _{0})=H(\pi
_{0};\gamma _{0}),
\end{eqnarray}%
where the first equality holds by Lemma \ref{Lemma GMM suff condition C}(a),
the second equality holds by Assumption GMM3(i), the convergence holds by
Assumption GMM3(vi) and the continuity of $g_{\psi }(\theta ;\gamma _{0})$
in $\pi $ in Assumption GMM1(v), and the third equality holds by Lemma \ref%
{Lemma GMM suff condition C}(d). $\square $

\subsection{\hspace{-0.23in}\textbf{.}\hspace{0.18in}Proofs of Section 3
Lemmas}

\noindent \textbf{Proof of Lemma \ref{Lemma Two step weight matrix}. }By the
triangle inequality,%
\begin{equation}
\left\Vert \mathcal{W}_{n}(\overline{\theta }_{n})-\mathcal{W}(\theta
_{0};\gamma _{0})\right\Vert \leq \left\Vert \mathcal{W}_{n}(\overline{%
\theta }_{n})-\mathcal{W}(\overline{\theta }_{n};\gamma _{0})\right\Vert
+\left\Vert \mathcal{W}(\overline{\theta }_{n};\gamma _{0})-\mathcal{W}%
(\theta _{0};\gamma _{0})\right\Vert ,  \label{weight ineq}
\end{equation}%
where the first term on the rhs is $o_{p}(1)$ because $\mathcal{W}%
_{n}(\theta )$ converges to $\mathcal{W}(\theta ;\gamma _{0})$ uniformly
over $\Theta .$ When $\beta _{0}\neq 0,$ the second \ term on the rhs of (%
\ref{weight ineq}) is $o_{p}(1)$ because $\mathcal{W}(\theta ;\gamma _{0})$
is continuous in $\theta $ and $\overline{\theta }_{n}\rightarrow _{p}\theta
_{0}.$ When $\beta _{0}=0,$ to show the second term on the rhs of (\ref%
{weight ineq}) is $o_{p}(1)$, we have%
\begin{eqnarray}
&&\hspace{-0.08in}\left\Vert \mathcal{W}(\overline{\theta }_{n};\gamma _{0})-%
\mathcal{W}(\theta _{0};\gamma _{0})\right\Vert  \notag \\
&\leq &\hspace{-0.08in}\left\Vert \mathcal{W}(\overline{\psi }_{n},\overline{%
\pi }_{n},;\gamma _{0})-\mathcal{W}(\psi _{0},\overline{\pi }_{n};\gamma
_{0})\right\Vert +||\mathcal{W}(\psi _{0},\overline{\pi }_{n};\gamma _{0})-%
\mathcal{W}(\psi _{0},\pi _{0};\gamma _{0})||  \notag \\
&\leq &\hspace{-0.08in}\sup_{\pi \in \Pi }\left\Vert \mathcal{W}(\overline{%
\psi }_{n},\pi ,;\gamma _{0})-\mathcal{W}(\psi _{0},\pi ;\gamma
_{0})\right\Vert ,  \label{weight ineq 2}
\end{eqnarray}%
where the first inequality holds by the triangle inequality, and the second
inequality holds because $\mathcal{W}(\psi _{0},\pi ;\gamma _{0})$ does not
depend on $\pi $ when $\beta _{0}=0,$ which in turn holds by Assumptions
GMM1(i) and GMM1(ii)$.$ The third line of (\ref{weight ineq 2}) is $o_{p}(1)$
because $\overline{\psi }_{n}\rightarrow _{p}\psi _{0}$ and $\mathcal{W}%
(\psi ,\pi ,;\gamma _{0})$ is continuous in $\psi $ uniformly over $\pi \in
\Pi ,$ where the latter holds because $\mathcal{W}(\theta ;\gamma _{0})$ is
continuous in $\theta $ and $\Pi $ is compact. This completes the proof. $%
\square $\bigskip

\noindent \textbf{Proof of Lemma \ref{Lemma GMM Smooth Sufficient}.} First
we show that Assumption GMM2(ii) holds under Assumption GMM2$^{\ast }$. For $%
\theta \in \Theta _{n}(\delta _{n}),$%
\begin{eqnarray}
\widetilde{g}_{n}(\theta ;\gamma _{0})-\widetilde{g}_{n}(\theta _{n};\gamma
_{0})\hspace{-0.08in} &=&\hspace{-0.08in}\frac{\partial }{\partial \theta
^{\prime }}\widetilde{g}_{n}(\theta _{n}^{\dag };\gamma _{0})(\theta -\theta
_{n})  \notag \\
&=&\hspace{-0.08in}\left( [\frac{\partial }{\partial \theta ^{\prime }}%
g_{n}(\theta _{n}^{\dag };\gamma _{0})-g_{\theta }(\theta _{n}^{\dag
};\gamma _{0})]B^{-1}(\beta _{n})\right) B(\beta _{n})(\theta -\theta _{n}) 
\notag \\
&=&\hspace{-0.08in}o_{p}(||B(\beta _{n})(\theta -\theta _{n})||),
\label{gmm sm 1}
\end{eqnarray}%
where the first equality holds by element-by-element mean-value expansions
with $\theta _{n}^{\dag }$ between $\theta $ and $\theta _{n}$ (and $\theta
_{n}^{\dag }$ may depend on the row)$,$ the second equality holds by the
definition of $\widetilde{g}_{n}(\theta ,\gamma _{0}),$ and the last
equality holds uniformly over $\theta \in \Theta _{n}(\delta _{n})$ by
Assumption GMM2$^{\ast }$(iii). Assumption GMM2(ii) follows from (\ref{gmm
sm 1}) using the "$||B(\beta _{n})(\theta -\theta _{n})||$" part of the
denominator in Assumption GMM2(ii).

The proof for Assumption GMM2(i) is analogous to the proof of
Assumption\linebreak GMM2(ii). For $\psi \in \Psi (\pi ):||\psi -\psi
_{0,n}||\,\leq \delta _{n},$%
\begin{eqnarray}
\widetilde{g}_{n}(\psi ,\pi ;\gamma _{0})-\widetilde{g}_{n}(\psi _{0,n},\pi
;\gamma _{0})\hspace{-0.08in} &=&\hspace{-0.08in}\left( \frac{\partial }{%
\partial \psi ^{\prime }}g_{n}(\psi _{0,n}^{\dag }(\pi ),\pi ;\gamma
_{0})-g_{\psi }(\psi _{0,n}^{\dag }(\pi ),\pi ;\gamma _{0})\right) (\psi
-\psi _{0,n})  \notag \\
&=&\hspace{-0.08in}o_{p\pi }(||\psi -\psi _{0,n}||),  \label{gmm sm 2}
\end{eqnarray}%
where the first equality holds by element-by-element mean-value expansions
with $\psi _{0,n}^{\dag }(\pi )$ between $\psi $ and $\psi _{0,n}$ (and $%
\psi _{0,n}^{\dag }(\pi )$ may depend on the row), and the second equality
holds uniformly over $\psi \in \Psi (\pi ):||\psi -\psi _{0,n}||\,\leq
\delta _{n}$ by Assumption GMM2$^{\ast }$(ii). Assumption GMM2(i) follows
from (\ref{gmm sm 2}) using the "$||\psi -\psi _{0,n}||$" part of the
denominator in Assumption GMM2(i). $\square \bigskip $

\noindent \textbf{Proof of Lemma \ref{Lemma As GMM4}. }Assumption GMM4 is
the same as Assumption C6 of AC1. Hence, it suffices to verify the latter.
We verify Assumption C6 of AC1 by verifying the sufficient condition
Assumption C6$^{\ast \ast }$ given in Lemma 8.5 in Appendix A of AC1-SM.
Because $\beta $ is a scalar, it remains to show Assumption C6$^{\ast \ast }$%
(ii) of AC1 holds. By Lemma \ref{Lemma GMM suff condition C}(c), the
covariance matrix $\Omega _{G}(\pi _{1},\pi _{2};\gamma _{0})$ in Assumption
C6$^{\ast \ast }$(ii) is%
\begin{eqnarray}
\Omega _{G}(\pi _{1},\pi _{2};\gamma _{0})\hspace{-0.08in} &=&\hspace{-0.08in%
}g_{\psi }^{\ast }(\psi _{0},\pi _{1},\pi _{2};\gamma _{0})^{\prime }%
\widetilde{\Omega }_{g}(\gamma _{0})g_{\psi }^{\ast }(\psi _{0},\pi _{1},\pi
_{2};\gamma _{0})^{\prime },\text{ where}  \notag \\
\widetilde{\Omega }_{g}(\gamma _{0})\hspace{-0.08in} &=&\hspace{-0.08in}%
\mathcal{W}(\psi _{0};\gamma _{0})\Omega _{g}(\gamma _{0})\mathcal{W}(\psi
_{0};\gamma _{0})
\end{eqnarray}%
and $\widetilde{\Omega }_{g}(\gamma _{0})$ does not depend on $\pi _{1}$ and 
$\pi _{2}$ by Assumptions GMM1(i) and GMM3(i). Because $g_{\psi }^{\ast
}(\psi _{0},\pi _{1},\pi _{2};\gamma _{0})\in R^{k\times (d_{\zeta }+2)}$
and $k\geq d_{\theta }\geq d_{\zeta }+2,$ Assumption C6$^{\ast \ast }$(ii)
is implied by Assumptions GMM1$^{\ast }$(vii), GMM4$^{\ast }$(ii), and GMM4$%
^{\ast }$(iii). $\square $

\section{ \hspace{-0.34in}\textbf{.}\hspace{0.2in}Supplemental Appendix C:
Proofs for Wald Tests}

\subsection{\hspace{-0.23in}\textbf{.}\hspace{0.18in}Proofs of Asymptotic
Distributions}

\setcounter{equation}{0}\hspace{0.25in}Most of the results in Section \ref%
{Wald Tests Sec} of AC3 are stated to hold under some combination of
Assumptions GMM1-GMM5 or under certain assumptions from AC1 (plus some other
assumptions). We prove the results of this section using the stated
assumptions from AC1. Lemmas \ref{Lemma GMM A and B3}-\ref{Lemma GMM
Sufficient D} in Supplemental Appendix B show that the appropriate
combination of Assumptions GMM1-GMM5 imply the corresponding assumptions
from AC1.\medskip

\noindent \textbf{Proof of Lemma \ref{Lemma Sufficient R2}. }(i) When $%
d_{\pi }^{\ast }=d_{r},$ $\eta _{n}(\widehat{\theta }_{n})=0$ by definition
in (\ref{Defn eta}).

(ii) When $d_{r}=1,$ $d_{\pi }^{\ast }=0$ or $d_{\pi }^{\ast }=1$ by
Assumption R1(iii). If $d_{\pi }^{\ast }=1,$ $\eta _{n}(\widehat{\theta }%
_{n})=0$ by definition in (\ref{Defn eta}). If $d_{\pi }^{\ast }=0,$ $r_{\pi
}(\theta )=0$ for $\theta \in \Theta _{\delta }$ by Assumption R1(iii). By
the mean-value expansion, we have%
\begin{equation}
r(\psi _{n},\widehat{\pi }_{n})-r(\psi _{n},\pi _{n})=r_{\pi }(\psi _{n},%
\widetilde{\pi }_{n})(\widehat{\pi }_{n}-\pi _{n}),  \label{MVT_pi}
\end{equation}%
where $\widetilde{\pi }_{n}$ is between $\widehat{\pi }_{n}$ and $\pi _{n}.$
For $n$ large enough that $||\beta _{n}||<\delta ,$ $(\psi _{n},\widetilde{%
\pi }_{n})\in \Theta _{\delta }$ and $r_{\pi }(\psi _{n},\widetilde{\pi }%
_{n})=0,$ which implies $\eta _{n}(\widehat{\theta }_{n})=o_{p}(1).$

(iii) From (\ref{MVT_pi}), we have%
\begin{equation}
\eta (\widehat{\theta }_{n})=n^{1/2}A_{1}(\widehat{\theta }_{n})r_{\pi
}(\psi _{n},\widetilde{\pi }_{n})(\widehat{\pi }_{n}-\pi _{n}).
\end{equation}%
Under Assumption R2$^{\ast }$(iii), $A_{1}(\widehat{\theta }_{n})r_{\pi
}(\psi _{n},\widetilde{\pi }_{n})\rightarrow _{p}0$ because the column space
of $r_{\pi }(\theta )$ is the same for all $\theta \in \Theta _{\delta },$
by definition the rows of $A_{1}(\theta )$ are in the null space of $r_{\pi
}(\theta )^{\prime }$ $\forall \theta \in \Theta _{\delta },$ and $\widehat{%
\theta }_{n}\in \Theta _{\delta }$ holds with probability that goes to one
by Lemma 3.1(a) of AC1 using Assumptions A and B3(i)-(ii) of AC1. This gives
the desired result. $\square $\bigskip

\noindent \textbf{Proof of Lemma \ref{Lemma Sufficient Linear}. }Under
Assumption R$_{\text{L}}$, $r_{\theta }(\theta )=R$ $\forall \theta \in
\Theta $ and $R$ has full row rank. Assumption R1 is satisfied directly.
Moreover, under Assumption R$_{\text{L}}$, $r_{\pi }(\theta )$ does not
depend on $\theta .$ This implies Assumption R2$^{\ast }$(iii), which is a
sufficient condition of Assumption R2 by Lemma \ref{Lemma Sufficient R2}. $%
\square $\bigskip

The proof of Theorem \ref{Theorem Wald Nonlinear} below uses the following
Lemma.\textbf{\ }Define $\widehat{\omega }_{n}=\widehat{\beta }_{n}/||%
\widehat{\beta }_{n}||.$

\begin{lemma}
\hspace{-0.08in}\textbf{. }\label{Lemma Omega_hat}Suppose Assumption \emph{%
V1 (}vector $\beta $\emph{) }holds. In addition, suppose Assumptions \emph{%
GMM1-GMM4 }hold \emph{(}or Assumptions \emph{A, B1-B3,} and \emph{C1-C8} of 
\emph{AC1 }hold\emph{).}

\noindent \emph{(a)} Under $\{\gamma _{n}\}\in \Gamma (\gamma _{0},0,b)$
with $||b||<\infty ,$ $\widehat{\omega }_{n}\rightarrow _{d}\omega ^{\ast
}(\pi ^{\ast }(\gamma _{0},b);\gamma _{0},b).$

\noindent \emph{(b)} Under $\{\gamma _{n}\}\in \Gamma (\gamma _{0},\infty
,\omega _{0}),$ $\widehat{\omega }_{n}\rightarrow _{p}\omega _{0}.$
\end{lemma}

\noindent \textbf{Proof of Lemma \ref{Lemma Omega_hat}.} To prove Lemma \ref%
{Lemma Omega_hat}(a), we have%
\begin{equation}
\widehat{\omega }_{n}=n^{1/2}\widehat{\beta }_{n}/||n^{1/2}\widehat{\beta }%
_{n}||\rightarrow _{d}\frac{\tau _{\beta }(\pi ^{\ast }(\gamma
_{0},b);\gamma _{0},b)}{||\tau _{\beta }(\pi ^{\ast }(\gamma _{0},b);\gamma
_{0},b)||}=\omega ^{\ast }(\pi ^{\ast }(\gamma _{0},b);\gamma _{0},b)
\end{equation}%
by the continuous mapping theorem, because $n^{1/2}\widehat{\beta }%
_{n}\rightarrow _{d}\tau _{\beta }(\pi ^{\ast }(\gamma _{0},b);\gamma
_{0},b) $ by Theorem \ref{Thm dist'n of estimator b=finite}(a) and Comment 2
to Theorem \ref{Thm dist'n of estimator b=finite}(a) and $P(\tau _{\beta
}(\pi ^{\ast };\gamma _{0},b)=0)=0$ by Assumption V1(iv) (vector $\beta $).

Next, we prove that Lemma \ref{Lemma Omega_hat}(b) holds when $\beta _{0}=0.$
By Lemma 3.4 in AC1, $||\beta _{n}||^{-1}(\widehat{\beta }_{n}-\beta
_{n})=o_{p}(1).$ This implies that $\widehat{\beta }_{n}=\beta _{n}+||\beta
_{n}||o_{p}(1)$ and $||\widehat{\beta }_{n}||/||\beta _{n}||=1+o_{p}(1).$
Hence,%
\begin{equation}
\widehat{\omega }_{n}=\frac{\widehat{\beta }_{n}}{||\widehat{\beta }_{n}||}=%
\frac{\widehat{\beta }_{n}-\beta _{n}}{||\beta _{n}||}\frac{||\beta _{n}||}{%
||\widehat{\beta }_{n}||}+\frac{\beta _{n}}{||\beta _{n}||}\frac{||\beta
_{n}||}{||\widehat{\beta }_{n}||}\rightarrow _{p}\omega _{0}.
\end{equation}

Under $\{\gamma _{n}\}\in \Gamma (\gamma _{0},\infty ,\omega _{0})$ with $%
\beta _{0}\neq 0,$ $\widehat{\omega }_{n}\rightarrow \omega _{0}$ by the
continuous mapping theorem given that $\widehat{\beta }_{n}\rightarrow
_{p}\beta _{0}$ by Lemma 3.3(b)\label{Lemma consistency of pihat b=inf IN
AC1} in AC1. $\square $\bigskip

\noindent \textbf{Proof of Theorem \ref{Theorem Wald Nonlinear}.} Under the
null hypothesis $H_{0}:r(\theta _{n})=v_{n},$ the Wald statistic defined in (%
\ref{Defn Wald}) with $v=v_{n}$ becomes%
\begin{equation}
W_{n}=n(r(\widehat{\theta }_{n})-r(\theta _{n}))^{\prime }(r_{\theta }(%
\widehat{\theta }_{n})B^{-1}(\widehat{\beta }_{n})\widehat{\Sigma }%
_{n}B^{-1}(\widehat{\beta }_{n})r_{\theta }(\widehat{\theta }_{n})^{\prime
})^{-1}(r(\widehat{\theta }_{n})-r(\theta _{n})).  \label{Wald Nonlinear 1}
\end{equation}

Before proving the specific results in parts (a) and (b), we analyze the
Wald statistic under $\{\gamma _{n}\}\in \Gamma (\gamma _{0},0,b).$ With the
rotation represented by $A(\widehat{\theta }_{n}),$ the Wald statistic in (%
\ref{Wald Nonlinear 1}) can be written as%
\begin{equation}
W_{n}=n(r(\widehat{\theta }_{n}\hspace{-0.01in})-r(\theta _{n}\hspace{-0.01in%
}))^{\prime }\hspace{-0.02in}A(\widehat{\theta }_{n}\hspace{-0.01in}%
)^{\prime }(r_{\theta }^{A}(\widehat{\theta }_{n}\hspace{-0.01in})\hspace{%
-0.01in}B^{-1\hspace{-0.01in}}(\widehat{\beta }_{n})\widehat{\Sigma }%
_{n}B^{-1}\hspace{-0.01in}(\widehat{\beta }_{n})r_{\theta }^{A}(\widehat{%
\theta }_{n}\hspace{-0.01in})^{\prime })^{-1}\hspace{-0.02in}A(\widehat{%
\theta }_{n}\hspace{-0.01in})(r(\widehat{\theta }_{n}\hspace{-0.01in}%
)-r(\theta _{n}\hspace{-0.01in})).  \label{Wald Nonlinear 2}
\end{equation}%
To deal with the normalizing matrix $B^{-1}(\widehat{\beta }_{n}),$ part of
which diverges as $n\rightarrow \infty $ and $\beta _{n}\rightarrow 0,$ we
define a $d_{r}\times d_{r}$ matrix 
\begin{equation}
B^{\ast }(\widehat{\beta }_{n})=\left[ 
\begin{array}{cc}
I_{(d_{r}-d_{\pi }^{\ast })} & 0 \\ 
0 & \iota (\widehat{\beta }_{n})I_{d_{\pi }^{\ast }}%
\end{array}%
\right]  \label{B_star}
\end{equation}%
where $\iota (\beta )=\beta $ when $\beta $ is a scalar and $\iota (\beta
)=||\beta ||$ when $\beta $ is a scalar . We write the Wald statistic in (%
\ref{Wald Nonlinear 2}) as%
\begin{eqnarray}
W_{n}\hspace{-0.08in} &=&\hspace{-0.08in}\varrho (\widehat{\theta }%
_{n})^{\prime }(\overline{r}_{\theta }(\widehat{\theta }_{n})\widehat{\Sigma 
}_{n}\overline{r}_{\theta }(\widehat{\theta }_{n})^{\prime })^{-1}\varrho (%
\widehat{\theta }_{n}),\text{ where}  \label{Wald Nonlinear 3} \\
\varrho (\widehat{\theta }_{n})\hspace{-0.08in} &=&\hspace{-0.08in}%
n^{1/2}B^{\ast }(\widehat{\beta }_{n})A(\widehat{\theta }_{n})(r(\widehat{%
\theta }_{n})-r(\theta _{n}))\text{ and }\overline{r}_{\theta }(\widehat{%
\theta }_{n})=B^{\ast }(\widehat{\beta }_{n})r_{\theta }^{A}(\widehat{\theta 
}_{n})B^{-1}(\widehat{\beta }_{n}).  \notag
\end{eqnarray}%
Note that 
\begin{equation}
\overline{r}_{\theta }(\widehat{\theta }_{n})=\left[ 
\begin{array}{cc}
r_{\psi }^{\ast }(\widehat{\theta }_{n}) & 0 \\ 
\iota (\widehat{\beta }_{n})r_{\psi }^{0}(\widehat{\theta }_{n}) & r_{\pi
}^{\ast }(\widehat{\theta }_{n})%
\end{array}%
\right] =r_{\theta }^{\ast }(\widehat{\theta }_{n})+\left[ 
\begin{array}{cc}
0 & 0 \\ 
\iota (\widehat{\beta }_{n})r_{\psi }^{0}(\widehat{\theta }_{n}) & 0%
\end{array}%
\right] =r_{\theta }^{\ast }(\widehat{\theta }_{n})+o_{p}(1),  \label{r_bar}
\end{equation}%
where the $o_{p}\left( 1\right) $ term holds because $\iota (\widehat{\beta }%
_{n})=o_{p}\left( 1\right) $ under $\{\gamma _{n}\}\in \Gamma (\gamma
_{0},0,b)$ and $r_{\psi }^{0}(\widehat{\theta }_{n})=O_{p}(1)$ under
Assumption R1(i).

The next step is to derive the asymptotic distribution of $\varrho (\widehat{%
\theta }_{n})$ under $\{\gamma _{n}\}\in \Gamma (\gamma _{0},0,b).$ Note that%
\begin{eqnarray}
r(\widehat{\theta }_{n})-r(\theta _{n})\hspace{-0.08in} &=&\hspace{-0.08in}r(%
\widehat{\psi }_{n},\widehat{\pi }_{n})-r(\psi _{n},\widehat{\pi }%
_{n})+r(\psi _{n},\widehat{\pi }_{n})-r(\psi _{n},\pi _{n})  \notag \\
&=&\hspace{-0.08in}r_{\psi }(\widehat{\theta }_{n})(\widehat{\psi }_{n}-\psi
_{n})+(r(\psi _{n},\widehat{\pi }_{n})-r(\psi _{n},\pi
_{n}))+o_{p}(n^{-1/2}),  \label{decomp}
\end{eqnarray}%
where the first equality is trivial and the second equality holds by a
mean-value expansion, $\widehat{\psi }_{n}-\psi _{n}=O_{p}(n^{-1/2}),$ and
Assumption R1(i). From (\ref{B_star}) and $A(\theta )=[A_{1}^{\prime
}(\theta ):A_{2}^{\prime }(\theta )]^{\prime },$ we have 
\begin{eqnarray}
\varrho (\widehat{\theta }_{n})\hspace{-0.08in} &=&\hspace{-0.08in}\left( 
\begin{array}{c}
n^{1/2}A_{1}(\widehat{\theta }_{n})(r(\widehat{\theta }_{n})-r(\theta _{n}))
\\ 
n^{1/2}\iota (\widehat{\beta }_{n})A_{2}(\widehat{\theta }_{n})(r(\widehat{%
\theta }_{n})-r(\theta _{n}))%
\end{array}%
\right) =\varrho _{1}(\widehat{\theta }_{n})+\varrho _{2}(\widehat{\theta }%
_{n})+o_{p}\left( 1\right) ,\text{ where}  \notag \\
\varrho _{1}(\widehat{\theta }_{n})\hspace{-0.08in} &=&\hspace{-0.08in}%
\left( 
\begin{array}{c}
n^{1/2}A_{1}(\widehat{\theta }_{n})r_{\psi }(\widehat{\theta }_{n})(\widehat{%
\psi }_{n}-\psi _{n}) \\ 
n^{1/2}\iota (\widehat{\beta }_{n})A_{2}(\widehat{\theta }_{n})(r(\psi _{n},%
\widehat{\pi }_{n})-r(\psi _{n},\pi _{n}))%
\end{array}%
\right) ,  \notag \\
\varrho _{2}(\widehat{\theta }_{n})\hspace{-0.08in} &=&\hspace{-0.08in}%
\left( 
\begin{array}{c}
\eta _{n}(\widehat{\theta }_{n}) \\ 
n^{1/2}\iota (\widehat{\beta }_{n})A_{2}(\widehat{\theta }_{n})r_{\psi }(%
\widehat{\theta }_{n})(\widehat{\psi }_{n}-\psi _{n})%
\end{array}%
\right) =\left( 
\begin{array}{c}
\eta _{n}(\widehat{\theta }_{n}) \\ 
o_{p}\left( 1\right)%
\end{array}%
\right) ,  \label{Varrho}
\end{eqnarray}%
the second equality in $\varrho (\widehat{\theta }_{n})$ uses (\ref{decomp}%
), and the $o_{p}\left( 1\right) $ term associated with $\varrho _{2}(%
\widehat{\theta }_{n})$ holds by $n^{1/2}(\widehat{\psi }_{n}-\psi
_{n})=O_{p}(1)$ and $\iota (\widehat{\beta }_{n})=o_{p}(1)$ under $\{\gamma
_{n}\}\in \Gamma (\gamma _{0},0,b).$ Under Assumption R2, $\eta _{n}(%
\widehat{\theta }_{n})=o_{p}\left( 1\right) ,$ and, hence, $\varrho _{2}(%
\widehat{\theta }_{n})=o_{p}\left( 1\right) .$

In part (a), in which case $\{\gamma _{n}\}\in \Gamma (\gamma _{0},0,b)$ and 
$||b||<\infty ,$ we have%
\begin{eqnarray}
\varrho _{1}(\widehat{\theta }_{n})\hspace{-0.08in} &=&\hspace{-0.08in}%
\overline{B}_{n}(\widehat{\pi }_{n})\tau _{n}^{A}(\widehat{\pi }_{n}),\text{
where}  \notag \\
\tau _{n}^{A}(\pi )\hspace{-0.08in} &=&\hspace{-0.08in}\left( 
\begin{array}{c}
r_{\psi }^{\ast }(\widehat{\psi }_{n}(\pi ),\pi )n^{1/2}(\widehat{\psi }%
_{n}(\pi )-\psi _{n}) \\ 
A_{2}(\widehat{\psi }_{n}(\pi ),\pi )(r(\psi _{n},\pi )-r(\psi _{n},\pi
_{n}))%
\end{array}%
\right) \text{ and }  \notag \\
\overline{B}_{n}(\pi )\hspace{-0.08in} &=&\hspace{-0.08in}\left[ 
\begin{array}{cc}
I_{(d_{r}-d_{\pi }^{\ast })} & 0 \\ 
0 & \iota (n^{1/2}\widehat{\beta }_{n}(\pi ))I_{d_{\pi }^{\ast }}%
\end{array}%
\right] .  \label{Varrho 1}
\end{eqnarray}%
Using Assumption R1(i), Lemma 3.1(a) of AC1, Lemma 9.2(b) in Appendix B of
AC1-SM, and $\tau _{n}(\pi )=n^{1/2}(\widehat{\psi }_{n}(\pi )-\psi
_{n})\Rightarrow \tau (\pi ;\gamma _{0},b)$ in (9.21) of AC1-SM, we have%
\begin{equation}
\left( 
\begin{array}{c}
\tau _{n}^{A}(\cdot ) \\ 
\overline{B}_{n}(\cdot )%
\end{array}%
\right) \Rightarrow \left( 
\begin{array}{c}
\tau ^{A}(\cdot ;\gamma _{0},b) \\ 
\overline{B}(\cdot ;\gamma _{0},b)%
\end{array}%
\right)  \label{tao_A and B_bar}
\end{equation}%
under $\{\gamma _{n}\}\in \Gamma (\gamma _{0},0,b)$ with $||b||<\infty .$
From (\ref{Wald Nonlinear 3}), (\ref{r_bar}), (\ref{Varrho}), and (\ref%
{Varrho 1}), in the case of a scalar $\beta ,$ we have%
\begin{eqnarray}
W_{n}\hspace{-0.08in} &=&\hspace{-0.08in}\tau _{n}^{A}(\widehat{\pi }%
_{n})^{\prime }\overline{B}_{n}(\widehat{\pi }_{n})(r_{\theta }^{\ast }(%
\widehat{\theta }_{n})\widehat{\Sigma }_{n}r_{\theta }^{\ast }(\widehat{%
\theta }_{n})^{\prime })^{-1}\overline{B}_{n}(\widehat{\pi }_{n})\tau
_{n}^{A}(\widehat{\pi }_{n})+o_{p}\left( 1\right)  \notag \\
&=&\hspace{-0.08in}\lambda _{n}(\widehat{\pi }_{n})+o_{p}(1)\rightarrow
_{d}\lambda (\pi ^{\ast }(\gamma _{0},b);\gamma _{0},b),  \label{Wald last}
\end{eqnarray}%
where $\lambda _{n}(\pi )$ is defined implicitly, $\widehat{\Sigma }_{n}=%
\widehat{\Sigma }_{n}(\widehat{\theta }_{n})=\widehat{J}_{n}(\widehat{\theta 
}_{n})^{-1}\widehat{V}_{n}(\widehat{\theta }_{n})\widehat{J}_{n}(\widehat{%
\theta }_{n})^{-1}$ by Assumption V1 (scalar $\beta $), and the convergence
follows from the joint convergence $(\lambda _{n}(\cdot ),\widehat{\pi }%
_{n})\Rightarrow (\lambda (\cdot ;\gamma _{0},b),\pi ^{\ast }(\gamma
_{0},b)) $ and the continuous mapping theorem. The latter joint convergence
holds by (\ref{tao_A and B_bar}), Assumptions V1 (scalar $\beta $) and R1,
Theorem \ref{Thm dist'n of estimator b=finite}(a), the uniform consistency
of $\widehat{\psi }_{n}(\pi )$ over $\pi \in \Pi ,$ and the fact that $\tau
_{n}^{A}(\cdot ),$ $\overline{B}_{n}(\cdot ),$ and $\widehat{\pi }_{n}$ are
continuous functions of the empirical process $G_{n}(\cdot )$ with
probability one.

In the case of a vector $\beta ,$ (\ref{Wald last}) holds with $\widehat{%
\Sigma }_{n}(\widehat{\theta }_{n})$ replaced by $\widehat{\Sigma }_{n}(%
\widehat{\theta }_{n}^{+})=\widehat{J}_{n}^{-1}(\widehat{\theta }%
_{n}^{+})\allowbreak \widehat{V}_{n}(\widehat{\theta }_{n}^{+})\widehat{J}%
_{n}^{-1}(\widehat{\theta }_{n}^{+})$ using Assumption V1 (vector $\beta )$
and with $\lambda _{n}(\widehat{\pi }_{n})$ replaced by $\lambda _{n}(%
\widehat{\pi }_{n},\widehat{\omega }_{n}),$ which is defined implicitly. In
this case, the convergence in (\ref{Wald last}) follows from the joint
convergence $(\lambda _{n}(\cdot ),\widehat{\pi }_{n},\widehat{\omega }%
_{n})\Rightarrow (\lambda (\cdot ;\gamma _{0},b),$ $\pi ^{\ast }(\gamma
_{0},b),$ $\omega ^{\ast }(\pi ^{\ast }(\gamma _{0},b);\gamma _{0},b),$
which holds by the same argument as above plus Lemma \ref{Lemma Omega_hat}%
(a) and Assumption V1 (vector $\beta $). This completes the proof of part
(a).

The proof of part (b) is the same for the scalar and vector $\beta $ cases
because it relies on Assumption V2 which applies in both cases. To prove
part (b), we first analyze the case where $\{\gamma _{n}\}\in \Gamma (\gamma
_{0},\infty ,\omega _{0})$ and $\beta _{0}=0.$ In this case, $\{\gamma
_{n}\}\in \Gamma (\gamma _{0},0,b)$ with $b\notin R^{d_{\beta }},$ so (\ref%
{Wald Nonlinear 2})-(\ref{Varrho}) apply. As in (\ref{Variance Matrix Defns}%
), $\Sigma (\gamma _{0})=J^{-1}(\gamma _{0})V(\gamma _{0})J^{-1}(\gamma
_{0}).$ We have%
\begin{eqnarray}
\varrho _{1}(\widehat{\theta }_{n})\hspace{-0.08in} &=&\hspace{-0.08in}%
\left( 
\begin{array}{c}
n^{1/2}r_{\psi }^{\ast }(\widehat{\theta }_{n})(\widehat{\psi }_{n}-\psi
_{n}) \\ 
n^{1/2}\iota (\widehat{\beta }_{n})A_{2}(\widehat{\theta }_{n})(r_{\pi }(%
\widehat{\theta }_{n})+o_{p}\left( 1\right) )(\widehat{\pi }_{n}-\pi _{n})%
\end{array}%
\right)  \notag \\
&=&\hspace{-0.08in}\left( 
\begin{array}{c}
n^{1/2}r_{\psi }^{\ast }(\widehat{\theta }_{n})(\widehat{\psi }_{n}-\psi
_{n}) \\ 
n^{1/2}\iota (\beta _{n})A_{2}(\widehat{\theta }_{n})r_{\pi }(\widehat{%
\theta }_{n})(\widehat{\pi }_{n}-\pi _{n})+o_{p}\left( 1\right)%
\end{array}%
\right)  \notag \\
&=&\hspace{-0.08in}r_{\theta }^{\ast }(\widehat{\theta }_{n})n^{1/2}B(\beta
_{n})(\widehat{\theta }_{n}-\theta _{n})+o_{p}(1)  \notag \\
&\rightarrow &\hspace{-0.15in}_{d}\text{ }N(0,r_{\theta }^{\ast }(\theta
_{0})\Sigma (\gamma _{0})r_{\theta }^{\ast }(\theta _{0})),  \label{Varrho 2}
\end{eqnarray}%
where the first equality holds by a mean-value expansion, the fact that $%
\widehat{\pi }_{n}$ is consistent under $\{\gamma _{n}\}\in \Gamma (\gamma
_{0},\infty ,\omega _{0})$, and the continuity of $r_{\pi }(\theta )$ which
holds by Assumption R1, the second equality holds by $n^{1/2}(\widehat{\beta 
}_{n}-\beta _{n})=O_{p}(1)$ and $||\beta _{n}||n^{1/2}(\widehat{\pi }%
_{n}-\pi _{n})=O_{p}(1)$ under $\{\gamma _{n}\}\in \Gamma (\gamma
_{0},\infty ,\omega _{0}),$ the third equality holds by the definitions of $%
B(\beta )$ and $r_{\theta }^{\ast }(\theta ),$ and the convergence in
distribution holds by Theorem \ref{Thm dist'n of estimator b=inf}(a). The
result of part (b) follows from (\ref{Wald Nonlinear 3}), (\ref{r_bar}), (%
\ref{Varrho}), (\ref{Varrho 2}), and Assumptions D2 and D3(ii) of AC1 and
Assumption V2.

Under $\{\gamma _{n}\}\in \Gamma (\gamma _{0},\infty ,\omega _{0})$ and $%
\beta _{0}\neq 0,$ 
\begin{equation}
n^{1/2}(r(\widehat{\theta }_{n})-r(\theta _{n}))\rightarrow
_{d}N(0,r_{\theta }(\theta _{0})B^{-1}(\beta _{0})\Sigma (\gamma
_{0})B^{-1}(\beta _{0})r_{\theta }(\theta _{0})^{\prime })  \label{wald b 3}
\end{equation}%
by Theorem \ref{Thm dist'n of estimator b=inf}(a) and the delta method. By
Assumptions R1(i) and V2, 
\begin{equation}
r_{\theta }(\widehat{\theta }_{n})B^{-1}(\widehat{\beta }_{n})\widehat{%
\Sigma }_{n}B^{-1}(\widehat{\beta }_{n})r_{\theta }(\widehat{\theta }%
_{n})^{\prime }\rightarrow _{p}r_{\theta }(\theta _{0})B^{-1}(\beta
_{0})\Sigma \left( \gamma _{0}\right) B^{-1}(\beta _{0})r_{\theta }(\theta
_{0})^{\prime }.  \label{wald b 4}
\end{equation}%
The desired result follows from (\ref{Wald Nonlinear 1}), (\ref{wald b 3}),
and (\ref{wald b 4}). $\square $\bigskip

\noindent \textbf{Proof of Corollary \ref{Cor Wald Linear}. }By Lemma \ref%
{Lemma Sufficient Linear}, Assumption R2 is satisfied. Based on Theorem \ref%
{Theorem Wald Nonlinear}, it suffices to show that the stochastic process $%
\{\lambda (\pi ;\gamma _{0},b):\pi \in \Pi \}$ can be written as $\{\lambda
_{L}(\pi ;\gamma _{0},b):\pi \in \Pi \}$ under Assumption R$_{\text{L}}$.
Under Assumption R$_{\text{L}}$, $r_{\theta }(\theta ),$ $A(\theta ),$ and $%
r_{\theta }^{\ast }(\theta )$ do not depend on $\theta ,$ and, hence,%
\begin{equation}
\tau ^{A}(\pi ;\gamma _{0},b)=\left( 
\begin{array}{c}
r_{\psi }^{\ast }\tau (\pi ;\gamma _{0},b) \\ 
A_{2}r_{\pi }\cdot (\pi -\pi _{0})%
\end{array}%
\right) =\left( 
\begin{array}{c}
r_{\psi }^{\ast }\tau (\pi ;\gamma _{0},b) \\ 
r_{\pi }^{\ast }\cdot (\pi -\pi _{0})%
\end{array}%
\right) =R^{\ast }\overline{\tau }(\pi ;\gamma _{0},b).  \label{Wald linear}
\end{equation}%
The desired result follows from (\ref{Wald linear}) and $r_{\theta }^{\ast
}(\pi )=R^{\ast }$ $\forall \pi \in \Pi .$ $\square $\bigskip

\noindent \textbf{Proof of Theorem \ref{Theorem Divergence}. }From the proof
of Theorem \ref{Theorem Wald Nonlinear}, we know that $\varrho _{1}(\widehat{%
\theta }_{n})=O_{p}(1)$ under $\{\gamma _{n}\}\in \Gamma (\gamma _{0},0,b).$
Therefore, when $||\eta _{n}(\widehat{\theta }_{n})||\rightarrow _{p}\infty
, $ it follows from (\ref{Varrho}) that $||\varrho (\widehat{\theta }%
_{n})||\rightarrow _{p}\infty .$ This result, together with (\ref{Wald
Nonlinear 3}), (\ref{r_bar}), and Assumptions R1 and V1, completes the
proof. $\square $

\subsection{\hspace{-0.23in}\textbf{.}\hspace{0.18in}Proofs of Asymptotic
Size Results}

\noindent \textbf{Proof of Theorem \ref{Theorem Wald Asy Sz}. }The proof is
the same as the proof of Theorem 4.4 of AC1, which is given in Appendix B of
AC1-SM, but with $|T_{n}|,$ $|T(h)|,$ and $z_{1-\alpha /2}$ replaced by $%
W_{n},$ $W(h),$ and $\chi _{d_{r},1-\alpha }^{2},$ respectively; with
Theorem 4.1 of AC1 replaced by Theorem \ref{Theorem Wald Nonlinear}; and
with Assumption V3 of AC1 replaced by Assumption V4. $\square $\bigskip

\noindent \textbf{Proof of Corollary \ref{Corollary Wald 2}. }By Theorem \ref%
{Theorem Divergence}, $P_{\gamma _{n}}(W_{n}\leq \chi _{d_{r},1-\alpha
}^{2})\rightarrow _{p}0$ under $\{\gamma _{n}\}\in \Gamma (\gamma _{0},0,b)$
for which $||\eta _{n}(\widehat{\theta }_{n})||\rightarrow _{p}\infty .$ As
a result, the nominal $1-\alpha $ Wald CS has $AsySz=0$ by the definition of
asymptotic size. $\square $\bigskip

\noindent \textbf{Proof of Theorem \ref{Theorem Robust Size Wald CS}. }The
proof of Theorem \ref{Theorem Robust Size Wald CS} is the same as the proof
of Theorem 5.1 of AC1, which is given in Appendix B of AC1-SM, but with $%
|T_{n}|,$ $|T(h)|,$ and $z_{1-\alpha /2}$ replaced by $W_{n},$ $W(h),$ and $%
\chi _{d_{r},1-\alpha }^{2},$ respectively; with $c_{|t|,1-\alpha }^{LF},$ $%
c_{|t|,1-\alpha }(h),...$ replaced by $c_{W,1-\alpha }^{LF},$ $c_{W,1-\alpha
}(h),...$ throughout; with Theorem 4.1\label{Theorem T stat IN AC1 copy(1)}
of AC1 replaced by Theorem \ref{Theorem Wald Nonlinear}; and with Assumption
V3 of AC1 replaced by Assumption V4. $\square $

\section{ \hspace{-0.34in}\textbf{.}\hspace{0.2in}Supplemental Appendix D:
Uniform LLN\newline
and CLT}

\setcounter{equation}{0}\hspace{0.25in}In this Supplemental Appendix, we
state a uniform convergence result, a uniform LLN, and a CLT that are used
in the verification of Assumptions GMM1-GMM5 in the two examples considered
in the paper. Specifically, Lemma \ref{Lemma uniform convergence} is a
uniform convergence result for non-stochastic functions, Lemma \ref{Lemma
Uniform LLN, drift} is a uniform LLN, and Lemma \ref{Lemma CLT, array} is a
CLT. The latter two results are for strong mixing triangular arrays. These
are standard sorts of results. The proofs of these Lemmas are given in
Appendix A of Andrews and Cheng (2011).

\begin{lemma}
\hspace{-0.08in}\textbf{.} \label{Lemma uniform convergence}Let $%
\{q_{n}(\theta ):n\geq 1\}$ be non-stochastic functions on $\Theta .$
Suppose \emph{(i)} $q_{n}(\theta )\rightarrow 0$ $\forall \theta \in \Theta
, $ \emph{(ii)} $||q_{n}(\theta _{1})-q_{n}(\theta _{2})||\leq C\delta $ $%
\forall \theta _{1},\theta _{2}\in \Theta $ with $||\theta _{1}-\theta
_{2}||\leq \delta ,$ $\forall n\geq 1,$ for some $C<\infty $ and all $\delta
>0,$ and \emph{(iii)} $\Theta $ is compact. Then, $\sup_{\theta \in \Theta
}||q_{n}(\theta )||\rightarrow 0.$
\end{lemma}

\noindent \textbf{Assumption S1. }Under any $\gamma _{0}\in \Gamma ,$ $%
\{W_{i}:i\geq 1\}$ is a strictly stationary and strong mixing sequence with
mixing coefficients $\alpha _{m}\leq Cm^{-A}$ for some $A>d_{\theta
}q/(q-d_{\theta })$ and some $q>d_{\theta }\geq 2,$ or $\{W_{i}:i\geq 1\}$
is an i.i.d. sequence and the constant $q$ equals $2+\delta $ for some $%
\delta >0.$

\begin{lemma}
\hspace{-0.08in}\textbf{.} \label{Lemma Uniform LLN, drift}Suppose \emph{(i)}
Assumption \emph{S1} holds, $\emph{(ii)}$ for some function $M_{1}(w):%
\mathcal{W}\rightarrow R^{+}$ and all $\delta >0,$ $||s(w,\theta
_{1})-s(w,\theta _{2})||\leq M_{1}(w)\delta ,$ $\forall \theta _{1},\theta
_{2}\in \Theta $ with $||\theta _{1}-\theta _{2}||\leq \delta ,$ $\forall
w\in \mathcal{W},$ \emph{(iii)} $E_{\gamma }\sup_{\theta \in \Theta
}||s(W_{i},\theta )||^{1+\varepsilon }+E_{\gamma }M_{1}(W_{i})\leq C$ $%
\forall \gamma \in \Gamma $ for some $C<\infty $ and $\varepsilon >0,$ and 
\emph{(iv)} $\Theta $ is compact. Then, $\sup_{\theta \in \Theta
}||n^{-1}\sum_{i=1}^{n}s(W_{i},\theta )-E_{\gamma _{0}}s(W_{i},\theta
)||\rightarrow _{p}0$ under $\{\gamma _{n}\}\in \Gamma (\gamma _{0})$ and $%
E_{\gamma _{0}}s(W_{i},\theta )$ is uniformly continuous on $\Theta $ $%
\forall \gamma _{0}\in \Gamma .$
\end{lemma}

\noindent \textbf{Comment.} Note that the centering term in Lemma \ref{Lemma
Uniform LLN, drift} is $E_{\gamma _{0}}s(W_{i},\theta ),$ rather than $%
E_{\gamma _{n}}s(W_{i},\theta ).$

\begin{lemma}
\hspace{-0.08in}\textbf{.} \label{Lemma CLT, array}Suppose \emph{(i)}
Assumption \emph{S1} holds, \emph{(ii)} $s(w)\in R$ and $E_{\gamma
}|s(W_{i})|^{q}\leq C$ $\forall \gamma \in \Gamma $ for some $C<\infty $ and 
$q$ as in Assumption \emph{S1}$.$ Then, $n^{-1/2}\sum_{i=1}^{n}(s(W_{i})-E_{%
\gamma _{n}}s(W_{i}))\rightarrow _{d}N(0,V_{s}(\gamma _{0}))$ under $\{
\gamma _{n}\} \in \Gamma (\gamma _{0})$ $\forall \gamma _{0}\in \Gamma ,$
where $V_{s}(\gamma _{0})=\sum_{m=-\infty }^{\infty }\allowbreak Cov_{\gamma
_{0}}(s(W_{i}),s(W_{i+m})).$
\end{lemma}

\section{ \hspace{-0.34in}\textbf{.}\hspace{0.2in}Supplemental Appendix E:
Numerical Results}

\hspace{0.25in}Here we report some additional numerical results for the
nonlinear regression model with endogeneity.

Figures S-1 and S-2 report asymptotic and finite-sample ($n=500$) densities
of the estimators for $\beta $ and $\pi $ when $\pi _{0}=3.0.$ Figures S-3
to S-6 report asymptotic and finite-sample ($n=500$) densities of the $t$
and QLR statistics for $\beta $ and $\pi $ when $\pi _{0}=1.5.$ Figures S-7
and S-8 report CP's of nominal $0.95$ standard and robust $|t|$ and QLR CI's
for $\beta $ and $\pi $ when $\pi _{0}=3.0.$

\FRAME{ftbpFU}{3.557in}{1.7038in}{0pt}{\Qcb{Figure S-1. Asymptotic and
Finite-Sample ($n=500$) Densities of the Estimator of $\protect\beta $ in
the Nonlinear Regression Model with Endogeneity when $\protect\pi _{0}=3.0.$}%
}{}{gmm_beta_dens_30.eps}{\special{language "Scientific Word";type
"GRAPHIC";maintain-aspect-ratio TRUE;display "USEDEF";valid_file "F";width
3.557in;height 1.7038in;depth 0pt;original-width 6.9851in;original-height
2.9447in;cropleft "0.0861";croptop "1";cropright "0.9281";cropbottom
"0.0512";filename 'graphics/GMM_beta_dens_30.eps';file-properties "XNPEU";}}

\FRAME{ftbpFU}{3.5561in}{1.7038in}{0pt}{\Qcb{Figure S-2. Asymptotic and
Finite-Sample ($n=500$) Densities of the Estimator of $\protect\pi $ in the
Nonlinear Regression Model with Endogeneity when $\protect\pi _{0}=3.0.$}}{}{%
gmm_pi_dens_30.eps}{\special{language "Scientific Word";type
"GRAPHIC";maintain-aspect-ratio TRUE;display "USEDEF";valid_file "F";width
3.5561in;height 1.7038in;depth 0pt;original-width 6.9851in;original-height
2.9447in;cropleft "0.0861";croptop "1";cropright "0.9281";cropbottom
"0.0512";filename 'graphics/GMM_pi_dens_30.eps';file-properties "XNPEU";}}

\FRAME{ftbpFU}{3.557in}{1.7038in}{0pt}{\Qcb{Figure S-3. Asymptotic and
Finite-Sample ($n=500$) Densities of the $t$ Statistic for $\protect\beta $
in the Nonlinear Regression Model with Endogeneity when $\protect\pi %
_{0}=1.5 $ and the Standard Normal Density (Black Line).}}{}{%
gmm_beta_t_15.eps}{\special{language "Scientific Word";type
"GRAPHIC";maintain-aspect-ratio TRUE;display "USEDEF";valid_file "F";width
3.557in;height 1.7038in;depth 0pt;original-width 6.9851in;original-height
2.9447in;cropleft "0.0861";croptop "1";cropright "0.9281";cropbottom
"0.0512";filename 'graphics/GMM_beta_t_15.eps';file-properties "XNPEU";}}

\FRAME{ftbpFU}{3.557in}{1.7038in}{0pt}{\Qcb{Figure S-4. Asymptotic and
Finite-Sample (n=500) Densities of the QLR Statistic for $\protect\beta $ in
the Nonlinear Regression Model with Endogeneity when $\protect\pi _{0}=1.5$
and the $\protect\chi _{1}^{2}$ Density (Black Line).}}{}{gmm_beta_qlr_15.eps%
}{\special{language "Scientific Word";type "GRAPHIC";maintain-aspect-ratio
TRUE;display "USEDEF";valid_file "F";width 3.557in;height 1.7038in;depth
0pt;original-width 6.9851in;original-height 2.9447in;cropleft
"0.0861";croptop "1";cropright "0.9281";cropbottom "0.0512";filename
'graphics/GMM_beta_qlr_15.eps';file-properties "XNPEU";}}

\FRAME{ftbpFU}{3.557in}{1.7038in}{0pt}{\Qcb{Figure S-5. Asymptotic and
Finite-Sample ($n=500$) Densities of the $t$ Statistic for $\protect\pi $ in
the Nonlinear Regression Model with Endogeneity when $\protect\pi _{0}=1.5$
and the Standard Normal Density (Black Line).}}{}{gmm_pi_t_15.eps}{\special%
{language "Scientific Word";type "GRAPHIC";maintain-aspect-ratio
TRUE;display "USEDEF";valid_file "F";width 3.557in;height 1.7038in;depth
0pt;original-width 6.9851in;original-height 2.9447in;cropleft
"0.0861";croptop "1";cropright "0.9281";cropbottom "0.0512";filename
'graphics/GMM_pi_t_15.eps';file-properties "XNPEU";}}

\FRAME{ftbpFU}{3.557in}{1.7038in}{0pt}{\Qcb{Figure S-6. Asymptotic and
Finite-Sample (n=500) Densities of the QLR Statistic for $\protect\pi $ in
the Nonlinear Regression Model with Endogeneity when $\protect\pi _{0}=1.5$
and the $\protect\chi _{1}^{2}$ Density (Black Line).}}{}{gmm_pi_qlr_15.eps}{%
\special{language "Scientific Word";type "GRAPHIC";maintain-aspect-ratio
TRUE;display "USEDEF";valid_file "F";width 3.557in;height 1.7038in;depth
0pt;original-width 6.9851in;original-height 2.9447in;cropleft
"0.0861";croptop "1";cropright "0.9281";cropbottom "0.0512";filename
'graphics/GMM_pi_qlr_15.eps';file-properties "XNPEU";}}

\FRAME{ftbpFU}{3.4832in}{3.4177in}{0pt}{\Qcb{Figure S-7. Coverage
Probabilities of Standard $|t|$ and QLR CI's for $\protect\beta $ and $%
\protect\pi $ in the Nonlinear Regression Model with Endogeneity when $%
\protect\pi _{0}=3.0.$}}{}{gmm_std_cp_30.eps}{\special{language "Scientific
Word";type "GRAPHIC";maintain-aspect-ratio TRUE;display "USEDEF";valid_file
"F";width 3.4832in;height 3.4177in;depth 0pt;original-width
7.4858in;original-height 6.7464in;cropleft "0.0876";croptop
"0.9567";cropright "0.9268";cropbottom "0.0432";filename
'graphics/GMM_Std_CP_30.eps';file-properties "XNPEU";}}

\FRAME{ftbpFU}{3.4832in}{3.4177in}{0pt}{\Qcb{Figure S-8. Coverage
Probabilities of Robust $|t|$ and QLR CI's for $\protect\beta $ and $\protect%
\pi $ in the Nonlinear Regression Model with Endogeneity when $\protect\pi %
_{0}=3.0,$ $\protect\kappa =1.5,D=1,$ and $s(x)=\exp (-2x).$}}{}{%
gmm_rob_cp_30.eps}{\special{language "Scientific Word";type
"GRAPHIC";maintain-aspect-ratio TRUE;display "USEDEF";valid_file "F";width
3.4832in;height 3.4177in;depth 0pt;original-width 7.4858in;original-height
6.7464in;cropleft "0.0876";croptop "0.9567";cropright "0.9268";cropbottom
"0.0432";filename 'graphics/GMM_Rob_CP_30.eps';file-properties "XNPEU";}}

\begin{center}
{\LARGE R}{\Large EFERENCE}
\end{center}

\begin{description}
\item Andrews, D.W.K. \& X. Cheng (2011) Maximum likelihood estimation and
uniform inference with sporadic identification failure.\ Cowles Foundation
Discussion Paper No. 1824, Yale University.
\end{description}

\end{document}

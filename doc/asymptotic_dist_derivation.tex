\documentclass[11pt, letterpaper, twoside, final]{article}
\usepackage{amssymb, amsmath, amsthm} 
\usepackage{mathtools, thmtools}
\usepackage{csquotes}
\usepackage{cleveref}
\usepackage[margin=1in]{geometry}
\newcommand*{\dto}{\overset{d}{\longrightarrow}}
\newcommand*{\pto}{\overset{p}{\longrightarrow}}
\newcommand*{\mvert}{\,\middle\vert\,}
\newcommand*{\ivert}{\,\vert\,}
\newcommand*{\W}{\mathcal{W}}
\DeclareMathOperator*{\Var}{\mathbb{V}ar}
\DeclareMathOperator*{\Cov}{\mathbb{C}ov}
\DeclareMathOperator*{\E}{\mathbb{E}}
\newtheorem{lemma}{theorem}

\author{Xu Cheng and Eric Renault and Paul Sangrey} 
\title{Derivation of Asymptotic Covariance Matrix}

\date{\today}

\begin{document}

\maketitle

We want to estimate $\omega \coloneqq (\rho, c, \delta, \phi, \pi, \theta)'$ and get its asymptotic distribution.
We also have some auxiliary parameters: $\xi_1 \coloneqq (\beta, \gamma, \psi)'$ and $\xi_2 \coloneqq \phi^2$.
The way we do this by splitting $\omega$ in two parts.
The first $\omega_s$ is the vector of purely structural parameters: $(\phi, \pi, \theta)'$.
The second part $\omega_r$ is composed of parameters that we can estimate directly without model-based
cross-equation restrictions, but are of interest.
We collect all of the reduced parameters into a vector: $\xi \coloneqq (\omega_r', \xi_a)'$.
We start by constructing a GMM estimator for $\xi$, we will then show how to convert this into an estimator for
$\omega$.
Throughout, I will denote the partial derivative of some function $f$ with respect to some variable $x$ with
$f_x$.

\section{Stage 1}

We view estimating the first stage as a particular form of GMM.
From standard GMM theory, we know that the following holds, for some asymptotic covariance matrix $\Omega_{\xi}$.

\begin{equation}
    \sqrt{T} (\hat{\xi} - \xi)  \dto N\left(0, \Omega_{\xi}\right)
\end{equation}

We will construct $\Omega_{\xi}$ in three steps.
First, we will derive the asymptotic covariance matrices for each of $\omega_r, \xi_1, \xi_2$.
Then we will show how to combine them into one joint covariance matrix.

\subsection{$\omega_r$}\label{sec:omega_r}

\begin{equation}
    h\left(\sigma^2_{t},\sigma^2_{t+1} \mvert \omega_r\right) \coloneqq 
\begin{bmatrix}
    \sigma^2_{t+1} - (c \delta + \rho \sigma^2_{t}) \\
%
    \sigma^2_{t} \left(\sigma^2_{t+1} - (c \delta + \rho \sigma^2_{t})\right)\\
%
    \sigma^4_{t+1} - \left(c^{2} \delta + 2 c \rho \sigma^2_{t} + \left(c \delta + \rho
    \sigma^2_{t}\right)^{2}\right)\\
%
    \sigma^2_{t} \left(\sigma^4_{t+1} - \left(c^{2} \delta + 2 c \rho \sigma^2_{t} + \left(c \delta + \rho
    \sigma^2_{t}\right)^{2}\right)\right)\\

    \sigma^4_{t} \left(\sigma^4_{t+1} - \left(c^{2} \delta + 2 c \rho \sigma^2_{t} + \left(c \delta + \rho
    \sigma^2_{t}\right)^{2}\right)\right)\\
\end{bmatrix}
\end{equation}

By standard GMM theory, the following holds, if the weighting matrix \newline $W_{\omega_r,T} \pto \E[h(\sigma^2_t,
\sigma^2_{t+1} \ivert \omega_r)' h(\sigma^2_t, \sigma^2_{t+1} \ivert \omega_r)]^{-1}$.
We have 

\begin{equation}
    \sqrt{T}(\widehat{\omega}_r - \omega_r) \dto N\left(0, \Omega_{\omega_r}\right),
\end{equation}

\noindent where

\begin{equation}
    \Omega_{\omega_r} \coloneqq \left(\E\left[h_{\omega_r}(\sigma^2_{t}, \sigma^2_{t+1}, \xi_{1})\right]'
    \E[h(\sigma^2_t, \sigma^2_{t+1} \ivert \omega_r)' h(\sigma^2_t, \sigma^2_{t+1} \ivert \omega_r)]^{-1}
    \E\left[h_{\omega_r}(\sigma^2_{t}, \sigma^2_{t+1}, \xi_{1})\right]\right)^{-1}
\end{equation}

\noindent We estimate this by replacing the population expectations and covariances by their sample counterparts.

\subsection{$\xi_1$}\label{sec:est_xi2}

We estimate $\xi_1$ by weighted least squares, a special case of GMM. 

\begin{equation}
    \E\left[r_{t+1} \mvert \sigma^2_t, \sigma^2_{t+1}\right]  = \gamma + \beta \sigma^2_t + \psi \sigma^2_{t+1}
\end{equation}

The only unusual part is we know that $\Var(r_{t+1} \ivert \sigma^2_t, \sigma^2_{t+1}) = (1-\phi^2)
\sigma^2_{t+1}$.
Consequently, the regression results  are more efficient if we adjust for heteroskedasticity.
However, we do not know $\phi$, and so this might seem impossible.
However, since time-invariant parts of heteroskedasticity adjustments cancel, reweighting by the inverse of
$\sigma^2_{t+1}$ achieves is equivalent to the optimal reweighting. 
Also, since $\sigma^2_{t+1}$ is contained in the conditioning set, the fact that it is viewed as a random variable
in other parts  of the regression is irrelevant.
Let $u_t = \frac{r_{t+1} - (\gamma + \beta \sigma^2_{t} + \psi \sigma^2_{t+1}}{\sigma^2_{t+1}}$.

Since this regression is exactly identified, any positive-definite weight matrix, including the identity is
optimal.
Consequently, we have the following result, where the WLS covariance matrix has the standard form:

\begin{equation}
    \Omega_{\xi_1} = \E\left[\left(1, \sigma^2_{t}, \sigma^2_{t+1}\right) \left(1, \sigma^2_{t},
    \sigma^2_{t+1}\right)'\right]^{-1} \Var\left(u_t\right).
\end{equation}


\subsection{Step 3: $\xi_2$}

We know that $\Var(r_{t+1} \vert \sigma^2_{t+1} \sigma^2_t) = (1-\phi^2) \sigma^2_{t+1}$.
This implies $\Var(\frac{r_{t+1}}{\sigma_{t+1}} \vert \sigma^2_{t+1}, \sigma^2_t) = 1 - \phi^2$.
Since we consistently estimate the conditional mean of $r_{t+1}$ in \cref{sec:est_xi2}, the residuals ---
$\widehat{u}_t = \frac{r_{t+1} - \widehat{\gamma} - \widehat{\beta}\sigma^2_t - \widehat{\psi}
\sigma^2_{t+1}}{\sigma_{t+1}}$ --- satisfy $\frac{1}{T} \sum_{t=1}^T \hat{u}_t^2 \pto (1 - \phi^2)$.

Define $\widehat{\xi}_2 \coloneqq 1 - \frac{1}{T} \sum_{t=1}^T \hat{u}_t^2 \pto (1 - \phi^2)$.
Then $\widehat{\xi}_2 \dto N(0, \Omega_{\xi_2})$ for some covariance matrix $\Omega_{\xi_2}$, since this is a  GMM
estimator.
What is $\Omega_{\xi_2}$.
Since we are just identified, standard GMM theory says it is the covariance of the moment condition scaled by the
appropriate derivative. 
Since we are estimating a mean shifted by a constant, the derivative equals $1$.
Consequently, $\Omega_{\xi_2} = \Var(\frac{u^2_t}{\sigma^2_{t+1}})$, which can be estimated by
$\frac{1}{T} \sum_{t=1}^T (\frac{\widehat{u}_t^2}{\sigma^2_{t+1}} - \frac{1}{T} \sum_{t=1}^T
\frac{\widehat{u}_t^2}{\sigma^2_{t+1}})^2$, i.e.\@ the sample covariance of the squared residuals. 

\subsection{Combining $\Omega_{\omega_r}, \Omega_{\xi_2}$, and $\Omega_{\xi_2}$}

Each of $\Omega_{\xi_i}$ are of the form $(\E[h_{\xi_{i}}(\sigma^2_{t+1}, \sigma^2_t \ivert \xi_i)]'
\Var(h(\sigma^2_{t+1}, \sigma^2_t \ivert \xi_i) \ivert \xi_i)^{-1} \E[h_{\xi_{i}}(\sigma^2_{t+1}, \sigma^2_t
\ivert \xi_i) \ivert \xi_i])^{-1}$.
Consequently, the off-diagonal blocks of the joint covariance matrix $\Omega$ come from two places:
the derivatives and the covariance of the moments.  
Since the moments in the first stage do not depend on the parameters in the second stage, and vice-versa, the
derivatives to not cause any co-movement. 
The other cases are trickier, and so we consider them each in turn.

Consider the covariance between $h(\sigma^2_{t+1}, \sigma^2_t, \omega_r)$ and $h(\sigma^2_{t+1}, \sigma^2_t,
\xi_2)$, which we can rearrange since the moments are mean zero.  

\begin{gather}
    \Cov\left(h\left(r_{t+1}, \sigma^2_{t+1}, \sigma^2_t \mvert \omega_{r} \right) ,
      h\left(\sigma^2_{t+1} \sigma^2_t \mvert \xi_2 \right) \right) 
%
%
    = \E\left[h(r_{t+1},  \sigma^2_{t+1}, \sigma^2_t \mvert \omega_r) h\left(\sigma^2_{t+1}, \sigma^2_t \mvert
       \xi_2 \right) \right]
%
%
       \intertext{By the law of iterated expectations.}
%
%
    = \E\left[\E[h\left(r_{t+1},  \sigma^2_{t+1}, \sigma^2_t \mvert \omega_r\right) \mvert \sigma^2_{t+1},
       \sigma^2_t] \E\left[h\left(\sigma^2_{t+1}, \sigma^2_t \mvert \xi_2 \right) \mvert \sigma^2_{t+1},
       \sigma^2_t\right] \right]
\end{gather}

The first term in the expression above equals zero, and, hence, so does the entire expression.
In other words, the first two set of moment conditions are independent.  
By an identical argument, the first and third moments are independent as well.

We now consider how the second and their sets of moments are related.
Since the derivatives are with respect to different parameters (and constant) no dependence arises from there.
The question is how are the moment conditions in the second and third steps related.
The second stage moment condition is a conditional mean and third stage moment is a conditional covariance.
Let $u_t$ denote the error term in that regression (as it did above).

\begin{equation}
    \E\left[\E\left[\frac{r_{t+1} - \E\left[r_{t+1}\mvert \sigma^2_{t+1} \sigma^2_t\right]}{\sigma_{t+1}} \right]
    \E\left[\frac{(r_{t+1} - \E\left[r_{t+1} \mvert \sigma^2_{t+1} \sigma^2_t\right])^2}{\sigma^2_{t+1}}\right]
    \right] 
%
    = \E\left[\E\left[\frac{u_t u_t^2}{\sigma^2_{t+1}}\right] \mvert \sigma^2_t, \sigma^2_{t+1} \right]  = 0
\end{equation}

Since $u_t$ is conditionally Gaussian, its conditional third moment equals zero.  
By the law of iterated expectations, its unconditional moment does as well.

In other words, the second and third set of moments are uncorrelated.
Now, the careful reader might be worried about filling in the population expectations instead of their estimators
in the regression above.
However, since the expectations are linear and consistently estimable, this error vanishes in the limit. 
Intuitively, OLS mean and variance estimates are asymptotically independent.

In addition, since all three components are asymptotically independent; the inverse of a block-diagonal matrix is
block-diagonal, and we using optimal weighting matrices in each part, we are using an
optimal weighting matrix for $\xi$, not just its components.


\section{Stage 2}

In this stage, we convert the estimates for $\xi$ into estimates for $\omega$. 
We do this by specifying a link function.
Since $(\rho, c, \delta)'$ shows up in both $\xi$ and $\omega$, the link function for those parameters is the
identify function.
The third set of parameters $\xi_3 = \phi^2$, and so we use that as a link function.
We also use the reduced-form estimates as link functions, as well, i.e.\@

\begin{equation}
    g(\xi, \omega) = \xi - (\rho, c, \delta, \beta(\rho, c, \phi, \pi, \theta), \gamma(\rho, \delta, c, \phi, \pi,
    \theta), \psi(\rho, c, \phi, \theta), \phi^2)'
\end{equation}

We specify $g(\xi, \omega)$ in this way because it gives us the correct off-diagonal elements.
If we estimated $(\rho, c, \delta)$ by themselves and just plugged them in here, we would have to relate the
off-diagonal terms in a separate step.
In addition, by using the optimal weight matrix for this stage, we estimate all of the parameters as efficiently
possible using our moment conditions.

Doing this implies we must be careful and treat the $\rho,c$, and $\delta$ on each sides of the equation as
different.
The 2nd-stage sample criterion function is 

\begin{equation}
    Q_T(\omega) \coloneqq \frac{1}{2} g(\widehat{\xi}_T, \omega)' \W_{T} g(\widehat{\xi}_T, \omega)
\end{equation}

\noindent  with second stage weight matrix $\W_T$.
We need to estimate $\omega$, and so we differentiate and get the first-order condition at $\omega_0$, where $\W
\coloneqq \lim_{T \to \infty} \E[W_T]$.
This gives

\begin{equation}
    \frac{\partial Q_T}{\partial \omega}(\omega_0) =  g_{\omega}\left(\xi_0, \omega_0\right)  \W g\left(\xi_0,
    \omega_0\right) = 0.
\end{equation}

\noindent We now expand $\widehat{\xi}_T$ around $\xi_0$, for some $\widetilde{\xi}_T$ between $\xi_0$ and
$\widetilde{\xi}_T$, 

\begin{align}
    \sqrt{T} \frac{\partial Q}{\partial \omega}(\omega_0) 
%
    &= g_{\omega}\left(\omega_0, \widehat{\xi}_T\right) \W_T \left[\sqrt{T} g\left(\omega_{0},\xi_0\right) +
       g_{\xi}\left(\omega_{0}, \widetilde{\xi}_T\right)' \sqrt{T} \left(\widetilde{\xi}_T - \xi_0\right)\right]
%
%
    \intertext{The first term equals zero by assumption, and the derivative is the identity matrix.}
%
%
    &= g_{\omega}\left(\omega, \widehat{\xi}_T\right) \W_T \left[ \sqrt{T} \left(\widetilde{\xi}_T -
       \xi_0\right)\right]
\end{align}

\noindent We also need to compute the Hessian and evaluate it at $\widehat{\omega}_T$ a consistent estimator for
$\omega$.

\begin{align}
    \frac{\partial^2 Q}{\partial \omega \partial \omega'}(\widehat{\omega}_T) &=
%
    g_{\omega}\left(\widehat{\omega}_T, \widehat{\xi}_T\right)' \W_T g_{\omega} \left(\widehat{\omega}_T,
    \widehat{\xi}_T\right)+ g_{\omega, \omega}\left(\widehat{\omega}_T, \widehat{\xi}_T\right) \W_T
    g\left(\widehat{\omega}_T, \widehat{\xi}_T\right)' \\
%
    &\pto g_{\omega}\left(\omega, \xi_0\right)' \W g_{\omega} \left(\omega, \xi_0\right) + 0. 
\end{align}

\noindent if we use the optimal weight matrix --- $\W \coloneqq \E[g_{\omega_s}(\theta_0, \xi_0)]$.
Standard extremum estimator theory gives

\begin{equation}
    \sqrt{T} \left(\widehat{\omega}_T - \omega_{0}\right)  \dto N\left(0, \left(\E[g_{\omega}(\theta_0, \xi_0)]'
    \Omega_{\xi}^{-1} \E[g_{\omega}(\theta_0, \xi_0)]\right)^{-1}\right).
\end{equation}

\noindent The covariance in the middle is GMM-covariance of the reduced-form parameters.
We estimate it by plugging $\widehat{\xi}$ into to the formulas above and their derivatives.
We estimate the asymptotic covariance matrix by replacing its components with their sample counterparts.


\section{Asymptotic Distribution of the Reduced-Form Parameter}

This section gives the asymptotic distribution of the reduced-form parameter. 

Write $\omega =(\omega_{1},\omega_{2},\omega_{3}),$ where $\omega_{1}=(\rho ,c)$, $\omega_{2} = (\gamma ,\beta ,\psi)$, and $\omega_{3} = \phi ^{2}$. 
Below we describe the estimator $\widehat{\omega}_{1},\widehat{\omega}_{2},\widehat{\omega}_{3}$ and provide
the asymptotic distribution of $\widehat{\omega} = (\widehat{\omega}_{1},\widehat{\omega
}_{2},\widehat{\omega}_{3})$.
We estimate these parameters separately because $\omega_{1}$ only shows up in the conditional mean and variance of
$r_{t+1};$ $\omega_{2}$ only shows up in the conditional mean of $\sigma_{t+1}^{2};$ and $\phi $ only shows up in
the conditional variance of $\sigma_{t+1}^{2}.$

We estimate $\omega_{1}$ by GMM based on the moment condition: 

\begin{eqnarray}
    \mathbb{E}[h_{t}(\omega_{1,0}) & = &0,\text{ where}  \nonumber \\
%
    h_{t}(\omega_{1}) & = &\left(
        \begin{array}{c} 
            \sigma_{t+1}^{2}-\left( c\delta +\rho \sigma_{t}^{2}\right)  \\ 
%
            \sigma_{t}^{2}\left( \sigma_{t+1}^{2}-\left( c\delta +\rho \sigma _{t}^{2}\right) \right)  \\ 
%
            \sigma_{t+1}^{4}-\left( c^{2}\delta +2c\rho \sigma_{t}^{2}+\left( c\delta +\sigma_{t+1}^{2}-\left(
            c\delta +\rho \sigma_{t}^{2}\right) ^{2}\right) \right)  \\ 
%
            \sigma_{t}^{2}\left( \sigma_{t+1}^{4}-\left( c^{2}\delta +2c\rho \sigma _{t}^{2}+\left( c\delta
            +\sigma_{t+1}^{2}-\left( c\delta +\rho \sigma _{t}^{2}\right) ^{2}\right) \right) \right)  \\ 
%
            \sigma_{t}^{4}\left( \sigma_{t+1}^{4}-\left( c^{2}\delta +2c\rho \sigma _{t}^{2}+\left( c\delta
            +\sigma_{t+1}^{2}-\left( c\delta +\rho \sigma _{t}^{2}\right) ^{2}\right) \right) \right) 
        \end{array}\right).
\end{eqnarray}
%
The optimal GMM estimator is
%
\begin{eqnarray}
    \widehat{\omega}_{1} & = & \underset{\omega_{1}\in \Lambda_{1}}{\arg\min} 
%
    \overline{h}_{T}(\omega_{1})^{\prime}W_{T}\overline{h}_{T}(\omega_{1}), \text{ where}  \nonumber \\ 
%
    \overline{h}_{T}(\omega_{1}) & = &T^{-1}\sum_{t = 1}^{T}h_{t}(\omega_{1}), \nonumber \\
%
    W_{T} & = &T^{-1}\sum_{t = 1}^{T}h_{t}(\widetilde{\omega}_{1})h_{t}(\widetilde{\omega}_{1})^{\prime
   }-\overline{h}_{T}(\widetilde{\omega}_{1})\overline{h} _{T}(\widetilde{\omega}_{1})^{\prime},
\end{eqnarray}
%
where $\widetilde{\omega}_{1}$ is the preliminary GMM estimator based on the identify covariance matrix.

We estimate $\omega_{2}$ by the GLS estimator because $\gamma, \beta, \psi$ are the intercept and linear
coefficients of the conditional mean function and the conditional variance is proportional to $\sigma_{t+1}^{2}$. 
Define $x_{t} = \sigma_{t+1}^{-1}(1,\sigma_{t}^{2},\sigma_{t+1}^{2})$ and $y_{t} = \sigma_{t+1}^{-1}r_{t+1}$. 
The GLS\ estimator of $\omega_{2}$ is
%
\begin{equation}
    \widehat{\omega}_{2} = \left( \sum_{t = 1}^{T}x_{t}x_{t}^{\prime}\right) ^{-1}\sum_{t = 1}^{T}x_{t}y_{t}.
\end{equation}

We estimate $\omega_{3}$ by the sample variance estimator. 
Define 
%
\begin{equation}
    \widehat{y}_{t} = x_{t}\widehat{\omega}_{2} = \sigma_{t+1}^{-1}(\widehat{\gamma}+\widehat{\beta
   }\sigma_{t}^{2}+\widehat{\psi}\sigma_{t+1}^{2}).
\end{equation}
%
The estimator of $\omega_{3}$ is 
\begin{equation}
    \widehat{\omega}_{3} = \max \{1-T^{-1}\sum_{t = 1}^{T}\left( y_{t}-\widehat{y}_{t}\right) ^{2},0\}.
\end{equation}%
[**XC. In practice, do we need to impose the estimator is positive?]

The next lemma provides the asymptotic distribution of the estimator $\widehat{\omega}$. 
Let $h_{\omega ,t}(\omega_{1})\in \mathbb{R}^{5\times 2}$ denote the derivative of $h_{t}(\omega_{1})$ w.r.t.\@
$\omega_{1}$.
Define
%
\begin{eqnarray}
    \Omega_{1} & = &\left \{ \mathbb{E}\left[ h_{\omega ,t}\left( \omega _{1,0}\right) \right]^{\prime}
        \mathbb{E}[h_{t}(\omega_{1,0})h_{t}(\omega _{1,0})^{\prime}]^{-1}\mathbb{E}\left[
    h_{\omega,t}\left(\omega_{1,0}\right) \right] \right \} ^{-1},  \notag \\ 
%
    \Omega_{2} & = &\mathbb{E}\left[ x_{t}x_{t}^{\prime}\right] ^{-1}\mathbb{E[(} y_{t}-x_{t}^{\prime
   }\omega_{2,0})^{2}],  \notag \\
%
    \Omega_{3} & = &\mathbb{V[}\left( y_{t}-x_{t}^{\prime}\omega_{2,0}\right)^{2}]
\end{eqnarray}

\begin{lemma}
Suppose Assumptions *** hold. Then

\begin{equation}
    T^{1/2}\left( 
%
    \begin{array}{c}
        \widehat{\omega}_{1}-\omega_{1,0} \\ 
        \widehat{\omega}_{2}-\omega_{2,0} \\ 
        \widehat{\omega}_{3}-\omega_{3,0}%
    \end{array}\right) 
%
    \rightarrow_{d}\xi_{\omega}  = 
%
   \left(\begin{array}{c}
            \xi_{\omega 1} \\ 
            \xi_{\omega 2} \\ 
            \xi_{\omega 3}%
   \end{array}\right) 
%
   \sim N\left( 0,
%
    \begin{array}{ccc}
        \Omega_{1} & 0 & 0 \\ 
        0 & \Omega_{2} & 0 \\ 
        0 & 0 & \Omega_{3}%
    \end{array}\right).
\end{equation}
\end{lemma}


\begin{proof}
    Will be added later.
\end{proof}


\section{Robust Inference for Risk Price}

The reduced-form parameters are $\omega  = (\rho ,c,\gamma ,\beta ,\psi ,\phi ^{2})$. 
Using the conditional mean and conditional variance derived in the paper, we estimate $\, \omega_{1} = (\rho ,c)$
by the GMM estimator, estimate $\omega_{2} = (\psi ,\beta ,\gamma )$ by the GLS estimator, and estimate
$\omega_{3} = \phi^{2}$ by the method of moments estimator for the variance.

We can show that the estimator $\widehat{\omega}$ satisfies

\begin{equation}
    T^{1/2}(\widehat{\omega}-\omega_{0})\rightarrow_{d}\upsilon_{\omega}\sim N(0,V).
\end{equation}%

See the next section for details. 
Note that these estimators do not involve the structural parameters $\theta $ and $\pi$.
We do not plug in $\beta ,\gamma ,\psi $ as functions of $\theta $ and $\pi .$ Instead, we treat $\beta ,\gamma
,\psi $ just as linear coefficients and estimate them by GLS.

We estimate the structural parameters $\theta $ and $\pi $ using $\widehat{ \omega}$ and the link functions
specified below. 
First, we know that
%
\begin{equation}
    \label{psi_fn} 
    \psi_{0}=\phi_{0}\left( c_{0}\left( 1+\rho_{0}\right) \right) ^{-1/2}+\left( 1-\phi_{0}^{2}\right)
    /2-(1-\phi_{0}^{2})\theta_{0}
\end{equation}
%
when all parameters are evaluated at the true values. 
This equation strongly identifies $\theta_{0}$ because $\phi_{0}$ is assumed to be negative and bounded away from
1 in magnitude.. 
It follows from (\cref{psi_fn}) that

\begin{equation}
    \theta_{0} = L(\omega_{0})=-(1-\phi_{0}^{2})^{-1}\left[ \psi_{0}-\phi_{0}\left( c_{0}\left( 1+\rho_{0}\right)
    \right)^{-1/2}-\left( 1-\phi_{0}^{2}\right) /2\right] .
\end{equation}
%
Thus, we estimate $\theta_{0}$ by
%
\begin{equation}
    \widehat{\theta}=L(\widehat{\omega}).
\end{equation}
%
By the delta method, we know that
%
\begin{equation}
    T^{1/2}(\widehat{\theta}-\theta_{0})\rightarrow_{d}L_{\omega}(\omega_{0})^{\prime}\upsilon_{\omega},
\end{equation}
%
where $L_{\omega}(\omega )\in R^{d_{\omega}}$ denote the derivative of $L(\omega )$ w.r.t.\@ $\omega$. 
The inference for $\theta $ is standard. 
A confidence interval for $\theta $ can be obtained by inverting the $t$-statistic with a critical value obtained
from the standard normal distribution.

Next, we consider inference for the structural parameter $\pi$. 
This is a non-standard problem because $\pi $ is potentially weakly identified. 
Define
%
\begin{equation}
    g(\pi ,\omega )=\left( 
%
    \begin{array}{c}
        \gamma -\left[ B\left( \pi +C\left( \theta_{L}-1\right) \right) -B\left( \pi +C\left( \theta_{L}\right)
        \right) \right]  \\ 
%
        \beta -\left[ A\left( \pi +C\left( \theta_{L}-1\right) \right) -A\left( \pi +C\left( \theta_{L}\right)
        \right) \right] 
    \end{array}\right),
%
    \text{ where}\ \theta_{L} = L(\omega).
\end{equation}
%
We know 
%
\begin{equation}
    g(\pi_{0}, \omega_{0})=0 \in \mathbb{R}^{2}.
\end{equation}
%
Inference on $\pi $ is based on the function $g(\pi ,\widehat{\omega})$ because $\widehat{\omega}$ is a
consistent estimator of $\omega_{0}$.
By the consistency of $\widehat{\omega}$,
%
\begin{equation}
    T^{1/2}\left[ g(\pi, \widehat{\omega}) - g(\pi,\omega_{0})\right] \Rightarrow \upsilon(\pi) = G(\pi,
    \omega_{0})^{\prime}\upsilon_{\omega},
\end{equation}%

\noindent where $G(\pi ,\omega )$ denote the derivative of $g(\pi ,\omega )$ w.r.t.\@ to $\omega$. 
The Gaussian process $\upsilon(\pi)$ has covariance kernel 
%
\begin{equation}
    \Sigma (\pi_{1},\pi_{2})=G(\pi_{1},\omega_{0})^{\prime}VG(\pi_{2},\omega_{0}).
\end{equation}
%
We can estimate $\Sigma (\pi_{1},\pi_{2})$ by 
%
\begin{equation}
    \widehat{\Sigma}(\pi_{1},\pi_{2})=G(\pi_{1},\widehat{\omega})^{\prime}
    \widehat{V}G(\pi_{2},\widehat{\omega}),
\end{equation}

\noindent where $\widehat{V}$ is a consistent estimator of $V.$

We construct a confidence interval for $\pi $ by inverting tests $H_{0}:\pi =\pi_{0}$ vs $H_{0}:\pi \neq \pi_{0}$. 
The test statistic is the QLR statistic:
%
\begin{equation}
    QLR=Tg(\pi_{0},\widehat{\omega})^{\prime}\widehat{\Sigma}(\pi_{0},\pi _{0})^{-1}g(\pi_{0},\widehat{\omega})
    - \underset{\pi \in \Pi}{\min}Tg(\pi ,\widehat{\omega})^{\prime}\widehat{\Sigma}(\pi,\pi)^{-1}
    g(\pi,\widehat{\omega}).  
\end{equation} 

To obtain the critical value, we follow the conditional inference approach in Andrews and Mikusheva (2016). To
this end, first construct a projection residual process:
%
\begin{equation}
    h(\pi ,\widehat{\omega})=g(\pi ,\widehat{\omega})-\widehat{\Sigma}(\pi ,\pi_{0})\widehat{\Sigma
   }(\pi_{0},\pi_{0})^{-1}g(\pi_{0},\widehat{\omega}).
\end{equation}
%
By construction, $h(\pi ,\widehat{\omega})$ and $g(\pi_{0},\widehat{\omega})$ are independent asymptotically. 
Conditional on $h(\pi ,\widehat{\omega})$, we obtain the $1-\alpha $ quantile of the QLR statistic, denoted
$c_{\alpha}(h)$, by sampling from the asymptotic distributions of $g(\pi_{0},\widehat{\omega})$ under the null.
Specifically, we take independent draws $\upsilon^{\ast}\sim N(0,\Sigma (\pi_{0},\pi_{0}))$ and produce simulated
process:
%
\begin{equation}
    g^{\ast}(\pi ,\widehat{\omega}) = h\left(\pi ,\widehat{\omega}\right) + \widehat{\Sigma} (\pi
    ,\pi_{0})\widehat{\Sigma}(\pi_{0},\pi_{0})^{-1}\upsilon^{\ast}.
\end{equation}%
%
We then calculate
\begin{equation}
    QLR^{\ast}=Tg^{\ast}(\pi_{0},\widehat{\omega})^{\prime}\widehat{\Sigma} (\pi_{0},\pi_{0})^{-1}g^{\ast
   }(\pi_{0},\widehat{\omega})-\underset{\pi \in \Pi}{\min}Tg^{\ast}(\pi ,\widehat{\omega})^{\prime
   }\widehat{\Sigma} (\pi ,\pi )^{-1}g^{\ast}(\pi ,\widehat{\omega}), 
\end{equation}
%
which is a random drawn from the conditional distribution of the $QLR$ statistic given $h_{T}(\pi ,\widehat{\omega
})$, when $g(\pi_{0},\widehat{ \omega})$ is drawn from its asymptotic distribution. 
In practice, we repeat this process for a large number of times and obtain $c_{\alpha}(h)$ by simulation.

We reject the null $H_{0}:\pi =\pi_{0}$ if $QLR\geq c_{\alpha}(h)$. 
The confidence interval for $\pi $ is the collection of null values that are not rejected as the null value. 
Note that the construction of this CI\ does not involve estimation of $\pi$.


\end{document}



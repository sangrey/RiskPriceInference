\documentclass[11pt, letterpaper, twoside, final]{article}
\usepackage{risk_price_inference}
\addbibresource{riskpriceinference.bib}

\author{Xu Cheng\thanks{University of Pennsylvania, The Perelman Center for Political Science and
    Economics, 133 South 36th Street, Philadelphia, PA 19104, \href{mailto:xucheng@upenn.edu}{xucheng@upenn.edu}}
    \and 
    Eric Renault\thanks{Brown University, Department of Economics -- Box B, 64 Waterman Street, Providence, RI
    02912, \href{mailto:eric_renault@brown.edu}{eric\_renault@brown.edu}}
    \and  Paul Sangrey\thanks{University of Pennsylvania, The Perelman Center for Political Science and
    Economics, 133 South 36th Street, Philadelphia, PA 19104, \href{mailto:paul@sangrey.io}{paul@sangrey.io}}}
    
\title{Inference for Risk Prices using Equity Data}

\date{\today}

\begin{document}

\begin{titlepage}


    \maketitle
    \thispagestyle{empty}
    \addtocounter{page}{-1}

    \begin{abstract} \singlespacing \noindent 
        How risks and returns are traded off is arguably the central object of study in modern asset pricing. 
        Even though the theoretical literature cohesively argues there should be a strong positive correlation
        between expected returns and volatility, empirically measuring it has proven difficult. 
        In addition, the option pricing literature shows that volatility enters as its own risk factor, not just
        as a predictor of returns.
        However, because as there is a negative contemporaneous correlation between the volatility and return
        processes and the return and volatility relationship is inherently nonlinear measuring these risk prices
        have proven delicate. 
        We develop methods to provide  valid inference for the prices of equity and volatility risk using only
        equity data that directly handles this identification problem. 
        We do this by adapting the results in weak identification literature that uses drifting sequences in order
        to be robust to the various parameters' identification strength. 
    \end{abstract}

    \jelcodes{C12, C14, C38,  C58, G12}

    \keywords{identification, robust inference, stochastic volatility, leverage, equity risk premium, volatility
    risk premium, risk price, confidence set, asymptotic size}

\end{titlepage}

\tableofcontents
\newpage

\phantomsection
\addcontentsline{toc}{section}{Introduction}

Modern finance is all about the risk return trade offs that investors face and how to optimally respond to them. 
In particular, the central question of asset pricing is what drives expected returns.
Standard economic theory predicts you must compensate investors with higher expected returns when they face more
risk.
In other words, we would expect a positive relationship between the mean and volatility of returns.
In \gentextcites{sharpe1964capital,lintner1965security} capital asset pricing model (CAPM) the expected
return varies proportionally with the volatility of the market return.
However, to price option prices well, we need not only a price of equity risk that loads upon the market variance,
but also a price of volatility risk as well \parencite{christoffersen2013capturing}.
Investors care not just about how their returns co-move with the market return but also how they co-move with the
market volatility.

This implies that current volatility will affect expected returns in two ways. 1) Through investors' preferences
over future market returns. 2) Through investors' preferences over future volatility. 
As one might expect distinguishing these two effects is quite difficult. 
The difference between how the two effect risk premia comes through their different nonlinear relationships in the
presence of contemporaneous correlation between the volatility and return processes. 
We adopt the framework of \textcite{khrapov2016affine}, which is a discrete-time exponentially-affine stochastic
volatility model.
They mention a potential identification strategy in that paper. 
We develop this strategy,  characterizing how the identification strength of the risk prices varies over the
parameter space, and show how to perform uniformly valid inference.

To take a step back and more fully develop empirical structure at hand, we consider how to measure the
relationship between market volatility and expected returns if we assume that this relationship is constant and
linear. 
Even in this simple case, unlike the consensus in the theoretical literature, the empirical literature has found
pinning down this relationship quite difficult.
Not only has its magnitude proven difficult to determine, but various estimates even differ in sign,
\parencite{lettau2010measuring}.

The empirical literature, which we examine in more detail in the literature review, has focused on point estimates
of this magnitude. 
However, if individual investors are ambiguity averse as in \textcite{hansen2001robust, jiu2012ambiguity}, they
will care not just about how the representative investor prices volatility but also their uncertainty regarding
this estimate. 
Furthermore, when economists calibrate models, they must know how precisely the data determine these parameters. 
If the need to alter the parameter value slightly in order to make their model perform well, are they bringing in
more restrictions or data to more precisely determine the parameter of interest or are they using a value that the
data tell us is incorrect?

Clearly, as obtaining precise believable point estimates of the price of volatility risk has proven quite
difficult, we should expect doing valid inference to be even more delicate.
To the best of our knowledge, this is the first paper to directly tackle this question.
Various authors report confidence intervals as well as their point estimates.
However, they do not take into account the weak identification that makes getting the point estimate difficult in
computing these estimates.

Why is it that measuring this price is so difficult when the theoretical literature is so cohesive?
Econometrically, it is because the volatility price is weakly identified, as in \textcite{andrews2012estimation},
in that the strength of the identification of the price depends upon the value of other parameters. 
This obviously begs the question --- what are these parameters? 

To estimate the risk prices, there are three different phenomena that must be distinguished.
First, the econometrician must disentangle the volatility feedback effect (leverage) which is a contemporaneous
relationship between the volatility and returns from the risk premium, which is a relationship between volatility
and expected returns. 
It is not a contemporaneous relationship, but rather a predictable one. 

Second, and just as important.
We must separate the equity risk price and the volatility risk price. 
In general, and we will show this below, the equity risk price is strongly identified even in the presence of the
volatility risk price. 
There is a simple linear relationship that we can use between the equity risk price and the expected return.
However, volatility risk being another priced factor will introduce nonlinear nuisance terms into this regression. 
They will not matter asymptotically, but in finite samples, they likely do.

Returning to identifying the price of volatility risk, we know that when the strength of identification varies over
the parameter space and we lack identification entirely for some parameter values, the finite sample distributions
are highly nonstandard. 
Consequently, the usual asymptotic approximations do not perform well. 

In addition, the finite-sample distributions are the relevant ones here. 
Even though we often have thousands of observations, since the variation in the expected return is so much smaller
than variation in the return itself, we have a very low signal-to-noise ratio.
Consequently, our estimators will continue to behave as if they were taken from a \textquote{small} sample even in
\textquote{large} samples.

The obvious next step is considering how we need to do this in practice.
Since the contribution of this paper is in terms of methodology and empirical results, we will take a model from
the literature that has the various components, instead of developing our own pricing model.
In particular, we take the model from \textcite{khrapov2016affine} and use it to estimate the relevant parameters. 

This model has a few nice features. 
First, it has both equity and volatility prices and a leverage effect. 
As such, it is the natural discrete-time analogue of the \textcite{heston1993closedform} option pricing model. 
It has an exponentially affine stochastic discount factor  and shares with \textcite{heston1993closedform} the
advantage of having a structure preserving change of measure between the physical and risk-neutral models.
By doing our analysis in discrete-time we are able to more directly compare our results to risk-premia estimates
outside of the option pricing literature and the jumps in high-frequency innovations will not dramatically affect
our results.  
If we were to use a diffusion process in continuous time, we would be severely counterfactually constraining the
higher-order  moments of the process in way that would likely bias our inference. 

As far as estimation is concerned, we derive a series of conditional means and variances.
We then take these means and variances and plug them into a general method of moments (GMM) criterion.
The data we use are the bivariate series $(r_{t+1}, \sigma^2_{t+1})$.
$r_{t+1}$ is the daily return on some asset, and we use its associated realized volatility for $\sigma^2_{t+1}$.
We go into further detail in \cref{sec:data} regarding how we obtained it, the time-span covered, and so forth.

\section{Literature Review}\label{sec:lit_review}


\section{The Model}\label{sec:model}

\addtocounter{subsection}{1}

We estimate the prices of some factors using moment conditions derived from a pricing model. 
As is standard in that literature, we will do  this by specifying the physical and risk-neutral measures and their
relationship, i.e.\@ the stochastic discount factor or pricing kernel.
This is non-trivial because we only observe equity data and so can only use moments with respect to the physical
measure to estimate the parameters. 
However, as will be seen in detail below the risk prices govern the stochastic discount factor (SDF), not the
physical measure directly. 
Consequently, we need to relate the physical and risk-neutral measures through SDF closely in order to get
restrictions on the behavior of the physical measure in terms of the risk prices. 

Let $\F_t$ be the representative investor's information set at time $t$, and $P_t$ be the price on the asset in
question, with associated return $r_{t+1}$ and volatility  $\sigma^2_{t+1}$.
Then standard asset pricing theory says there exists a stochastic discount factor $M_{t,t+1}$ that prices the
assets. 
% The advantage of defining $\QQ$ is that for some function --- $f$ -- of the future excess return --- $r_{t+1}$  --
% and volatility $\sigma^2_{t+1}$ and potentially the current information available -- $\F_t$, we can price this
% payoff as its expectation with respect to $\QQ$.
% In other  words, the price of $f(r_{t+1}, \sigma^2_{t+1}, \F_t)$ satisfies \cref{eqn:risk_neutral_measure_defn}
% for all $t$ and for all $f$.  
% This is useful because we can choose $f$ to make our estimation convenient.

% \begin{equation}
%     P_t(f) = \E_{\QQ}\left[ f\left(r_{t+1}, \sigma^2_{t+1}, \F_{t}\right)  \mvert \F_{t}\right]
%     \label{eqn:risk_neutral_measure_defn}
% \end{equation}

% Since $P_t(f), r_{t+1}$ and $\sigma_t^2$ are observable, if we specify a model for $\F_t$ in terms of observable
% (to the econometrician) variables, this provides a moment condition that we can use. 
% However, this condition does not identify everything we wish to estimate, in particular it does not identify the
% risk prices because $\QQ$ is not observable from equity data.

% To resolve this we complete the model by defining the stochastic discount factor --- $M_{t, t+1}$ --- as the
% Radon-Nikodym derivative between the $\PP$ and $\QQ$ measures. 
% No arbitrage guarantees that this will exist, \parencite{harrison1978martingales}.
% Since risk prices arise from investors' demand for compensation to hold risk, the risk price show up here 
% in the $\QQ$ measure. 
% (We collect the parameter of interest into a vector --- $\omega$.)


\begin{defn}{Asset Pricing Equation}\label{eqn:asset_pricing_eqn}
    \begin{equation}
        P_t  = \E\left[M_{t,t+1} P_{t+1} \mvert \F_t \right] 
    \end{equation}
\end{defn}

The main issue with \cref{eqn:asset_pricing_eqn} is that $M_{t,t+1}$ is not observable, and so we cannot use it to
estimate models.
However, if we can parameterize $M_{t,t+1}$ as a vector of observables and parameters, then we can. 
We specify the model by parameterizing the dynamics of the $P_t$ and $M_{t, t+1}(\omega)$
The object of interest here is the risk prices, which are such parameters.
Risk prices measure the level of compensation that the representative investor demands in order to face additional
risk.
Consequently, they determine the amount that the stochastic discount factor (SDF) twists the distribution of
$P_t$. 

In what follows, we write down a model for $P_t$ that implies $M_{t,t+1}$ is a known function of these risk
prices, parameters that govern the price and volatility dynamics, the volatility --- $\sigma^2_{t},
\sigma^2_{t+1}$, and the log excess returns --- $r_{t+1}$..
Having done that, we place this $M_{t,t+1}$ into \cref{eqn:asset_pricing_eqn} and estimate the model by matching
the reduced-form and model-implied dynamics of the returns and polities.

Throughout we assume that the two risks that are priced in the market are equity risk and volatility risk as we
discussed in the introduction.
This motivates us to focus on the first two moments of the data.
If higher moments, such as skewness and kurtosis are also priced factors, as in \textcites{harvey2000conditional,
conrad2012exante, chang2013market},  and we used higher sample moments as well to determine the price of our
risk-factors our resulting estimates would be biased, likely substantially so. 
Essentially, we misattribute compensation for bearing risk driven by the higher moments to risk driven by lower
moments.
This implies that we want a model that is paramterized in terms of these first two moments.
We use the conditional autoregressive CAR(1) model here, which we take from
\textcite{darolles2006structural,khrapov2016affine}

Following \textcite{khrapov2016affine}, we assume that the $P_t$ and $\sigma^2_t$  are first-order Markov and
there is no Granger-causality from return to the volatility and that returns are serially independent given the
volatility path.
In other words, the volatility drives all of the dynamics of the process.
Note, we do allow $\sigma^2_{t+1}$ and $r_{t+1}$ to be contemporaneously correlated, which they are in the data. 
We construct the model in terms of a series of Laplace transforms that we parameterize using some functions $A(x),
B(x), C(x), D(x)$, and $E(x)$ for all $x$ in its domain.
We collect the parameters in a vector $\omega$.

\subsection{The Stochastic Discount Factor}\label{sec:deriving_sdf_functions}

We start by parameterizing the stochastic discount factor. 
We assume an exponentially-affine structure for the SDF.
%This implies (along with PP being exponentially-affine that QQ is as well.
We let $\pi$ be the price of volatility risk and $\theta$ be the price of equity risk, and use the following
parameterization for  the SDF.

\begin{defn}{The Stochastic Discount Factor}
    \label{defn:SDF}
    \begin{equation}
        M_{t,t+1}(\pi, \theta) = \exp\left(m_{0}(\pi, \theta) + m_1(\pi, \theta) \sigma_t^2 - \pi \sigma^2_{t+1} -
        \theta r_{t+1}\right) 
    \end{equation}
\end{defn}

Since $M_{t,t+1}$ must integrate to $1$ for all $\sigma^2_t$, we can view $m_{0}(\pi, \theta)$ and $m_1(\pi,
\theta)$ as integration constants.

\subsection{Parameterizing the Physical Measure Dynamics}

We now introduce the data generating process for the volatility.
We use a conditional autoregressive gamma process as in \textcite{gourieroux2006autoregressive, khrapov2016affine}
for the volatility.
This implies that we can parameterize the process using the $A$ and $B$ defined as follows.

\begin{defn}{Volatility Dynamics Functions}
    \label{defn:physical_vol_dynamics}
    \begin{align}
        \label{defn:a_PP}
        A(x) &= \frac{\rho x}{1 + c x}, \\
        \label{defn:b_PP}
        B(x) &= \delta \log(1 + c x),\\
% 
        \text{with}&\  \rho \in [0,1), \quad c > 0, \quad \delta > 0. \nonumber
    \end{align}

\end{defn}

\noindent In this specification, $\rho$ is a persistence parameter, and $c$ is a scaling parameter.
We can see this this clearly from the following forms of the moment conditions.

\begin{remark}[Volatilty Moment Conditions] 
    \label{remark:vol_moment_conditions}
    \begin{align}
        \E\left[\sigma^2_{t+1} \mvert \sigma^2_t \right]  &= \rho \sigma^2_t  + c \delta\\
%
        \Var\left[\sigma^2_{t+1} \mvert \sigma^2_t \right]  &=  2 c \rho \sigma^2_t  + c^2
        \delta 
%
    \end{align}
\end{remark}

Since these two moment conditions are sufficient to derive the unconditional moments, all of the parameters are
identified as long as they are in the interior of their appropriately specified domains.
Intuitively, we are using linear regression to estimate the slope and intercept parameters.


\subsubsection{Return Dynamics}

We then turn to computing the moments of the return distribution. 
This is more subtle than computing the moments of the volatility dynamics because we have to relate the dynamics
of the returns to that of the volatility and to the stochastic discount factor, which is not observed. 
We use the conditional autoregressive CAR(1) model here, which we take from
\textcite{darolles2006structural,khrapov2016affine}
This model  specifies the conditional Laplace transform of the return as a function of $r_{t+1}$ given
$\sigma^2_{t+1}, \sigma^2_t$.
We start by specifying a parametric form for $C(x)$ and $D(x)$, and $E(x)$.
We paramterize $C(x)$ using a quadratic form, which, along with our other rescritions, implies that the return is
conditionall Gaussian.

\begin{defn}{Reduced-Form Functions}
    \label{defn:physical_return_dynamics}
    \begin{align}
        C(x) &\coloneqq \psi x + \frac{1 - \phi^2}{2} x^2, \\
        D(x) &\coloneqq \beta x,  \\
        C(x) &\coloneqq \gamma x. 
    \end{align}
\end{defn}

In addition, as these functions parameterize the Laplace transform, they equal zero at at $x=0$.
It might seem as first, that imposing linearity for $D(x)$ and $E(x)$ is a strong restriction.
This is not actually the case.
Since we assumed that $\sigma^2_{t+1}$ is an integrated volatility and hence greater than $\Var(r_{t+1}
\ivert \sigma^2_{t+1}, \sigma^2_{t})$ and variances are positive, the model implies they are linear.\footnote{We
show this in \cref{lemma:linearity_of_physical_functions} which is located in \cref{app:model_characterization}.}


To give an idea of how the dynamics of $\sigma^2_t$ evolve in this specfication, we compute the first two moments.

\begin{align}
    \label{eqn:rtn_cond_mean}
    \E\left[r_{t+1} \mvert \sigma^2_t, \sigma^2_{t+1}\right]  &= \psi \sigma^2_{t+1}  + \beta \sigma^2_t +
    \gamma \\
    \label{eqn:rtn_cond_vol}
    \Var\left[r_{t+1} \mvert \sigma^2_t, \sigma^2_{t+1}\right]  &= (1 - \phi^2) \sigma^2_{t+1}  
\end{align}



\subsection{Relating the Physical Measure Functions and the Risk Prices}

By plugging in the Laplace transforms for $\sigma^2_{t+1}$ and $r_{t+1}$ into \cref{defn:SDF}, we can derive the
following.

\begin{restatable}[Characterizing the SDF Integration Constants]{lemma}{sdfConstants}
    \label{lemma:characterizing_sdf_integration_constants}
    Let the SDF be given as in \cref{defn:SDF}, and the model be paramterized as in
    \cref{defn:physical_vol_dynamics} and \cref{defn:physical_return_dynamics}.

    Then the SDF constants follow the following equations.\footnote{This is Equation $3.4$ in
    \textcite[3.4]{khrapov2016affine}.}

    \begin{align}
        \label{eqn:sdf_functions_vs_physical_functions}
        m_0(\theta, \pi)  &= E(\theta) + B(C(\theta) + \pi) \\
        m_1(\theta, \pi)  &= D(\theta) + A(C(\theta) + \pi) \nonumber
    \end{align}

\end{restatable}


% Since the right-hand side of \cref{eqn:sdf_functions_vs_physical_functions} are entirely in terms of the physical
% measure functions, which are observable, we can estimate the SDF functions entirely using equity data.
% In other words, we do not need, at least in principle, option data to estimate the SDF.
% In particular, we can determine the risk prices from equity prices if the identified parts of the physical measure
% functions ($\alpha$, $\beta$, etc.) are \textquote{sufficiently} invertible.
% (I.e.\@ the matrix of their derivatives satisfies the appropriate non-singularity conditions.)

\subsubsection{Deriving the Conditional Mean}

The goal moving forward is to solve for $\gamma$, $\beta$, and $\psi$ in terms of $(rho, c, \delta)$ and $(\pi,
\theta)$.
Once we do this, we can use the difference in expected returns between the distribution given $\sigma^2_t$ and the
distribution given $\sigma^2_{t+1}$ and $\sigma^2_t$ to separately identify the two risk prices.
In this section, we focus on $\psi$ because it controls this difference in means and hence controls the strength
of identification.

Intuitively, the reason they are not identified in general is that if volatility and returns are uncorrelated, we
cannot disentangle the shift in the mean because the price of volatility risk from the shift in the mean induced
by price of equity risk.
They both show up in the same way in $\psi$, which we will show later.
However, if we have a leverage effect, when we condition on $\sigma^2_{t+1}$ two different components of the
return are moving -- the mean and variance.
Since we now have two moments, we can identify both parameters.

In general, $\psi$ will have three parts.
First, we have the Jensen's inequality term --- the mean will shift by a value proportional to the variance.
Second, the reduction in variance will shift the price as consumers are risk averse.
Third, since $\sigma^2_{t+1}$ and $r_{t+1}$ are correlated, (the leverage effect) the drift will change directly. 

The goal is to separate out the three components.
The way we do this is by constructing a pseudo-return --- $\widetilde{r}_{t+1}$ --- whose mean is not affected by
the change in variance caused by adding $\sigma^2_{t+1}$ to the information set.

We do this by exploiting the fact that $\E\left[M_{t,t+1}(\theta, \pi) \exp(r_{t+1}) \mvert \sigma^2_t \right] =
1$.
We want to relate $\E\left[\exp(r_{t+1}) \mvert \sigma^2_{t+1}, \sigma^2_t \right]$  to this expression.
Clearly, there are two differences between these expressions.
First, we have the SDF in the first expression.
Second, the conditioning information differs.
We can view these differences as two measure changes.
Since prices here are conditionally log-Gaussian, the measure change  can be paramterized in terms of the
covariance between the log SDF --- $m_{t,t+1} \coloneqq \log M_{t,t+1}$ --- and the return.

If we apply the logarithm to \cref{defn:SDF}, we get the following expression for $m_{t,t+1}$.

\begin{equation}
    \label{eqn:log_sdf}
        m_{t,t+1}(\pi, \theta) = m_{0}(\pi, \theta) + m_1(\pi, \theta) \sigma_t^2 - \pi \sigma^2_{t+1} - \theta
        r_{t+1}
\end{equation}

Except for $\theta r_{t+1}$, the terms on the right are constant given $\sigma^2_t, \sigma^2_{t+1}$, and so they
do not affect the conditional covariance between $m_{t,t+1}$ and $r_{t+1}$.
Hence,

\begin{equation}
    \Cov\left(m_{t,t+1}, r_{t+1} \mvert \sigma^2_{t+1}, \sigma^2_t \right)   
%
    = \Cov\left(-\theta r_{t+1}, r_{t+1} \mvert \sigma^2_{t+1}, \sigma^2_t \right)   
%
    = -\theta \Var\left(r_{t+1} \mvert \sigma^2_{t+1}\right).   
\end{equation}

\noindent Plugging in \cref{eqn:rtn_cond_vol}, we obtain:

\begin{equation}
    \label{eqn:return_covarinace}
    \Cov\left(m_{t,t+1}, r_{t+1} \mvert \sigma^2_{t+1}, \sigma^2_t \right)   
%
    = -\theta (1 - \phi^2) \sigma^2_{t+1}.
\end{equation}

The expression in \cref{eqn:return_covarinace} is the change in the mean driven by investors' risk aversion, and
is controlled by their equity risk price.
To see this, note it equals zero if $\theta = 0$.

The second term we need is the Jensen's effect term.
The mean of a conditionally log-Gaussian price depends on both the mean and variance of the underlying return.
It has the standard form: minus one-half the conditional variance $\left(-\frac{1 - \phi^2}{2}
\sigma^2_{t+1}\right)$. 

Having done this we can define a pseudo-return that only changes by an amount proportional to the leverage
effect, and is not affected by the changes in the return arising from the reduction in risk.
From \cref{eqn:rtn_cond_mean}, we know that $\E\left[r_{t+1} \mvert \sigma^2_t, \sigma^2_{t+1} \right] = \psi
\sigma^2_{t+1}$ plus some function  of $\sigma^2_t$.
However, this $\psi$ contains all three effects.
We want a pseudo-return $\widetilde{r}_{t+1}$ where $\E\left[\widetilde{r}_{t+1} \mvert \sigma^2_t, \sigma^2_{t+1}
\right] = \widetilde{\psi} \sigma^2_{t+1}$ for some $\widetilde{\psi}$ that only contains the direct effect, not
the Jensen or any risk-compensation.
We can then solve for $\widetilde{\psi}$ in terms of the paramters governing the dynamics of $r_{t+1}$.

\begin{restatable}[Separating the Leverage Effect from the Measure Changes]{lemma}{leverageVersusMeasureChange}
    \label{lemma:separating_leverage_effect}

    Let $\tilde{r}_{t+1} \coloneqq r_{t+1} - \frac{1 - \phi^2}{2} \sigma^2_{t+1} + (1 - \phi^2) \theta
    \sigma^2_{t+1}$, and let the $m_{t+1}$ have the form given by \cref{eqn:log_sdf}.
    
    Then 
    
    \begin{equation}
        \Var\left[\tilde{r}_{t+1} \mvert \sigma^2_t \right] = \E\left[\sigma^2_{t+1} \mvert \sigma^2_t\right].
    \end{equation}
\end{restatable}

This is not the only way we can compute $\Var\left[\tilde{r}_{t+1} \mvert \sigma^2_t \right]$.
We can also use the law of total variance.

\begin{equation}
    \Var\left[\widetilde{r}_{t+1} \mvert \sigma^2_t\right]  =
    \E\left[\Var\left[\widetilde{r}_{t+1} \mvert \sigma^2_{t+1}\right] \mvert \sigma_t^2 \right] +
    \Var\left[\E\left[\widetilde{r}_{t+1}\mvert \sigma_{t+1}^2\right] \mvert \sigma^2_t\right]
\end{equation}

Since $\sigma^2_{t+1}$ is constant given $\sigma^2_{t+1}, \sigma^2_t$, the conditional variance of
$\widetilde{r}_{t+1}$ is the same as $r_{t+1}$.
In addition, $\E\left[\tilde{r}_{t+1} \mvert \sigma^2_{t+1}\right]  = \psi \sigma^2_{t+1} - \frac{1-\phi^2}{2}
\sigma^2_{t+1} + (1- \phi^2) \theta \sigma^2_{t+1}$ plus $\sigma^2_t$-measurable terms.
We now fill in the values for both of the inside variables on the right-hand side using the formulas for
$r_{t+1}$, and we use \cref{lemma:separating_leverage_effect} on the left-hand side. 
We then take unconditional expectations of both sides, giving  us

\begin{equation}
    \E\left[\sigma^2_{t+1} \right]  = \E[ \E\left[(1 - \phi^2) \sigma^2_{t+1} \right]] +
    \E\left[\Var\left[\left(\psi - \frac{1-\phi^2}{2} + (1-\phi^2)\theta\right) \sigma^2_{t+1} \mvert
    \sigma^2_t\right]\right].
\end{equation}

Rearranging terms implies 

\begin{equation}
    \label{eqn:psi_tilde_eqn}
    \left(\psi - \frac{1-\phi^2}{2} + (1-\phi^2)\theta\right) = \phi \sqrt{\frac{\E\left[\sigma^2_{t+1}
    \right]}{\E \left[\Var\left(\sigma^2_{t+1} \mvert \sigma^2_t\right)\right]}}.
\end{equation}

We can now solve for $\psi$, and replace the moments on the right hand side by deriving the expectations from
\cref{remark:vol_moment_conditions} and some elementary calculations. 

\begin{equation}
    \label{eqn:psi_pp_as_func_of_params}
    \psi = \frac{\phi}{\sqrt{c (1 + \rho)}} + \frac{1 - \phi^2}{2}  - (1 - \phi^2) \theta
\end{equation}

Now that we have a formula for $\psi$, we can derive the formulas for $\beta$ and $\gamma$ in terms of the
other reduced-form parameters and the risk prices. 

\begin{restatable}[Reparamaterizing the Physical Distribution]{lemma}{physicalMeasureFunctions}

    \label{lemma:psi_function}
    
    Let the SDF be given as in \cref{defn:SDF}, and the model be paramterized as in
    \cref{defn:physical_vol_dynamics} and \cref{defn:physical_return_dynamics}. 
    Then the following equation holds

    \begin{equation}
        \psi(\pi, \theta) = \frac{\phi}{\sqrt{c (1 + \rho)}} + \frac{1 - \phi^2}{2} - (1 - \phi^2) \theta.  
    \end{equation}
\end{restatable}

\begin{proof}
    This is a straightforward implication of \cref{lemma:separating_leverage_effect} and the derivation in the
    text.
\end{proof}

\subsubsection{Deriving the other Reduced-Form Parameters}

The final goal is to identify the risk-prices $\theta$ and $\pi$.
In the previous sections, we have derived a series of moment conditions in terms of the parameters.
We now need to analyze when these moment conditions allow to identify the risk prices. 
The information that return data contain about equity pricing data is entirely encapsulated by the asset pricing
equation for excess returns.  
In what follows, we will use $rx_{t+1}$ as the excess log-return.
The definition of $M_{t,t+1}$ as a change of measure means that the following holds.

\begin{equation}
    \E\left[ M_{t,t+1}(\theta, \pi) \exp(r_{t+1}) \mvert \F_{t} \right] = 1
\end{equation}

We now characterize, the information in this set of moment conditions regarding the risk prices.
The difficult part is identifying the volatility risk price --- $\pi$, and so we will focus first on the
information regarding that parameter.
Substituting in the SDF formula from above and replacing all of the dependence on $\F_t$ with $\sigma^2_t$, yields

%Where exactly should I mention that I focusing on pi?

\begin{gather}
    \E \left[ \exps*{ - \pi \sigma^2_{t+1} - (\theta - 1) rx_{t+1} } \mvert \sigma^2_t \right]
        = \exps*{- m_0(\theta, \pi) - m_1(\theta, \pi) \sigma^2_t}.
%
    \intertext{Similar to above, we use the law of iterated expectations to substitute in the conditional Laplace
        transforms of $r_{t+1}$ and $\sigma^2_{t+1}$, obtaining}
%
    \E\left[\exps*{- A\left(\pi + C(\theta -1)\right) \sigma^2_t - B(\pi) - D(\theta-1) \sigma^2_t - E(\theta-1)}
    \mvert \sigma^2_t \right] = \exps*{- m_0(\theta, \pi) - m_1(\theta, \pi) \sigma^2_t}. 
\end{gather}

We now derive forms for $\beta$ and $\gamma$ as functions of the reduced-form parameters and risk-prices.

\begin{lemma}[Reparamterizing $\beta$ and $\gamma$]
    \label{lemma:reparamterizing_beta_and_gamma}
    Let the SDF be given as in \cref{defn:SDF}, and the model be paramterized as in
    \cref{defn:physical_vol_dynamics} and \cref{defn:physical_return_dynamics} and assume that the risk-neutral
    measures have distributions with the same parametric forms.  


    \begin{align}
       \label{eqn:beta_function}
       \gamma  &= B(\pi + C(\theta - 1) -  B(\pi + C(\theta))  \\
    %
        \label{eqn:gamma_function}
        \beta &=   A(\pi + C(\theta -1)) - A(\pi + C(\theta)) 
    \end{align}

\end{lemma}

\begin{proof}
    This comes from matching the coefficients of the $\F_t$-measurable variables and using the linearity of $D(x)$
    and $C(x)$ from \cref{lemma:linearity_of_physical_functions}. 
    We substitute in the formulas for $m_0(\theta, \pi)$ and $m_1(\theta, \pi)$ that we derived in
    \cref{lemma:characterizing_sdf_integration_constants} which gives us to equations that we solve for $\gamma$
    and $\beta$.

\end{proof}
 
\subsection{Identification of the Risk Prices}

We can characterize the identification restrictions in \cref{lemma:reparamterizing_beta_and_gamma} in two
different cases.\footnote{These equations are Equation 3.7 in
\textcite{khrapov2016affine}.}

\begin{enumerate}
    \item[Case 1:] The price of equity risk $\theta$ satisfies the following equations. 
        \begin{equation}
            C(\theta - 1) = C(\theta) 
            \label{eqn:lack_of_id_condition}
        \end{equation}

        If \cref{eqn:lack_of_id_condition} holds, then some simple algebra shows that $E(\theta) = E(\theta-1)$
        and $D(\theta) = D(\theta-1)$.
        In this situation, any value of $\pi$ satisfies \cref{lemma:reparamterizing_beta_and_gamma}. 
        Since these are the only places $\pi$ shows up in the model, the asset pricing equation does not identify
        $\pi$. 
        As noted by \textcite{khrapov2016affine}, this is in line with the common belief that the econometrician
        needs options data to be able to identify the price of volatility risk. 

    \item[Case 2:] 
        In general, there is no reason to expect the \cref{eqn:lack_of_id_condition} to hold.
        If it does not, it might seem reasonable to expect that we should be able to identify both $\theta$ and
        $\pi$.
        In other words, we should, in principle, at least be able to identify $\pi$ from the difference between the
        functions in the previous case when evaluated at $\theta-1$ and $\theta$.
\end{enumerate}

We now show that if $\phi = 0$, then \cref{eqn:lack_of_id_condition} holds, but it does not hold if $\phi \neq 0$.
In other words, as mentioned in \textcite[13]{khrapov2016affine}, a leverage effect will allow us to separately
identify $\theta$ and $\pi$.
To see this, we apply the definition of $C(x)$ from \cref{defn:physical_return_dynamics} and the formula for
$\psi(\pi, \theta)$ from \cref{lemma:psi_function}.
Some simple algebra gives 

\begin{align}
    C(\theta) - C(\theta - 1) &= C'(0)  + C''(0) \left(\theta - \frac{1}{2}\right)
%
    =  \psi + (1 - \phi^2) \left(\theta - \frac{1}{2}\right),  \\
%
    &= \frac{\phi}{\sqrt{c (1 + \rho )}} + \frac{1 - \phi^2}{2} - (1 - \phi^2) \theta +  (1 - \phi^2)
       \left(\theta - \frac{1}{2}\right) 
%
    \label{eqn:alpha_difference}
    = \frac{\phi}{\sqrt{c (1 + \rho )}}. 
\end{align}

This shows the leverage effect creates a wedge between $C(\theta)$ and $C(\theta-1)$ proportional to
$\frac{\phi}{\sqrt{c (1 + \rho )}}$.
Clearly, this equals zero if and only if $\phi = 0$.
Consequently, if $\phi \neq 0$, we can separately identify $\pi$ and $\theta$.

\section{GMM}\label{sec:GMM}

To estimate the model we use the various moment conditions that we have derived.
The one that we have yet to derive is the ones implied by the continuous-time model that allow us to estimate
$\phi$ and $\psi$.
Deriving them is straightforward because we constructed what the discrete-time equations, and they are linear.

\subsection{Moment Conditions for Reduced-Form Parameters}\label{sec:moment_conditions}

First, we define two functions that we use to reparameterize the moment conditions.
We do this because the model creates some cross-equation equations to eliminate two of the redundant parameters.
By doing this we are able to avoid the econometric complications that we would have to handle in the general
case.
Note, we are only specifying values for the return conditional on both $\sigma^2_{t+1}$ and $\sigma^2_t$ because
the other moments can be derived from them and the volatility moments.
Also, it is worth noting that we consider \cref{eqn:beta_function}, \cref{eqn:gamma_function}, and
\cref{lemma:psi_function} as implicitly defining those parameters as functions of the other parameters.
\cref{eqn:cond_expected_rtn_moment} can be derived from those equation and the first derivative of the conditional
log-cumulant function equaling the conditional mean.


\begin{defn}{Equilibrium Moment Conditions}
    \label{defn:equilibrium_moment_conditions}
    \begin{align}
        \label{eqn:cond_vol_mean}
        \E\left[\sigma^2_{t+1} \mvert \sigma^2_t \right]  &= \rho \sigma^2_t  + c \delta\\
%
        \label{eqn:cond_vol_var}
        \Var\left[\sigma^2_{t+1} \mvert \sigma^2_t \right]  &=  2 c \rho \sigma^2_t  + c^2 \delta \\
%
        \label{eqn:cond_expected_rtn_moment}
        \E\left[r_{t+1} \mvert \sigma^2_t, \sigma^2_{t+1}\right] &= \gamma(\pi, \theta) + \beta(\pi, \theta)
        \sigma^2_t + \psi(\pi, \theta) \sigma^2_{t+1} \\
%
        \label{eqn:cond_rtn_var}
        \Var\left[r_{t+1} \mvert \sigma^2_t, \sigma^2_{t+1}\right] &= (1 - \phi^2) \sigma^2_{t+1} 
\end{align}
\end{defn}

%TODO Bring in the discussion of the link function.

\subsection{Identified Set}\label{sec:identified_set}

Having constructed the moments and the instruments, we can use GMM to estimate the parameters.
We do need to run a constricted optimization though, because only certain values of the parameters are valid. 
Having done that we characterize the set where the GMM criterion strongly identifies the parameters.

\begin{restatable}[Identified Set]{lemma}{identifiedSet}

    Assume that the moment conditions specified in \cref{defn:equilibrium_moment_conditions} have the correct form
    and that the instruments we are using satisfy the standard exogeneity and relevant conditions. 

    Let the true parameter vector $\omega \coloneqq (\rho, c, \delta, \phi, \theta, \pi) \in [-1+\epsilon_1,
    1 - \epsilon_2] \times [M_1, M_2] \times [\epsilon_4, M_4]\times [M_5, M_6]\times \times [-1 +
    \epsilon_4, 1 - \epsilon_5] \times [M_7, M_8] \times [M_12, M_13]$, where the $M_{\ast}$ are some large (in
    magnitude) known constants and the $\epsilon_{\ast}$ are some small positive constants.  

    Let $Q_T(\omega, X)$  be the GMM objective function with moment conditions given in
    \cref{defn:equilibrium_moment_conditions}.  

    If there exists a $\epsilon$ such that $\abs{\phi} > \epsilon > 0$, then all of the parameters are identified. 
    If $\phi = 0$, then the objective function is independent of $\pi$. 
    Hence, $\pi$ is not identified but all of the other parameters are still identified.

\end{restatable}



\section{Weak Identification Setup}

In this section, take the model described in the previous sections and place it in the setup of
\textcite{andrews2014Gmm} so that we ca analyze the effects of possible lack of identification in the model in a
nice clean way.
The goal here is to perform valid inference for $\pi, \theta$ even when $\phi$ might be zero. 


From the discussion above, we can collect the parameters discussed above into a parameter vector of the following
form,i.e.\@ recall the following: $\omega = \lbrace \rho, c, \delta, \phi, \pi, \theta \rbrace$
To write it in the notation of \textcite{andrews2014Gmm}, we partition $\omega$ into three subsets.

\begin{align}
    \phi &\coloneqq \phi  \in (-1, 1) \\ 
    \zeta &\coloneqq \lbrace \rho, c, \delta, \theta \rbrace \in [0,1) \times \R_{++} \times \R_{++} \times
    \R  \\
    \pi &\coloneqq \pi \in \R
\end{align}

Let $\omega$ be the set of possible $\omega$, that as defined above.
It is worth noting that the parameter space has a product form, i.e.\@ the values do not affect the valid values
of the other parameters.

In this environment, $\pi$ is not identified when $\phi = 0$.
Both $\phi$ and $\zeta$ are always identified, and $\zeta$ does not affect the identification of $\pi$.

Let $Q_T(\omega)$ be the GMM criterion function, then the GMM estimator $\hat{\omega}_T$ satisfies the following.


\begin{equation}
    \widehat{\omega}_T \in \omega\ \text{and}\ Q_T(\hat{\omega}_T) = \inf_{\omega \in \Omega} Q_T(\omega) +
    o\left(T^{-1}\right) 
\end{equation}


Now that we have defined the parameters, we can characterize the set of assumptions necessary for valid inference.
We will work through the assumptions described in \textcite{andrews2014Gmm}.
The set of necessary assumptions is relatively complicated because we have to characterize the asymptotic
distribution under several different estimation strengths simultaneously, and the assumptions required to do that
  differ in the various cases. 
In what follows, we will use 

The first assumption specifies the basic identification
problem. It also provides conditions that are used to determine the
probability limit of the GMM estimator, when it exists, under all categories
of drifting sequences of distributions.
Let $\xi$ index the part of the distribution of the data $r_{t+1}, \sigma^2_{t+1}$ that is not determined by the
moment equations.
In general, it is a (likely infinite-dimensional) nuisance parameter that affects the distribution of the data. 


We collect the parameters that we are estimating $\omega$ and the nuisance parameter $\xi$ into one parameter,
$\gamma$ and associated parameter space $\Gamma$.
In the previous discussion we characterized the parameter spaces in a non-compact fashion, let $\omega^{*}$ be a
compact subset of $\omega$, where the true parameter values live.

\begin{defn}{Complete Parameter Space}
    \begin{equation}
        \Gamma \coloneqq \left\lbrace \gamma = (\omega, \xi) \mvert \omega \in \Omega, \xi \in \Xi \right\rbrace 
    \end{equation}
\end{defn}

We characterize these drifting sequences of distributions by sequences of true parameters $\gamma_T \coloneqq
(\omega_T, \phi_T)$.

\purple{TODO Add discussion of the limiting process.}
\purple{Verify that the assumptions on the parameter space hold.}
\purple{Discuss what happens if we lack identification and hence cannot consistently estimate the parameter.}


\begin{restatable}[Inference for $\omega$ under Weak Identification]{theorem}{InferenceWeakID}
    Let that $\phi_0  \in \left(\underline{\phi}_0,1\right)$, for some $\underline{\phi}_0 > -1$. 
    $\rho_0 \in \left[0,1\right)$, and $c_0 > 0$. 

    \purple{TODO  Add Conclusion}
\end{restatable}


\section{Simulations}

\section{Data}\label{sec:data}

\section{Empirical Results}

\section{Conclusion}

\newpage

\phantomsection
\addcontentsline{toc}{section}{References}
\printbibliography
\newpage

\begin{appendices}


\section{Model Characterization}\label{app:model_characterization}

\begin{lemma}[Linearity of $\beta$ and $\gamma$]
    \label{lemma:linearity_of_physical_functions}
    Letting $\sigma^2_{t+1}$ be the integrated volatility of a process with return $r_{t+1}$.
    Assume that $\sigma^2_{t+1}$ and $r_{t+1}$ follow a bivariate CAR(1) process parametrized as in
    \cref{defn:physical_vol_dynamics} and \cref{defn:physical_return_dynamics}. 
    Then $\beta''(0)$ and $\gamma''(0)$ both equal zero.
\end{lemma}

\begin{proof}
    By the \Ito\ Isometry, and the definition of $r_{t+1}$ as an integrated variance, the following holds for the
    returns' predictable information set $\F^r_{t-}$.  

    \begin{equation}
        \Var\left(r_{\tau+1}\mvert \F_{\tau-}\right) \leq \E\left(r^2_{\tau+1} \mvert \F_{\tau-}\right) 
        = \E\left(\sigma^2_{t+1}\mvert \F_{\tau-}\right)
    \end{equation}

    The integrated volatlity is predictable, and so $\sigma^2_{t+1}$ is contained in the return's predictable
    $\sigma$-algebra. 

    Consequently, $\Var\left(r_{t+1} \mvert \sigma^2_t, \sigma^2_{t+1}\right) \leq \sigma^2_{t+1}$
    In addition, variance must always be positive, and so $\Var\left(r_{t+1} \mvert \sigma^2_t,
    \sigma^2_{t+1}\right) \geq 0$.

    Since the second derivative of the log-cumulant funciton evaluted at zero equals the variance, we have the
    following set of inequalities.

    \begin{gather}
        0 \leq (1 - \phi^2) \sigma^2_{t+1} - \beta''(0) \sigma^2_t - \gamma''(0) \leq
        \sigma^2_{t+1} 
%
        \intertext{Dividing through by $\sigma^2_{t+1}$ and pulling the first term outside}
%
        \label{eqn:second_derivative_inequalities}
        \implies \phi^2 - 1 \leq -\frac{1}{\sigma^2_{t+1}} \left(\beta''(0)  \sigma^2_t +
        \gamma''(0)\right) \leq \phi^2 
%
    \end{gather}

    On the outside of the two inequalities we have constants, and the distribution of $\sigma^2_{t+1}$ given
    $\sigma^2_t$ is not bounded away from zero.
    Consequently, the only way for \cref{eqn:second_derivative_inequalities} to hold for all $\sigma^2_{t+1}$ is
    if the term inside the parantheses equals  zero.

    \begin{equation}
        0 = \beta''(0) \sigma^2_t + \gamma''(0)
    \end{equation}

    However, the only way for this to hold is for both $\gamma''(0)$ and $\beta''(0)$ to equal zero.
    This plus the condtional gaussianity of the returns implied by the paramtric model implies that $\gamma$
    and $\beta$  are both linear.

\end{proof}


\sdfConstants*

\begin{proof}

\begin{equation}
    \label{eqn:reweigted_sdf}
    \E\left[\exp\left(m_0(\theta, \pi) + m_1(\theta, \pi) \sigma^2_t - \pi \sigma^2_{t+1} - \theta r_{t+1}
    \right) \mvert \F_t \right] = 1 
\end{equation}

We can use \cref{eqn:reweigted_sdf} to relate $m_0(\theta, \pi)$ and $m_1(\theta, \pi)$ to the physical measure
functions. 

\begin{gather}
    \E\left[\exp\left(m_0(\theta, \pi) + m_1(\theta, \pi) \sigma^2_t - \pi \sigma^2_{t+1} - \theta r_{t+1}
    \right) \mvert \F_t \right] = 1 \\
%
    \intertext{By the law of iterated expectations.}
%
    \E\left[ \E\left[\exp\left(m_0(\theta, \pi) + m_1(\theta, \pi) \sigma^2_t - \pi \sigma^2_{t+1}\right)
        \exp\left( - \theta r_{t+1}\right) \mvert \F_t, \sigma^2_{t+1} \right]\right] = 1 \\
%
    \intertext{The second term is the Laplace transform of $r_{t+1}$.}
%
    \E\left[\exp\left(m_0(\theta, \pi) + m_1(\theta, \pi) \sigma^2_t - \pi \sigma^2_{t+1} \right)
        \exp(-C(\theta) \sigma^2_{t+1} - D(\theta) \sigma^2_{t} - E(\theta_2) \mvert \F_t \right] = 1 \\
%
    \intertext{Reorganizing terms.}
%
    \E\left[\exp\left(m_0(\theta, \pi) + m_1(\theta, \pi) \sigma^2_t - D(\theta) \sigma^2_{t} - E(\theta_2)
    \right) \exp(-\left(\pi + C(\theta)\right) \sigma^2_{t+1}) \mvert \F_t \right] = 1 \\ 
%
    \intertext{Substituting in the Laplace transform for $\sigma^2_{t+1}$.} 
%
    \label{eqn:expected_sdf_wrt_PP}
    \E\left[\exp(m_0(\theta, \pi) + m_1(\theta, \pi) \sigma^2_t - D(\theta) \sigma^2_{t} - E(\theta_2)  - A(\pi +
    C(\theta)) - B(\pi + C(\theta)) \mvert \F_t \right] = 1 
\end{gather}

\end{proof}

\leverageVersusMeasureChange*


\begin{proof}

    We start by considering the expectation of $\widetilde{r}_{t+1}$ and show that it equals zero.

    \begin{align}
        \E\left[\widetilde{r}_{t+1} \mvert \sigma^2_{t} \right]  &= \E\left[\E\left[r_{t+1} - \frac{1 - \phi^2}{2}
        \sigma^2_{t+1} + (1 - \phi^2) \theta \sigma^2_{t+1}\mvert \sigma^2_{t}, \sigma^2_{t+1}  \right] \mvert
        \sigma^2_t\right] \\
%
        \intertext{By the conditional Gaussianity of $r_{t+1}$, we can absorb the convexity correction into an
        exponential. Normally, it would introduce a $1/2$ variance term, but that cancels. The second term is a
        measurable with respect to the conditioning information and so is not affected.}
%
        &= \E\left[\log \E\left[\exp(r_{t+1}) \exp((1 - \phi^2) \theta \sigma^2_{t+1})\mvert \sigma^2_{t},
           \sigma^2_{t+1}  \right] \mvert \sigma^2_t\right] \\
%
        \intertext{We note that normally, the covariance term would cause the mean to fall, but again that term
        cancels with the second term. We can divide through by $\exp(r_t)$  and $m_{t-1,t}$ because they are
        measurable with respect to $\F_t$, and the remainder of the SDF is also contained within the information
        set, and so we can add it as well (\cref{eqn:log_sdf}).} 
%
        &\propto \E\left[\log \E\left[\frac{\exp(r_{t+1})}{\exp(r_t)} \frac{M_{t,t+1}}{M_{t-1,t}} \mvert
          \sigma^2_t, \sigma^2_{t+1} \right] \mvert \sigma^2_t \right]
%
        \intertext{We can pull the log outside of the outer expectation while adding at most a
        $\sigma^2_t$-measurable term.}
%
        &\propto \log \E\left[\frac{\exp(r_{t+1})}{\exp(r_t)} \frac{M_{t,t+1}}{M_{t-1,t}} \mvert \sigma^2_t
          \right]
%
        \intertext{This is the log expectation of a price change in the price discounted by the change in the
        SDF.}
%
        &= 0
    \end{align}

Because the mean of $\tilde{r}_{t+1}$ given $\sigma^2_t$  does not change in expectation when we condition on
$\sigma^2_{t+1}$ we can apply the \Ito\ Isometry.

\begin{equation}
    \label{eqn:return_var_versus_expected_vol_pp}
    \E\left[\Var\left(\tilde{r}_{t+1} \mvert \sigma^2_t\right)  \right] = \E[ \sigma^2_{t+1}]
\end{equation}

Intuitively, volatilities are squared returns, and so they are variances.
The tricky part here is that variances are centered second moments, not the second moments themselves.
The volatilities are also centered second moments, but the centering is not the same in general.
In continuous time, they would only differ by a drift term, which can be ignored, which is why the Ito\ Isometry
usually is used in that context.
Using discrete-time return, as we do here, we first have to appropriately recenter the variables, which is why it
applies to $\widetilde{r}_{t+1}$ but not to $r_{t+1}$.

\end{proof}


\section{Identification Proofs}


\identifiedSet*

\begin{proof}
    Since $Q_{T}$ is a quadratic in terms of deviations between sample and population moment conditions as long as
    the population moment can be inverted to solve for the parameters, the $Q_T$ process identifies them as well. 
    In addition, since we have a sufficient number of exogenous valid instruments the conditioning implied by
    projecting on the instruments does not affect the arguments above. 

    Identifying the four parameters that govern the volatility dynamics is not particularly complicated. 
    We have four parameters and four non-redundant moment conditions.
    The first two equations in    \cref{defn:equilibrium_moment_conditions} identify $\rho$ and $c \delta$.
    \cref{eqn:cond_vol_var} identifies $\rho c$ and $c^2 \delta$. 
    This allows us to separately identify $c$ and $\delta$.
    
    Identifying $\phi$ is also relatively straightforward. Since $r_{t+1}$ and $\sigma^2_{t+1}$ are known, as long
    as we know the conditional mean of $r_{t+1}$, then identifying $\phi$ is identified by \cref{eqn:cond_rtn_var}.
    Identifying the conditional mean of $r_{t+1}$ is straightforward because we observe volatility and the
    conditional mean is a linear equation in these variables. 
    

    Identifying the risk prices $\pi$ and $\theta$ is more complicated.
    We have to identify both parameters off of \cref{eqn:cond_expected_rtn_moment}. 
    This is in principle possible because we now have two non-redundant sources of variation in the data ---
    $\sigma^2_t$ and $\sigma^2_{t+1}$.

    The only place that \cref{defn:equilibrium_moment_conditions} that the risk-prices occurs in
    \cref{eqn:cond_expected_rtn_moment}. 
    We showed that $\gamma, \beta$ and $\psi$ functions are independent of $\pi$ if $\phi = 0$ in the discussion
    leading up \cref{eqn:alpha_difference}.
    We further showed that they are not independent if $\phi \neq 0$.
    In addition the dependence of the identification in terms of the non-singularity of the derivatives
    equilibrium conditions in terms of $\pi$ and $\theta$ depends smoothly on $\phi$. 
    This will be important later.
    
\end{proof}

\section{Inference Assumptions}

    In what follows, three sets of drifting sequences $\lbrace \gamma_T \rbrace$ are key. 
    
    \begin{defn}{Drifting Sequence Parameter Spaces}
        \begin{align}
            \Gamma\left(\gamma_0\right) &\coloneqq \left\lbrace \left\lbrace \gamma_T \in \Gamma \right\rbrace
            \mvert \gamma_T \to \gamma_0 \in \Gamma \right\rbrace\\ 
            \Gamma(\gamma_0, 0, b) &\coloneqq \left\lbrace \lbrace \gamma_T \rbrace \in \Gamma(\gamma_0) \mvert
            \phi_0 = 0\ \text{and}\ \sqrt{T} \phi_T \to b \in (\R \cup \lbrace \pm \infty) \right\rbrace \\
            \Gamma(\gamma_0, \infty, b_0) &\coloneqq \left\lbrace \lbrace \gamma_T \rbrace \in \Gamma (\gamma_0)
            \mvert \sqrt{T} \norm{\phi_T} \to \infty\ \text{and}\ \frac{\phi_T}{\norm{\phi_T}} \to b_0
            \right\rbrace 
        \end{align}
    \end{defn}
    
    These are the standard GMM regularity conditions appropriately adjusted for the lack of identification when
    $\phi =0$.
    
    \begin{assump}[GMM 1]\label{ass:GMM1}
    \begin{assumplist}
        \item If $\phi_0=0$, $\sampmom(\omega)$ and $\W_{T}(\omega)$ do not depend on $\pi$ for all $\omega \in \Omega$,
            for all $T \geq 1$, and for all $\gamma^{*}\in \Gamma.$ 
            \label{ass:GMM1a}
        \item If $\lbrace \gamma_{T} \rbrace \in \Gamma\left(\gamma_0\right)$, $\sup_{\omega \in \Omega}
            \norm*{\sampmom(\omega) - \E\left[g\left(\omega \mvert \gamma_0\right)\right]} \pto 0$ and $\sup_{\omega
            \in \omega} \norm{\W_{T}(\omega)-\E\left[\W\left(\omega \mvert \gamma_0\right)\right]} \pto 0$.
            \label{ass:GMM1b}
        \item When $\phi_0 = 0$,  $g_0\left(\phi, \zeta ,\pi \mvert \gamma_0\right) = 0$ if and only if $\phi
            =\phi_0$ and $\zeta = \zeta_0$ for all $\pi \in \Pi$ and for all $\gamma_0 \in \Gamma.$
            \label{ass:GMM1c}
        \item When $\phi_0 \neq 0$, $g_0\left(\omega \mvert \gamma_0\right)=0$ if and only if $\omega =\omega_0$ for all
            $\gamma_0 \in \Gamma.$
            \label{ass:GMM1d}
        \item  $g_0\left(\omega \mvert \gamma_0\right)$ is continuously differentiable in $\omega $ on $\omega$ with
            partial derivatives with respect to $\omega$ and $\xi$ denoted by $g_{\omega}\left(\theta \mvert
            \gamma_0\right) \in R^{k\times d_{\omega }}$ and $g_{\xi }\left(\omega \mvert \gamma_0\right)\in R^{k\times
            d_{\xi }}$, respectively.
            \label{ass:GMM1e}
        \item $\W\left(\omega \mvert \gamma_0\right)$ is continuous in $\omega$ on $\omega$ for all $\gamma_0\in
            \Gamma$.  \label{ass:GMM1f}
        \item $0 < \lambda_{\min}(\W\left(\xi_0, \pi \mvert \gamma_0\right))\leq \lambda_{\max }(\W\left(\xi_0,\pi
            \mvert \gamma_0\right)) < \infty$, $\forall \pi \in \Pi$, for all $\gamma_0 \in \Gamma$.
            \label{ass:GMM1g}
        \item $\lambda_{\min} (g_{\xi}\left(\xi_0,\pi \mvert \gamma_0\right)^{\prime} \W\left(\xi_0,\pi \mvert
            \gamma_0\right)g_{\xi }\left(\xi_0,\pi \mvert \gamma_0\right))>0$, for all $\pi \in \Pi$,  and for all 
            $\gamma_0 \in \Gamma$ with $\phi_0=0.$
            \label{ass:GMM1h}
        \item$\Xi(\pi)$ is compact for all $\pi \in \Pi$, and both $\Pi$ and $\omega$ are compact.
            \label{ass:GMM1i}
        \item For all $\epsilon > 0$, there exits a $\delta > 0$ such that $d_{H}\left(\Xi \left(\pi_{1}\right),
            \Xi \left( \pi_{2}\right) \right) < \epsilon$ for $\pi_{1}, \pi_{2} \in \Pi$ with
            $\norm*{\pi_{1}-\pi_{2}} < \delta$, where $d_{H}\left( \cdot \right)$ is the Hausdorff metric.
            \label{ass:GMM1j}
    \end{assumplist}
    \end{assump}
    
    
    
    \begin{assump}[GMM 2*]\label{ass:GMM2}
    \begin{assumplist}
        \item $\sampmom(\omega)$ is continuously differentiable in $\omega$ for all $T \geq 1$. 
            \label{ass:GMM2a}
        \item If $\{\gamma_T\} \in \Gamma\left(\gamma_0, 0, b\right)$, $\sup_{\left\lbrace \omega \in \Omega \mvert
            \norm*{(\phi, \zeta')' - (\phi_T, \zeta_0')} \leq \delta_T \right\rbrace}
            \norm*{\frac{\partial}{\partial (\phi, \zeta')'} \sampmom(\omega) - \E\left[\popmom_{(\phi,
            \zeta')'}(\omega) \mvert \gamma_0\right]} = o_p(1)$ for all deterministic sequences  $\delta_T \to 0$.
            \label{ass:GMM2b}
        \item  Let $\omega_T \coloneqq \left\lbrace \omega \in \Omega \mvert \norm*{(\phi, \zeta_) - (\phi_T, \zeta_T)}
            \leq \delta_T \norm*{\beta_T}\, \text{and}\, \norm*{\pi - \pi_T} \leq \delta_T \right\rbrace$.  Let
            $\delta_T$ be a deterministic sequence that converges to zero.  If $\{\gamma_T \} \in
            \Gamma\left(\gamma_0, \infty, b_0\right)$, then we have the following asymptotic behavior.
            $\sup_{\omega \in \Omega_T} \norm*{\left(\frac{\partial}{\partial \omega'} \overline{g}_T -
            \E\left[g_{\omega}(\omega) \mvert \gamma_0\right]\right) \diag\left(1_{1+d_\zeta}',
            (1/\phi_T)_{d_{\pi}}'\right)}  = o_p(1)$. 
            \label{ass:GMM2c}
    \end{assumplist}
    \end{assump}
    
    Once we have \nameref{ass:GMM1} and \nameref{ass:GMM2}, we use \nameref{ass:GMM3} to derive the asymptotic
    distribution under weak and semi-strong identification.
    These conditions will be characterized using the expected derivative of the population moment conditions. 
    
    \begin{defn}
        \label{defn:moment_derivative_func}
        \begin{equation}
            K_{T,g}\left(\omega \mvert \gamma^{*}\right) \coloneqq  \frac{1}{T} \sum_{i=1}^T \frac{\partial}{\partial
            \phi^{*}} \E \left[ \popmom(W_T, \omega) \mvert \gamma^{*} \right]
        \end{equation}
    \end{defn}
    
    
    \begin{assump}[GMM 3]\label{ass:GMM3}
    \begin{assumplist}
        \item $\sampmom(\omega) = \frac{1}{T} \sum_{i=1}^T \popmom(W_T, \omega)$  for some function $\popmom(W_T,
            \omega) : \R^{k \times k} \times \omega \to \R^k$.
            \label{ass:GMM3a}
        \item $\E\left[\popmom(W_T, \beta_0, \zeta^{*}, \pi) \mvert \gamma^{*} \right] = 0$ for all $\pi \in \Pi$ and
            for all $i \geq 1$ if $\gamma^{*} = \left(0,\zeta^{*}, \pi^{*}, \xi^{*} \right) \in \Gamma$.
            \label{ass:GMM3b}
        \item If $\{ \gamma_T \} \in \Gamma(\gamma_0, 0, b)$, $\frac{1}{\sqrt{T}} \sum_{i=1}^T \left(g(W_T,
            \zeta_{0,T}, \pi_T) - \E \left[g(W_T, \zeta_{0,T}, \pi_T)\mvert \gamma_T \right]\right)  \dto \N\left(0,
            \aleph(\gamma_0)\right)$, where $\aleph(\gamma_0)$ is a $k \times k$ matrix.
            \label{ass:GMM3c}
        \item 
            \label{ass:GMM3d}
            \begin{enumerate}
                \item  $K_{T,g}\left(\omega \mvert \gamma^{*}\right)$ exists for all $\{\omega, \gamma^{*} \} \in
                    \left(\omega_{\delta} \times \Gamma_{0}\right)$ and for all $T \geq 1$.
                \item $K_{T,g}\left(\phi_T, \zeta_T, \pi \mvert \widetilde{\gamma}_T\right)$ uniformly converges
                    to some non-stochastic matrix-valued function  $K_{g}\left(0, \zeta_0, \pi \mvert
                    \gamma_0\right)$ over $\pi \in \Pi$ for all deterministic sequences $\{\phi_T, \zeta_T,
                    \widetilde{\gamma}_T \}$ satisfying $\widetilde{\gamma}_T \in \Gamma$, $\widetilde{\gamma}_T
                    \to \gamma_0 \coloneqq (0, \zeta_0, \pi_0, \xi)$, $\{\phi_T, \zeta_T, \pi \} \in \omega$ and
                    $\{\phi_T, \zeta_T \} \to (0, \zeta_0)$.
                \item $K_g\left(\phi_0, \zeta_0, \pi \mvert \gamma_0\right)$ is continuous on $\Pi$ for all
                    $\gamma_0 \in \Gamma$ with $\phi_0 = 0$.
            \end{enumerate}
            \item $K\left(\phi_0, \zeta_0, \pi \mvert \gamma_0\right) = \popmom_{\phi, \zeta}\left(\phi_0,
                \pi\mvert \gamma_0\right) x$ for some $x \in \R^{1+d_{\zeta}}$ if and only $\pi =
                \pi_0$.\footnote{Since $\dim(\phi) = 1$, we can assume without loss of generality that the
                $\omega_0$ from \textcite{andrews2014Gmm} equals $1$.}
                \label{ass:GMM3e}
            \item If $\{ \gamma_T \} \in \Gamma(\gamma_0, 0, b)$, $\frac{1}{T} \sum_{i=1}^T
                \frac{\partial}{\partial \omega}  \E\left[ \popmom\left(W_T, \omega_T \right ) \mvert \gamma_T \right]
                \to \popmom_{\omega}\left(\omega_0 \mvert \gamma_0\right)$.
            \label{ass:GMM3f}
    \end{assumplist}
    \end{assump}

    \begin{defn}{\popmom*}
        \begin{equation}
            g_{\phi, \zeta}^{*}\left(\phi_0, \zeta_0, \pi_1, \pi_2 \mvert \gamma_0\right)  =
            \left[g_{\phi}\left(\phi_0, \zeta_0, \pi_1 \mvert \gamma_0\right)  , g_{\phi}\left(\phi_0, \zeta_0,
            \pi_2 \mvert \gamma_0\right) , g_{\zeta} \left(\phi_0, \zeta_0 \mvert \gamma_0\right)  \right]  \in
            \R^{k \times (d_{\zeta} + 2)}
        \end{equation}
    \end{defn}


    \begin{assump}[GMM 4]\label{ass:GMM4}
    \begin{assumplist}
        \item $\phi$ is a scalar.
            \label{ass:GMM4a}
        \item $g_{\phi, \zeta}^{*}\left(\phi_0, \zeta_0, \pi_1, \pi_2 \mvert \gamma_0\right)$ has full column
            rank. 
            \label{ass:GMM4b}
        \item $\aleph(\gamma_0)$ is positive definite for all $\gamma_0 \in \gamma $ with $\phi_0 = 0$. 
            \label{ass:GMM4c}
    \end{assumplist}
    \end{assump}

\section{Inference Proofs}


\begin{restatable}[Uniform Convergence under Strong Identification]{lemma}{UllnStrongID}
    \label{lemma:UniformConvergenceStrongID}
    Let $\omega$ be the identified set.
    Further assume that $\phi_0 \neq 0$. 
    Let $\sampmom$ be the sample moment condition defined above, and $\W_T$ be the associated optimal weight matrix
    estimator.
    Then we have the following convergence.

    \begin{align}
        &\sup_{\omega \in \Omega} \norm*{\sampmom(\omega) - \E\left[g\left(\omega \mvert
          \gamma_0\right)\right]}_{Fro} \pto 0 \\ 
        &\sup_{\omega \in \Omega} \norm{\W_{T}(\omega)-\E\left[\W\left(\omega \mvert \gamma_0\right)\right]} \pto 
    \end{align}

\end{restatable}

\begin{proof}

    In this proof we rely heavily on the continuity of the moment conditions over their domain. 
    This can be seen from simple inspection since we assumed that $\phi_0 \geq \underline{\phi} -1$.
    Furthermore since $\omega$ is compact, this continuity implies uniform continuity.
    
    For any positive definite weight-matrix by \textcite[Lemma 2.3]{newey1994large} our criterion function has
    a unique optimum.
    The data, $\sigma^2_{t+1}, r_{t+1}$, are ergodic and stationary.
    Since the moment conditions are not redundant the optimal (GMM) weight matrix $\W$ is positive definite. 
    In addition, $\popmom$ is continuous at each $\omega$, given the restrictions above and properties of
    characteristic functions imply that $\popmom$ is uniformly bounded. 
    For convenience, we assume that the space of $\omega$ is compact.
    This should not be an issue here because the parameters  are either a priori bounded, such as $\phi$ or we
    have substantial a priori knowledge on their plausible magnitudes.
    Hence, \textcite[Theroem 2.6]{newey1994large} implies our estimator is consistent.
    
    However, when we allow for weak identification late on, we need this convergence to be uniform. 
    One straightforward way to show this is to show that our criterion function is globally Lipschitz in a set of
    high probability. 
    
    The other issue is that we need the weight matrix to converge uniformly to its expectation.
    Since the moments are continuous functions over their domain as is the square function.
    This convergence is uniform if and only if the matrix inverse is continuous.
    
    Since we have a finite number of non-redundant moments, the minimum eigenvalue, \\
    $\lambda_{min}\left(\W\left(\phi_0, \zeta_0, \pi \mvert \gamma_0\right)\right) > 0$, and so the matrix inverse
    is uniformly continuous in $\gamma_0$ with respect to the Frobenius norm, which is the sum of the eigenvalues.
    (Recall, that the eigenvalues of the inverse are the inverse of the eigenvalues.)


\end{proof}


\begin{assump}[Weak Dependence]
    \label{assumption:weak_dependence}
    $z_t \coloneqq \begin{pmatrix} r_{t+1} \\ \sigma^2_{t+1} \end{pmatrix}$ are $\alpha$-mixing with $\alpha_t =
       O\left(T^{-5}\right)$
\end{assump}


\begin{theorem}[Inference for $\omega$ under Strong Identification]
    Assume that $\phi_0  \in (-1,1) \setminus 0$, $\rho_0 \in [0,1)$, and $c_0 > 0$. 
    Further assume that the data are ergodic, stationary, and satisfy \cref{assumption:weak_dependence}.
    Then the following convergence in distribution holds.

    \begin{equation}
    \sqrt{T} (\widehat{\omega}_T - \omega_0) \dto \N\left(0, \left(G' \E[\W] G\right)^{-1}\right)
    \end{equation}
\end{theorem}

\begin{proof}

    By the above arguments, we have a consistent estimator for $\omega$ and the optimal weight matrix $\W \coloneqq
    (\E\left[g g'\right])^{-1}$, and we will assume that the true value $\omega_{0}$ is in the interior of its
    sample space $\omega$.\footnote{Throughout we will use subscript \num{0}  to denote true values for parameters.}
    Let $G \coloneqq \E\left[\frac{\partial}{\partial \omega} \popmom \right]$ Clearly, $g$ is continuously
    differentiable, and its derivative $G$ is continuous.
    In addition, by the identification discussion $G' W \nabla G$ is nonsingular.
    
    %TODO Replace this with strong (alpha) mixing of order (1+\epsilon). 
    %Don't you need a lot more than this...?

    Since, $\norm*{g_t}$ is almost surely bounded by $1$ it has all of its moments and $z_t$ being $\alpha$-mixing
    implies $g_t$ is as well by the central limit theorem for strongly mixing process $\sqrt{T} \sampmom(\omega^{*})
    \dto \N\left(0, \E\left[\W\right]^{-1}\right)$ as required. 
    Consequently, by \textcite[theorem 3.2]{newey1994large} we have convergence in distribution as well as
    convergence in probability.
    

\end{proof}


\InferenceWeakID*

\begin{proof}
We prove this result by showing that Assumptions GMM 1-4 are satisfied.

\begin{proofpart}
    \label{part:main_theorem_proof_part1}
    In this part, we show that \nameref{ass:GMM1} is satisfied. 
    To do this, we break \nameref{ass:GMM1}  down into three subsections.
    Assumptions \namedref{ass:GMM1a}, \namedref{ass:GMM1b}, \namedref{ass:GMM1c}, and \namedref{ass:GMM1d} state
    that when $\phi = 0$, the moment conditions contain no information regarding $\pi$, but when $\phi \neq 0$,
    the model is identified.
    
    \purple{We need to show the above statement}

    We further showed the relevant uniform convergence to verity \namedref{ass:GMM1b} in
    \cref{lemma:UniformConvergenceStrongID}.
    
    The next two assumptions (\namedref{ass:GMM1e} and \namedref{ass:GMM1f}) are  technical conditions regarding
    the behavior of the moment conditions and weight matrix. 
    Since our moment conditions are derived from an infinitely-differentiable  characteristic function and the
    weight matrix is the optimal one, they both hold trivially.
    
    The third subsection of Assumption \nameref{ass:GMM1} concerns the weight matrix.
    Since we are using the inverse covariance matrix of valid non-redundant model, assumptions
    \namedref{ass:GMM1g} and \namedref{ass:GMM1h} automatically hold.
    
    The last two assumptions, \namedref{ass:GMM1i} and \namedref{ass:GMM1j} require that the parameter spaces do
    not vary too much with the parameters and are compact.
    Since $\omega$ is compact, \namedref{ass:GMM1i} holds trivially, and since it has  has a product form,
    \namedref{ass:GMM1j}  holds trivially as well.
    
\end{proofpart}


\begin{proofpart}
    \label{part:mainTheoremProofPart2}

    In this section, we show that the derivatives of the moment conditions have the correct behavior locally to
    the true parameters.
    We have to do this for the different classes of drifting sequences.
    We will do this by verifitying \cref{ass:GMM2}.
    This is valid since \textcite{andrews2014Gmm} show that this is a sufficient condition for their Assumption
    GMM2, which is what we actually need. 

    Our moment conditions are sample averages of the characteristic function, they satisfy \namedref{ass:GMM2a}
    automatically. 
    Since characteristic functions are uniformly bounded, by the dominated convergence theorem we can interchange the 
    expectation and derivative operators. 
    Hence \namedref{ass:GMM2b} and \namedref{ass:GMM2c} are equivalent to the statements in terms of the moment
    conditions themselves mutatis mutandis.  
    In addition, sice the derivate is a linear operator, we can pull it outside of the norm.
    The reason that the uniform law of large numbers in \cref{part:main_theorem_proof_part1} does not trivially
    imply this result is because we are not considering sequences $\phi_T \to \phi_0$. 


    We create a mean value expansions around around $(\phi_0, \zeta_T, \pi_T)$ of the sample moment condition and
    around $(\phi_0, \zeta_0, \pi_0)$ for the population moment condition.
    (This is not the same in both cases, not is it the true parameter for the drifting sequence in the case of the
    sample moment condition.)
    In addition, also since we are considering continuous functions of compact spaces --- the $\delta_T$ ball in
    $\R^{\dim(\omega)}$ --- pointwise convergence implies uniform convergence, and so we only need to show pointwise
    convergence below.

    \begin{alignat}{2}
        & &&\norm*{\sampmom(\phi_T, \zeta_T, \pi_T) -  \E\left[\popmom(\phi_T, \zeta_T, \pi_T) \mvert
          \gamma_0\right] } \\ 
        \intertext{We take a mean value expansion of both functions around $\omega_0$. The point at which the
        derivative in the two locations is taken may not be the same.}
        &= &&\left\lVert \sampmom(\phi_0, \zeta_0, \pi_0) + \frac{\partial}{\partial (\phi, \zeta,
           \pi)}\sampmom(\widetilde{\phi}^s, \widetilde{\zeta}^s, \widetilde{\pi}^s)\left((\phi_0, \zeta_0, \pi_0)
            - (\phi, \zeta, \pi)\right)\right. \\
        &  &&\quad \left. - \E\left[\popmom(\phi_0, \zeta_0, \pi_0) \mvert \gamma_0\right] + 
           \frac{\partial}{\partial (\phi, \zeta, \pi)} \E\left[\popmom(\widetilde{\phi}^p, \widetilde{\zeta}^p,
           \widetilde{\pi}^p)\mvert \gamma_0\right] \left((\phi_0, \zeta_0, \pi_0) - (\phi, \zeta,
           \pi)\right) \right\rVert \\ 
        \intertext{By the triangle inequality.}
        &\leq && \norm*{\sampmom(\phi_0, \zeta_0, \pi_0) - \E\left[\popmom(\phi_0, \zeta_0, \pi_0) \mvert
           \gamma_0\right]}  \\
        &+  && \norm*{\frac{\partial}{\partial (\phi, \zeta)}\sampmom(\widetilde{\phi}^s, \widetilde{\zeta}^s,
          \widetilde{\pi}^s)\left((\phi_0, \zeta_0)  - (\phi, \zeta)\right) -  \frac{\partial}{\partial (\phi,
          \zeta)} \E\left[\popmom(\widetilde{\phi}^s, \widetilde{\zeta}^p, \widetilde{\pi}^p)\mvert \gamma_0\right]
          \left((\phi_0, \zeta_0) - (\phi, \zeta)\right)} \\
        &+  && \norm*{\frac{\partial}{\partial \pi}\sampmom(\widetilde{\phi}^s, \widetilde{\zeta}^s,
          \widetilde{\pi}^s)\left(\pi_0 - \pi\right) -  \frac{\partial}{\partial \pi}
          \E\left[\popmom(\widetilde{\phi}^p, \widetilde{\zeta}^p, \widetilde{\pi}^p)\mvert \gamma_0\right]
          \left(\pi_0 - \pi\right)} 
          \label{eqn:pi_derivative_term}
    \end{alignat}

    By the uniform law of law numbers in \cref{part:main_theorem_proof_part1}, the first equation is $o_p(1)$.
    For $(\phi_0, \zeta_0) - (\phi, \zeta)$ small, the middle term is bounded by the quantity below. 


    \begin{equation}
        \norm*{\frac{\partial}{\partial (\phi, \zeta)}\sampmom(\widetilde{\phi}^s, \widetilde{\zeta}^s,
        \widetilde{\pi}^s) -  \frac{\partial}{\partial (\phi, \zeta)} \E\left[\popmom(\widetilde{\phi}^s,
        \widetilde{\zeta}^p, \widetilde{\pi}^p)\mvert \gamma_0\right]} \norm*{(\phi_0, \zeta_0) - (\phi, \zeta)}
    \end{equation}

    The first term is almost surely bounded, and the second term is less than $\delta_T$ by assumption, and so the
    product is $o_p(1)$.

    The hard part is the third expression.
    Like before we can bound the pull the $\pi_0 - \pi$ term out of the equation.
    However, this term is no longer converges to zero.
    We will consider the two cases, separately.
    Throughout, we will refer to the behavior of the following equation, which bounds
    \cref{eqn:pi_derivative_term}.


    \begin{equation}
        \norm*{\frac{\partial}{\partial \pi}\sampmom(\widetilde{\phi}^s, \widetilde{\zeta}^s, \widetilde{\pi}^s) -
        \frac{\partial}{\partial \pi} \E\left[\popmom(\widetilde{\phi}^p, \widetilde{\zeta}^p,
        \widetilde{\pi}^p)\mvert \gamma_0\right] } \norm*{\pi_0 - \pi} 
        \label{eqn:pi_derivative_norm_bound}
    \end{equation}

    In general, $\tilde{\pi}^s$ and $\tilde{\pi}^p$ can be arbitrarily far apart.
    However, since $\sampmom \pto \E\left[\popmom \mvert \gamma_0\right]$, and the limiting value is independent of
    $\pi$, the derivative does not depend upon $\pi$ asymptotically by the dominated convergence theorem.
    This applies that the difference between the two derivatives evaluated at different $\pi$ converges to zero.
    Consequently, \cref{eqn:pi_derivative_norm_bound} is $o_p(1)$ and we have shown \namedref{ass:GMM2b}.

    \begin{equation}
        \norm*{\abs*{\frac{\partial}{\partial \pi}\sampmom(\widetilde{\phi}^s, \widetilde{\zeta}^s,
        \widetilde{\pi}^s) - \frac{\partial}{\partial \pi} \E\left[\popmom(\widetilde{\phi}^p,
        \widetilde{\zeta}^p, \widetilde{\pi}^p)\mvert \gamma_0\right]} (1, 1, T)} \norm*{\pi_0 - \pi} 
        \label{eqn:pi_derivative_rescaled_bound}
    \end{equation}

    If we consider the setup in \namedref{ass:GMM2c}, $\tilde{\pi}^s$ and $\tilde{\pi}^p$ are now close together.
    However, we need to show that \cref{eqn:pi_derivative_rescaled_bound} is $o_p(1)$.
    Since we are considering the limiting behavior of a function with a continuous derivative, we can assume that
    the derivative is uniformly bounded without loss of generality. 
    (The constant might depend upon the true value, but not $T$.)
    By a Taylor series expansion of $\sampmom$ around the true value, $\frac{\widetilde{\pi}^p}{\sqrt{T}} \to 0$,
    and $\norm{\pi_0 - \pi} \propto \delta_T$ the result follows.

\end{proofpart}

\begin{proofpart}
    \label{part:mainTheoremProofPart3}

    Assumption \namedref{ass:GMM3a} is trivially satisfied,  and we showed that \namedref{ass:GMM3b} is satisfied
    in \cref{sec:GMM}.  
        
    The conceptual idea driving the reulsts this section is that moment conditions for each $T$ minus their
    conditional exceptions converge to a normal random variable.
    In other words, we are almost in a standard triangular C.L.T.\@ setup with weak time-dependence.

    In particular, both $\sigma^2_t$ and $r_t$ are infinitely differentiable functions of the innovations to
    the volatility and return processes and $\popmom$ are infinitely differentiable functions of $r_{t+1}$ and
    $r_t$ and the innovations are i.i.d.\@ across time by assumption.
    (Note, i.i.d.\@ implies strong mixing of any size.)
    Consequently, $\popmom$ is near epoch dependent (NED) of any size as defined in
    \textcite{andrews1991empirical}.  
    (Take $s>2.$) 
    By \textcite[Theorem 3]{andrews1991empirical}, we have the necessary finite-dimensional convergence in
    distribution to a Gaussian random variable. 

    We now show that \namedref{ass:GMM3d} holds.
    Clearly, $K_{t,g}\left(\omega \mvert \gamma^{*}\right)$ always exists.
    It uniformly converges because the derivatives of the moments are continuous functions of the data and the
    parameters, the process is ergodic, and the characteristic function lives on a compact set.
    It is also clearly continuous.
    By the dominated convergence theorem, we can exchange the derivative and expectations.
    In addition, since \popmom\ does not depend upon $\phi_T$, the limiting behavior is independent of the value
    of $\phi_T$.
    Hence \namedref{ass:GMM1d} holds.
    
    \namedref{ass:GMM3e} says the derivative of the moment function does not depend the true parameter $\phi$ if
    and only if  $\pi =  \pi_0$. 
    We showed this in the proof of \cref{part:mainTheoremProofPart2}.
    \namedref{ass:GMM3f} follows directly from the compactness of the parameter space and the continuity of the
    cross derivatives of \popmom\ by the dominated convergence theorem.

\end{proofpart}

\begin{proofpart}
    \label{mainTheoremProofPart4}
   
    \namedref{ass:GMM4a} holds trivially.

    \purple{This is no longer true.}
    We verified \namedref{ass:GMM4b} when we showed that $\pi$ and $\zeta$ are strongly identified

    \namedref{ass:GMM4c} holds because the identification conditions do not create any singularity in the
    asymptotic covariance matrix. 

\end{proofpart}

\end{proof}





\end{appendices}


\end{document}



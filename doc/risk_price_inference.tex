\documentclass[11pt, letterpaper, twoside, final]{article}
\usepackage{risk_price_inference}
\addbibresource{riskpriceinference.bib}

\author{Xu Cheng\thanks{University of Pennsylvania, The Perelman Center for Political Science and Economics, 133 South 36th Street, Philadelphia, PA 19104, \href{mailto:xucheng@upenn.edu}{xucheng@upenn.edu}} \and Eric Renault\thanks{Brown University, Department of Economics -- Box B, 64 Waterman Street, Providence, RI 02912, \href{mailto:eric_renault@brown.edu}{eric\_renault@brown.edu}} \and Paul Sangrey\thanks{University of Pennsylvania, The Perelman Center for Political Science and Economics, 133 South 36th Street, Philadelphia, PA 19104, \href{mailto:paul@sangrey.io}{paul@sangrey.io}}}

\title{Identification Robust Inference for Risk Prices in Structural Stochastic Volatility Models}

\date{\today}

\begin{document}

\begin{titlepage}


\maketitle
\thispagestyle{empty}
\addtocounter{page}{-1}

\begin{abstract} 

\singlespacing \noindent 
In structural stochastic volatility asset pricing models, changes in volatility affect risk premia through two channels: (1) the investor's willingness to bear high volatility in order to get high expected returns as measured by the market risk price, and (2) the investorâ€™s direct aversion to changes in future volatility as measured by the volatility risk price. Disentangling these channels is difficult and poses a subtle identification problem that invalidates standard inference. We adopt the discrete-time exponentially affine model of \textcite{khrapov2016affine}, which links the identification of volatility risk price to the leverage effect. In particular, we develop a minimum distance criterion that links the market risk price, the volatility risk price, and the leverage effect to the well-behaved reduced-form parameters governing the return and volatility's joint distribution. The link functions are almost flat if the leverage effect is close to zero, making estimating the volatility risk price difficult. We adapt the conditional quasi-likelihood ratio test \textcite{andrews2016conditional} develop in a nonlinear GMM framework to a minimum distance framework. The resulting conditional quasi-likelihood ratio test is uniformly valid. We invert this test to derive robust confidence sets that provide correct coverage for the prices regardless of the leverage effect's magnitude. 

\end{abstract} 

\vspace{\baselineskip}

\jelcodes{C12, C14, C38, C58, G12}

\vspace{\baselineskip}

\keywords{weak identification, robust inference, stochastic volatility, leverage, equity risk premium, volatility risk premium, risk price, confidence set, asymptotic size}

\end{titlepage}

\tableofcontents
\newpage

\section{Introduction}

Modern finance is all about the risk return trade offs that investors face and how to optimally respond to them.  In particular, the central question of asset pricing is what drives expected returns. Standard economic theory predicts you must compensate investors with higher expected returns when they face more risk. In other words, we would expect a positive relationship between the mean and volatility of returns. In \gentextcites{sharpe1964capital,lintner1965security} capital asset pricing model (CAPM) the expected return varies proportionally with the volatility of the market return. However, to price option prices well, we need not only a price of equity risk that loads upon the market variance, but also a price of volatility risk as well \parencite{christoffersen2013capturing}. Investors care not just about how their returns co-move with the market return but also how they co-move with the market volatility.

This implies that current volatility will affect expected returns in two ways.  1) Through investors' preferences over future market returns. 2) Through investors' preferences over future volatility. As one might expect distinguishing these two effects is quite difficult. The difference between how the two effect risk premia comes through their different nonlinear relationships in the presence of contemporaneous correlation between the volatility and return processes. We adopt the framework of \textcite{khrapov2016affine}, which is a discrete-time exponentially-affine stochastic volatility model. They mention a potential identification strategy in that paper.  We develop this strategy, characterizing how the identification strength of the risk prices varies over the parameter space, and show how to perform uniformly valid inference.

To take a step back and more fully develop empirical structure at hand, we consider how to measure the
relationship between market volatility and expected returns if we assume that this relationship is constant and
linear. Even in this simple case, unlike the consensus in the theoretical literature, the empirical literature
has found pinning down this relationship quite difficult. Not only has its magnitude proven difficult to
determine, but various estimates even differ in sign, \parencite{lettau2010measuring}.

The empirical literature, which we examine in more detail in the literature review, has focused on point estimates of this magnitude. However, if individual investors are ambiguity averse as in \textcite{hansen2001robust, jiu2012ambiguity}, they will care not just about how the representative investor prices volatility but also their uncertainty regarding this estimate. Furthermore, when economists calibrate models, they must know how precisely the data determine these parameters. If the need to alter the parameter value slightly in order to make their model perform well, are they bringing in more restrictions or data to more precisely determine the parameter of interest or are they using a value that the data tell us is incorrect?

Clearly, as obtaining precise believable point estimates of the price of volatility risk has proven quite difficult, we should expect doing valid inference to be even more delicate. To the best of our knowledge, this is the first paper to directly tackle this question. Various authors report confidence intervals as well as their point estimates. However, they do not take into account the weak identification that makes getting the point estimate difficult in computing these estimates.   Why is it that measuring this price is so difficult when the theoretical literature is so cohesive? Econometrically, it is because the volatility price is weakly identified, as in \textcite{andrews2012estimation}, in that the strength of the identification of the price depends upon the value of other parameters. This obviously begs the question --- what are these parameters?  

To estimate the risk prices, there are three different phenomena that must be distinguished. First, the econometrician must disentangle the volatility feedback effect (leverage) which is a contemporaneous relationship between the volatility and returns from the risk premium, which is a relationship between volatility and expected returns. It is not a contemporaneous relationship, but rather a predictable one.  

Second, and just as important. We must separate the equity risk price and the volatility risk price.  In general, and we will show this below, the equity risk price is strongly identified even in the presence of the volatility risk price.  There is a simple linear relationship that we can use between the equity risk price and the expected return. However, volatility risk being another priced factor will introduce nonlinear nuisance terms into this regression.  They will not matter asymptotically, but in finite samples, they likely do.

Returning to identifying the price of volatility risk, we know that when the strength of identification varies over the parameter space and we lack identification entirely for some parameter values, the finite sample distributions are highly nonstandard.  Consequently, the usual asymptotic   approximations do not perform well. 

In addition, the finite-sample distributions are the relevant ones here.  Even though we often have thousands of observations, since the variation in the expected return is so much smaller than variation in the return itself, we have a very low signal-to-noise ratio. Consequently, our estimators will continue to behave as if they were taken from a \textquote{small} sample even in \textquote{large} samples.

The obvious next step is considering how we need to do this in practice. Since the contribution of this paper is in terms of methodology and empirical results, we will take a model from the literature that has the various components, instead of developing our own pricing model. In particular, we take the model from \textcite{khrapov2016affine} and use it to estimate the relevant parameters. 

This model has a few nice features.  First, it has both equity and volatility prices and a leverage effect.  As such, it is the natural discrete-time analogue of the \textcite{heston1993closedform} option pricing model.  It has an exponentially affine stochastic discount factor and shares with \textcite{heston1993closedform} the advantage of having a structure preserving change of measure between the physical and risk-neutral models. By doing our analysis in discrete-time we are able to more directly compare our results to risk-premia estimates outside of the option pricing literature and the jumps in high-frequency innovations will not dramatically affect our results.  If we were to use a diffusion process in continuous time, we would be severely counterfactually constraining the higher-order moments of the process in way that would likely bias our inference. 

As far as estimation is concerned, we derive a series of conditional means and variances. We then take these means and variances and plug them into a general method of moments (GMM) criterion. The data we use are the bivariate series $(r_{t+1}, \sigma^2_{t+1})$. $r_{t+1}$ is the daily return on some asset, and we use its associated realized volatility for $\sigma^2_{t+1}$. We go into further detail in \cref{sec:data} regarding how we obtained it, the time-span covered, and so forth.

\section{The Model}\label{sec:model}

\addtocounter{subsection}{1}

We estimate the prices of some factors using moment conditions derived from a pricing model.  As is standard in that literature, we will do this by specifying the physical and risk-neutral measures and their relationship, i.e.\@ the stochastic discount factor or pricing kernel. This is non-trivial because we only observe equity data and so can only use moments with respect to the physical measure to estimate the parameters.  However, as will be seen in detail below the risk prices govern the stochastic discount factor (SDF), not the physical measure directly.  Consequently, we need to relate the physical and risk-neutral measures through SDF closely in order to get restrictions on the behavior of the physical measure in terms of the risk prices. 

Let $\F_t$ be the representative investor's information set at time $t$, and $P_t$ be the price on the asset in question, with associated return $r_{t+1}$ and volatility $\sigma^2_{t+1}$. Then standard asset pricing theory says there exists a stochastic discount factor $M_{t,t+1}$ that prices the assets.  % The advantage of defining $\QQ$ is that for some function --- $f$ -- of the future excess return --- $r_{t+1}$ -- % and volatility $\sigma^2_{t+1}$ and potentially the current information available -- $\F_t$, we can price this % payoff as its expectation with respect to $\QQ$. % In other words, the price of $f(r_{t+1}, \sigma^2_{t+1}, \F_t)$ satisfies \cref{eqn:risk_neutral_measure_defn} % for all $t$ and for all $f$.  % This is useful because we can choose $f$ to make our estimation convenient.

% \begin{equation} %   P_t(f) = \E_{\QQ}\left[ f\left(r_{t+1}, \sigma^2_{t+1}, \F_{t}\right) \mvert \F_{t}\right] %   \label{eqn:risk_neutral_measure_defn} % \end{equation}

% Since $P_t(f), r_{t+1}$ and $\sigma_t^2$ are observable, if we specify a model for $\F_t$ in terms of observable % (to the econometrician) variables, this provides a moment condition that we can use.  % However, this condition does not identify everything we wish to estimate, in particular it does not identify the % risk prices because $\QQ$ is not observable from equity data.

% To resolve this we complete the model by defining the stochastic discount factor --- $M_{t, t+1}$ --- as the % Radon-Nikodym derivative between the $\PP$ and $\QQ$ measures.  % No arbitrage guarantees that this will exist, \parencite{harrison1978martingales}. % Since risk prices arise from investors' demand for compensation to hold risk, the risk price show up here  % in the $\QQ$ measure.  % (We collect the parameter of interest into a vector --- $\omega$.)


\begin{defn}{Asset Pricing Equation}\label{eqn:asset_pricing_eqn}
  \begin{equation}
    P_t = \E\left[M_{t,t+1} P_{t+1} \mvert \F_t \right] 
  \end{equation}
\end{defn}

The main issue with \cref{eqn:asset_pricing_eqn} is that $M_{t,t+1}$ is not observable, and so we cannot use it to estimate models. However, if we can parameterize $M_{t,t+1}$ as a vector of observables and parameters, then we can.  We specify the model by parameterizing the dynamics of the $P_t$ and $M_{t, t+1}(\omega)$ The object of interest here is the risk prices, which are such parameters. Risk prices measure the level of compensation that the representative investor demands in order to face additional risk. Consequently, they determine the amount that the stochastic discount factor (SDF) twists the distribution of $P_t$. 

In what follows, we write down a model for $P_t$ that implies $M_{t,t+1}$ is a known function of these risk prices, parameters that govern the price and volatility dynamics, the volatility --- $\sigma^2_{t}, \sigma^2_{t+1}$, and the log excess returns --- $r_{t+1}$.. Having done that, we place this $M_{t,t+1}$ into \cref{eqn:asset_pricing_eqn} and estimate the model by matching the reduced-form and model-implied dynamics of the returns and polities.

Throughout we assume that the two risks that are priced in the market are equity risk and volatility risk as we discussed in the introduction. This motivates us to focus on the first two moments of the data. If higher moments, such as skewness and kurtosis are also priced factors, as in \textcites{harvey2000conditional, conrad2012exante, chang2013market}, and we used higher sample moments as well to determine the price of our risk-factors our resulting estimates would be biased, likely substantially so.  Essentially, we misattribute compensation for bearing risk driven by the higher moments to risk driven by lower moments. This implies that we want a model that is paramterized in terms of these first two moments. We use the conditional autoregressive CAR(1) model here, which we take from \textcite{darolles2006structural,khrapov2016affine}

Following \textcite{khrapov2016affine}, we assume that the $P_t$ and $\sigma^2_t$ are first-order Markov and there is no Granger-causality from return to the volatility and that returns are serially independent given the volatility path. In other words, the volatility drives all of the dynamics of the process. Note, we do allow $\sigma^2_{t+1}$ and $r_{t+1}$ to be contemporaneously correlated, which they are in the data.  We construct the model in terms of a series of Laplace transforms that we parameterize using some functions $A(x), B(x), C(x), D(x)$, and $E(x)$ for all $x$ in its domain. We collect the parameters in a vector $\omega$.

\subsection{The Stochastic Discount Factor}\label{sec:deriving_sdf_functions}

We start by parameterizing the stochastic discount factor.  We assume an exponentially-affine structure for the SDF. We let $\pi$ be the price of volatility risk and $\theta$ be the price of equity risk, and use the following parameterization for the SDF.

\begin{defn}{The Stochastic Discount Factor}
  \label{defn:SDF}
  \begin{equation}
    M_{t,t+1}(\pi, \theta) = \exp\left(m_{0}(\pi, \theta) + m_1(\pi, \theta) \sigma_t^2 - \pi \sigma^2_{t+1} -
    \theta r_{t+1}\right) 
  \end{equation}
\end{defn}

Since $M_{t,t+1}$ must integrate to $1$ for all $\sigma^2_t$, we can view $m_{0}(\pi, \theta)$ and $m_1(\pi, \theta)$ as integration constants.

\subsection{Parameterizing the Physical Measure Dynamics}

We now introduce the data generating process for the volatility. We use a conditional autoregressive gamma process as in \textcite{gourieroux2006autoregressive, khrapov2016affine} for the volatility. This implies that we can parameterize the process using the $A$ and $B$ defined as follows.

\begin{defn}{Volatility Dynamics Functions}
  \label{defn:physical_vol_dynamics}
  \begin{align}
    \label{defn:a_PP}
    A(x) &= \frac{\rho x}{1 + c x}, \\
    \label{defn:b_PP}
    B(x) &= \delta \log(1 + c x),\\
% 
    \text{with}&\ \rho \in [0,1), \quad c > 0, \quad \delta > 0. \nonumber
  \end{align}

\end{defn}

In this specification, $\rho$ is a persistence parameter, and $c$ is a scaling parameter.
We can see this this clearly from the following forms of the moment conditions.

\begin{remark}[Volatilty Moment Conditions] 
  \label{remark:vol_moment_conditions}
  \begin{align}
    \E\left[\sigma^2_{t+1} \mvert \sigma^2_t \right] &= \rho \sigma^2_t + c \delta\\
%
    \Var\left[\sigma^2_{t+1} \mvert \sigma^2_t \right] &= 2 c \rho \sigma^2_t + c^2
    \delta 
%
  \end{align}
\end{remark}

Since these two moment conditions are sufficient to derive the unconditional moments, all of the parameters are identified as long as they are in the interior of their appropriately specified domains. Intuitively, we are using linear regression to estimate the slope and intercept parameters.

\subsubsection{Return Dynamics}

We then turn to computing the moments of the return distribution.  This is more subtle than computing the moments of the volatility dynamics because we have to relate the dynamics of the returns to that of the volatility and to the stochastic discount factor, which is not observed.  We use the conditional autoregressive CAR(1) model here, which we take from \textcite{darolles2006structural,khrapov2016affine} This model specifies the conditional Laplace transform of the return as a function of $r_{t+1}$ given $\sigma^2_{t+1}, \sigma^2_t$. We start by specifying a parametric form for $C(x)$ and $D(x)$, and $E(x)$. We paramterize $C(x)$ using a quadratic form, which, along with our other rescritions, implies that the return is conditionall Gaussian.

\begin{defn}{Reduced-Form Functions}
  \label{defn:physical_return_dynamics}
  \begin{align}
    C(x) &\coloneqq \psi x + \frac{1 - \phi^2}{2} x^2, \\
    D(x) &\coloneqq \beta x, \\
    C(x) &\coloneqq \gamma x. 
  \end{align}
\end{defn}

In addition, as these functions parameterize the Laplace transform, they equal zero at at $x=0$. It might seem as first, that imposing linearity for $D(x)$ and $E(x)$ is a strong restriction. This is not actually the case. Since we assumed that $\sigma^2_{t+1}$ is an integrated volatility and hence greater than $\Var(r_{t+1} \ivert \sigma^2_{t+1}, \sigma^2_{t})$ and variances are positive, the model implies they are linear.\footnote{We show this in \cref{lemma:linearity_of_physical_functions} which is located in \cref{app:model_characterization}.}


To give an idea of how the dynamics of $\sigma^2_t$ evolve in this specfication, we compute the first two moments.

\begin{align}
  \label{eqn:rtn_cond_mean}
  \E\left[r_{t+1} \mvert \sigma^2_t, \sigma^2_{t+1}\right] &= \psi \sigma^2_{t+1} + \beta \sigma^2_t +
  \gamma \\
  \label{eqn:rtn_cond_vol}
  \Var\left[r_{t+1} \mvert \sigma^2_t, \sigma^2_{t+1}\right] &= (1 - \phi^2) \sigma^2_{t+1} 
\end{align}



\subsection{Relating the Physical Measure Functions and the Risk Prices}

By plugging in the Laplace transforms for $\sigma^2_{t+1}$ and $r_{t+1}$ into \cref{defn:SDF}, we can derive the following.

\begin{restatable}[Characterizing the SDF Integration Constants]{lemma}{sdfConstants}
  \label{lemma:characterizing_sdf_integration_constants}
  Let the SDF be given as in \cref{defn:SDF}, and the model be paramterized as in
  \cref{defn:physical_vol_dynamics} and \cref{defn:physical_return_dynamics}.

  Then the SDF constants follow the following equations.\footnote{This is Equation $3.4$ in
  \textcite[3.4]{khrapov2016affine}.}

  \begin{align}
    \label{eqn:sdf_functions_vs_physical_functions}
    m_0(\theta, \pi) &= E(\theta) + B(C(\theta) + \pi) \\
    m_1(\theta, \pi) &= D(\theta) + A(C(\theta) + \pi) \nonumber
  \end{align}

\end{restatable}


% Since the right-hand side of \cref{eqn:sdf_functions_vs_physical_functions} are entirely in terms of the physical
% measure functions, which are observable, we can estimate the SDF functions entirely using equity data.
% In other words, we do not need, at least in principle, option data to estimate the SDF.
% In particular, we can determine the risk prices from equity prices if the identified parts of the physical measure
% functions ($\alpha$, $\beta$, etc.) are \textquote{sufficiently} invertible.
% (I.e.\@ the matrix of their derivatives satisfies the appropriate non-singularity conditions.)

\subsubsection{Deriving the Conditional Mean}

The goal moving forward is to solve for $\gamma$, $\beta$, and $\psi$ in terms of $(rho, c, \delta)$ and $(\pi, \theta)$. Once we do this, we can use the difference in expected returns between the distribution given $\sigma^2_t$ and the distribution given $\sigma^2_{t+1}$ and $\sigma^2_t$ to separately identify the two risk prices. In this section, we focus on $\psi$ because it controls this difference in means and hence controls the strength of identification.

Intuitively, the reason they are not identified in general is that if volatility and returns are uncorrelated, we cannot disentangle the shift in the mean because the price of volatility risk from the shift in the mean induced by price of equity risk. They both show up in the same way in $\psi$, which we will show later. However, if we have a leverage effect, when we condition on $\sigma^2_{t+1}$ two different components of the return are moving -- the mean and variance. Since we now have two moments, we can identify both parameters.

In general, $\psi$ will have three parts. First, we have the Jensen's inequality term --- the mean will shift by a value proportional to the variance. Second, the reduction in variance will shift the price as consumers are risk averse. Third, since $\sigma^2_{t+1}$ and $r_{t+1}$ are correlated, (the leverage effect) the drift will change directly. 

The goal is to separate out the three components. The way we do this is by constructing a pseudo-return --- $\widetilde{r}_{t+1}$ --- whose mean is not affected by the change in variance caused by adding $\sigma^2_{t+1}$ to the information set.

We do this by exploiting the fact that $\E\left[M_{t,t+1}(\theta, \pi) \exp(r_{t+1}) \mvert \sigma^2_t \right] = 1$. We want to relate $\E\left[\exp(r_{t+1}) \mvert \sigma^2_{t+1}, \sigma^2_t \right]$ to this expression. Clearly, there are two differences between these expressions. First, we have the SDF in the first expression. Second, the conditioning information differs. We can view these differences as two measure changes. Since prices here are conditionally log-Gaussian, the measure change can be paramterized in terms of the covariance between the log SDF --- $m_{t,t+1} \coloneqq \log M_{t,t+1}$ --- and the return.

If we apply the logarithm to \cref{defn:SDF}, we get the following expression for $m_{t,t+1}$:
%
 \begin{equation}
  \label{eqn:log_sdf}
    m_{t,t+1}(\pi, \theta) = m_{0}(\pi, \theta) + m_1(\pi, \theta) \sigma_t^2 - \pi \sigma^2_{t+1} - \theta
    r_{t+1}.
\end{equation}
%
Except for $\theta r_{t+1}$, the terms on the right are constant given $\sigma^2_t, \sigma^2_{t+1}$, and so they do not affect the conditional covariance between $m_{t,t+1}$ and $r_{t+1}$.  Hence,
%
\begin{equation}
  \Cov\left(m_{t,t+1}, r_{t+1} \mvert \sigma^2_{t+1}, \sigma^2_t \right)  
%
  = \Cov\left(-\theta r_{t+1}, r_{t+1} \mvert \sigma^2_{t+1}, \sigma^2_t \right)  
%
  = -\theta \Var\left(r_{t+1} \mvert \sigma^2_{t+1}\right).  
\end{equation}
%
Plugging in \cref{eqn:rtn_cond_vol}, we obtain:
%
\begin{equation}
  \label{eqn:return_covarinace}
  \Cov\left(m_{t,t+1}, r_{t+1} \mvert \sigma^2_{t+1}, \sigma^2_t \right)  
%
  = -\theta (1 - \phi^2) \sigma^2_{t+1}.
\end{equation}

The expression in \cref{eqn:return_covarinace} is the change in the mean driven by investors' risk aversion, and is controlled by their equity risk price. To see this, note it equals zero if $\theta = 0$.

The second term we need is the Jensen's effect term. The mean of a conditionally log-Gaussian price depends on both the mean and variance of the underlying return. It has the standard form: minus one-half the conditional variance $\left(-\frac{1 - \phi^2}{2} \sigma^2_{t+1}\right)$. 

Having done this we can define a pseudo-return that only changes by an amount proportional to the leverage effect, and is not affected by the changes in the return arising from the reduction in risk. From \cref{eqn:rtn_cond_mean}, we know that $\E\left[r_{t+1} \mvert \sigma^2_t, \sigma^2_{t+1} \right] = \psi \sigma^2_{t+1}$ plus some function of $\sigma^2_t$. However, this $\psi$ contains all three effects. We want a pseudo-return $\widetilde{r}_{t+1}$ where $\E\left[\widetilde{r}_{t+1} \mvert \sigma^2_t, \sigma^2_{t+1} \right] = \widetilde{\psi} \sigma^2_{t+1}$ for some $\widetilde{\psi}$ that only contains the direct effect, not the Jensen or any risk-compensation. We can then solve for $\widetilde{\psi}$ in terms of the paramters governing the dynamics of $r_{t+1}$.

\begin{restatable}[Separating the Leverage Effect from the Measure Changes]{lemma}{leverageVersusMeasureChange}
  \label{lemma:separating_leverage_effect}

  Let $\tilde{r}_{t+1} \coloneqq r_{t+1} - \frac{1 - \phi^2}{2} \sigma^2_{t+1} + (1 - \phi^2) \theta
  \sigma^2_{t+1}$, and let the $m_{t+1}$ have the form given by \cref{eqn:log_sdf}.
  
  Then 
  
  \begin{equation}
    \Var\left[\tilde{r}_{t+1} \mvert \sigma^2_t \right] = \E\left[\sigma^2_{t+1} \mvert \sigma^2_t\right].
  \end{equation}
\end{restatable}
%
This is not the only way we can compute $\Var\left[\tilde{r}_{t+1} \mvert \sigma^2_t \right]$.
We can also use the law of total variance:
%
 \begin{equation}
  \Var\left[\widetilde{r}_{t+1} \mvert \sigma^2_t\right] =
  \E\left[\Var\left[\widetilde{r}_{t+1} \mvert \sigma^2_{t+1}\right] \mvert \sigma_t^2 \right] +
  \Var\left[\E\left[\widetilde{r}_{t+1}\mvert \sigma_{t+1}^2\right] \mvert \sigma^2_t\right].
\end{equation}

Since $\sigma^2_{t+1}$ is constant given $\sigma^2_{t+1}, \sigma^2_t$, the conditional variance of $\widetilde{r}_{t+1}$ is the same as $r_{t+1}$. In addition, $\E\left[\tilde{r}_{t+1} \mvert \sigma^2_{t+1}\right] = \psi \sigma^2_{t+1} - \frac{1-\phi^2}{2} \sigma^2_{t+1} + (1- \phi^2) \theta \sigma^2_{t+1}$ plus $\sigma^2_t$-measurable terms. We now fill in the values for both of the inside variables on the right-hand side using the formulas for $r_{t+1}$, and we use \cref{lemma:separating_leverage_effect} on the left-hand side.  We then take unconditional expectations of both sides, giving us

\begin{equation}
  \E\left[\sigma^2_{t+1} \right] = \E[ \E\left[(1 - \phi^2) \sigma^2_{t+1} \right]] +
  \E\left[\Var\left[\left(\psi - \frac{1-\phi^2}{2} + (1-\phi^2)\theta\right) \sigma^2_{t+1} \mvert
  \sigma^2_t\right]\right].
\end{equation}

Rearranging terms implies 

\begin{equation}
  \label{eqn:psi_tilde_eqn}
  \left(\psi - \frac{1-\phi^2}{2} + (1-\phi^2)\theta\right) = \phi \sqrt{\frac{\E\left[\sigma^2_{t+1}
  \right]}{\E \left[\Var\left(\sigma^2_{t+1} \mvert \sigma^2_t\right)\right]}}.
\end{equation}

We can now solve for $\psi$, and replace the moments on the right hand side by deriving the expectations from \cref{remark:vol_moment_conditions} and some elementary calculations. 

\begin{equation}
  \label{eqn:psi_pp_as_func_of_params}
  \psi = \frac{\phi}{\sqrt{c (1 + \rho)}} + \frac{1 - \phi^2}{2} - (1 - \phi^2) \theta
\end{equation}

Now that we have a formula for $\psi$, we can derive the formulas for $\beta$ and $\gamma$ in terms of the other reduced-form parameters and the risk prices. 

\begin{restatable}[Reparamaterizing the Physical Distribution]{lemma}{physicalMeasureFunctions}

  \label{lemma:psi_function}
  
  Let the SDF be given as in \cref{defn:SDF}, and the model be paramterized as in
  \cref{defn:physical_vol_dynamics} and \cref{defn:physical_return_dynamics}. 
  Then the following equation holds

  \begin{equation}
    \psi(\pi, \theta) = \frac{\phi}{\sqrt{c (1 + \rho)}} + \frac{1 - \phi^2}{2} - (1 - \phi^2) \theta. 
  \end{equation}
\end{restatable}

\begin{proof}
  This is a straightforward implication of \cref{lemma:separating_leverage_effect} and the derivation in the
  text.
\end{proof}

\subsubsection{Deriving the other Reduced-Form Parameters}

The final goal is to identify the risk-prices $\theta$ and $\pi$. In the previous sections, we have derived a series of moment conditions in terms of the parameters. We now need to analyze when these moment conditions allow to identify the risk prices.  The information that return data contain about equity pricing data is entirely encapsulated by the asset pricing equation for excess returns.  In what follows, we will use $rx_{t+1}$ as the excess log-return. The definition of $M_{t,t+1}$ as a change of measure means that the following holds:

\begin{equation}
  \E\left[ M_{t,t+1}(\theta, \pi) \exp(r_{t+1}) \mvert \F_{t} \right] = 1.
\end{equation}

We now characterize, the information in this set of moment conditions regarding the risk prices. The difficult part is identifying the volatility risk price --- $\pi$, and so we will focus first on the information regarding that parameter. Substituting in the SDF formula from above and replacing all of the dependence on $\F_t$ with $\sigma^2_t$, yields

%Where exactly should I mention that I focusing on pi?

\begin{gather}
  \E \left[ \exps*{ - \pi \sigma^2_{t+1} - (\theta - 1) rx_{t+1} } \mvert \sigma^2_t \right]
    = \exps*{- m_0(\theta, \pi) - m_1(\theta, \pi) \sigma^2_t}.
%
  \intertext{Similar to above, we use the law of iterated expectations to substitute in the conditional Laplace
    transforms of $r_{t+1}$ and $\sigma^2_{t+1}$, obtaining}
%
  \E\left[\exps*{- A\left(\pi + C(\theta -1)\right) \sigma^2_t - B(\pi) - D(\theta-1) \sigma^2_t - E(\theta-1)}
  \mvert \sigma^2_t \right] = \exps*{- m_0(\theta, \pi) - m_1(\theta, \pi) \sigma^2_t}. 
\end{gather}

We now derive forms for $\beta$ and $\gamma$ as functions of the reduced-form parameters and risk-prices.

\begin{lemma}[Reparamterizing $\beta$ and $\gamma$]
  \label{lemma:reparamterizing_beta_and_gamma}
  Let the SDF be given as in \cref{defn:SDF}, and the model be paramterized as in
  \cref{defn:physical_vol_dynamics} and \cref{defn:physical_return_dynamics} and assume that the risk-neutral
  measures have distributions with the same parametric forms. 


  \begin{align}
    \label{eqn:beta_function}
    \gamma &= B(\pi + C(\theta - 1) - B(\pi + C(\theta)) \\
  %
    \label{eqn:gamma_function}
    \beta &=  A(\pi + C(\theta -1)) - A(\pi + C(\theta)) 
  \end{align}

\end{lemma}

\begin{proof}
  This comes from matching the coefficients of the $\F_t$-measurable variables and using the linearity of $D(x)$
  and $C(x)$ from \cref{lemma:linearity_of_physical_functions}. 
  We substitute in the formulas for $m_0(\theta, \pi)$ and $m_1(\theta, \pi)$ that we derived in
  \cref{lemma:characterizing_sdf_integration_constants} which gives us to equations that we solve for $\gamma$
  and $\beta$.

\end{proof}
 
\subsection{Identification of the Risk Prices}

We can characterize the identification restrictions in \cref{lemma:reparamterizing_beta_and_gamma} in two
different cases.\footnote{These equations are Equation 3.7 in
\textcite{khrapov2016affine}.}

\begin{enumerate}
  \item[Case 1:] The price of equity risk $\theta$ satisfies the following equations. 
    \begin{equation}
      C(\theta - 1) = C(\theta) 
      \label{eqn:lack_of_id_condition}
    \end{equation}

    If \cref{eqn:lack_of_id_condition} holds, then some simple algebra shows that $E(\theta) = E(\theta-1)$
    and $D(\theta) = D(\theta-1)$.
    In this situation, any value of $\pi$ satisfies \cref{lemma:reparamterizing_beta_and_gamma}. 
    Since these are the only places $\pi$ shows up in the model, the asset pricing equation does not identify
    $\pi$. 
    As noted by \textcite{khrapov2016affine}, this is in line with the common belief that the econometrician
    needs options data to be able to identify the price of volatility risk. 

  \item[Case 2:] 
    In general, there is no reason to expect the \cref{eqn:lack_of_id_condition} to hold.
    If it does not, it might seem reasonable to expect that we should be able to identify both $\theta$ and
    $\pi$.
    In other words, we should, in principle, at least be able to identify $\pi$ from the difference between the
    functions in the previous case when evaluated at $\theta-1$ and $\theta$.
\end{enumerate}

We now show that if $\phi = 0$, then \cref{eqn:lack_of_id_condition} holds, but it does not hold if $\phi \neq 0$. In other words, as mentioned in \textcite[13]{khrapov2016affine}, a leverage effect will allow us to separately identify $\theta$ and $\pi$. To see this, we apply the definition of $C(x)$ from \cref{defn:physical_return_dynamics} and the formula for $\psi(\pi, \theta)$ from \cref{lemma:psi_function}. Some simple algebra gives 

\begin{align}
  C(\theta) - C(\theta - 1) &= C'(0) + C''(0) \left(\theta - \frac{1}{2}\right)
%
  = \psi + (1 - \phi^2) \left(\theta - \frac{1}{2}\right), \\
%
  &= \frac{\phi}{\sqrt{c (1 + \rho )}} + \frac{1 - \phi^2}{2} - (1 - \phi^2) \theta + (1 - \phi^2)
    \left(\theta - \frac{1}{2}\right) 
%
  \label{eqn:alpha_difference}
  = \frac{\phi}{\sqrt{c (1 + \rho )}}. 
\end{align}

This shows the leverage effect creates a wedge between $C(\theta)$ and $C(\theta-1)$ proportional to $\frac{\phi}{\sqrt{c (1 + \rho )}}$. Clearly, this equals zero if and only if $\phi = 0$. Consequently, if $\phi \neq 0$, we can separately identify $\pi$ and $\theta$.

\section{The Estimation Criterion in Population}\label{sec:EstimationCriterion}

To estimate the model we use the various moment conditions that we have derived. The one that we have yet to derive is the ones implied by the continuous-time model that allow us to estimate $\phi$ and $\psi$. Deriving them is straightforward because we constructed what the discrete-time equations, and they are linear.  

\subsection{Moment Conditions for Reduced-Form Parameters}\label{sec:moment_conditions}

First, we define two functions that we use to reparameterize the moment conditions. We do this because the model creates some cross-equation equations to eliminate two of the redundant parameters. By doing this we are able to avoid the econometric complications that we would have to handle in the general case. Note, we are only specifying values for the return conditional on both $\sigma^2_{t+1}$ and $\sigma^2_t$ because the other moments can be derived from them and the volatility moments. Also, it is worth noting that we consider \cref{eqn:beta_function}, \cref{eqn:gamma_function}, and \cref{lemma:psi_function} as implicitly defining those parameters as functions of the other parameters. \cref{eqn:cond_expected_rtn_moment} can be derived from those equation and the first derivative of the conditional log-cumulant function equaling the conditional mean.


\begin{defn}{Equilibrium Moment Conditions}
  \label{defn:equilibrium_moment_conditions}
  \begin{align}
    \label{eqn:cond_vol_mean}
    \E\left[\sigma^2_{t+1} \mvert \sigma^2_t \right] &= \rho \sigma^2_t + c \delta\\
%
    \label{eqn:cond_vol_var}
    \Var\left[\sigma^2_{t+1} \mvert \sigma^2_t \right] &= 2 c \rho \sigma^2_t + c^2 \delta \\
%
    \label{eqn:cond_expected_rtn_moment}
    \E\left[r_{t+1} \mvert \sigma^2_t, \sigma^2_{t+1}\right] &= \gamma(\pi, \theta) + \beta(\pi, \theta)
    \sigma^2_t + \psi(\pi, \theta) \sigma^2_{t+1} \\
%
    \label{eqn:cond_rtn_var}
    \Var\left[r_{t+1} \mvert \sigma^2_t, \sigma^2_{t+1}\right] &= (1 - \phi^2) \sigma^2_{t+1} 
\end{align}
\end{defn}

%TODO Bring in the discussion of the link function.

\subsection{Identified Set}\label{sec:identified_set}

Having constructed the moments and the instruments, we can use GMM to estimate the parameters. We do need to run a constricted optimization though, because only certain values of the parameters are valid.  Having done that we characterize the set where the GMM criterion strongly identifies the parameters.

\begin{restatable}[Identified Set]{lemma}{identifiedSet}

  Assume that the moment conditions specified in \cref{defn:equilibrium_moment_conditions} have the correct form
  and that the instruments we are using satisfy the standard exogeneity and relevant conditions. 

  Let the true parameter vector $\omega \coloneqq (\rho, c, \delta, \phi, \theta, \pi) \in [-1+\epsilon_1,
  1 - \epsilon_2] \times [M_1, M_2] \times [\epsilon_4, M_4]\times [M_5, M_6]\times \times [-1 +
  \epsilon_4, 1 - \epsilon_5] \times [M_7, M_8] \times [M_12, M_13]$, where the $M_{\ast}$ are some large (in
  magnitude) known constants and the $\epsilon_{\ast}$ are some small positive constants. 

  Let $Q_T(\omega, X)$ be the GMM objective function with moment conditions given in
  \cref{defn:equilibrium_moment_conditions}. 

  If there exists a $\epsilon$ such that $\abs{\phi} > \epsilon > 0$, then all of the parameters are identified. 
  If $\phi = 0$, then the objective function is independent of $\pi$. 
  Hence, $\pi$ is not identified but all of the other parameters are still identified.

\end{restatable}


We want to estimate $\omega \coloneqq (\rho, c, \delta, \phi, \pi, \theta)'$ and get its asymptotic distribution.
We also have some auxiliary parameters: $\xi_1 \coloneqq (\beta, \gamma, \psi)'$ and $\xi_2 \coloneqq \phi^2$.
The way we do this by splitting $\omega$ in two parts.
The first $\omega_s$ is the vector of purely structural parameters: $(\phi, \pi, \theta)'$.
The second part $\omega_r$ is composed of parameters that we can estimate directly without model-based
cross-equation restrictions, but are of interest.
We collect all of the reduced parameters into a vector: $\xi \coloneqq (\omega_r', \xi_a)'$.
We start by constructing a GMM estimator for $\xi$, we will then show how to convert this into an estimator for
$\omega$.
Throughout, I will denote the partial derivative of some function $f$ with respect to some variable $x$ with
$f_x$.

\section{Estimating the Reduced-Form Parameters}\label{sec:EstimatingReducedForm}

We view estimating the first stage as a particular form of GMM.
From standard GMM theory, we know that the following holds, for some asymptotic covariance matrix $\Omega_{\xi}$,
%
\begin{equation}
    \sqrt{T} (\hat{\xi} - \xi)  \dto N\left(0, \Omega_{\xi}\right)
\end{equation}

We construct $\Omega_{\xi}$ in three steps.
First, we will derive the asymptotic covariance matrices for each of $\omega_r, \xi_1, \xi_2$.
Then we will show how to combine them into one joint covariance matrix.

\subsection{Covariance of the Reduced-Form Parameters}\label{sec:omega_r}

\begin{equation}
    h\left(\sigma^2_{t},\sigma^2_{t+1} \mvert \omega_r\right) \coloneqq 
\begin{bmatrix}
    \sigma^2_{t+1} - (c \delta + \rho \sigma^2_{t}) \\
%
    \sigma^2_{t} \left(\sigma^2_{t+1} - (c \delta + \rho \sigma^2_{t})\right)\\
%
    \sigma^4_{t+1} - \left(c^{2} \delta + 2 c \rho \sigma^2_{t} + \left(c \delta + \rho
    \sigma^2_{t}\right)^{2}\right)\\
%
    \sigma^2_{t} \left(\sigma^4_{t+1} - \left(c^{2} \delta + 2 c \rho \sigma^2_{t} + \left(c \delta + \rho
    \sigma^2_{t}\right)^{2}\right)\right)\\

    \sigma^4_{t} \left(\sigma^4_{t+1} - \left(c^{2} \delta + 2 c \rho \sigma^2_{t} + \left(c \delta + \rho
    \sigma^2_{t}\right)^{2}\right)\right)\\
\end{bmatrix}
\end{equation}

By standard GMM theory, the following holds, if the weighting matrix \newline $W_{\omega_r,T} \pto \E[h(\sigma^2_t,
\sigma^2_{t+1} \ivert \omega_r)' h(\sigma^2_t, \sigma^2_{t+1} \ivert \omega_r)]^{-1}$.
We have 

\begin{equation}
    \sqrt{T}(\widehat{\omega}_r - \omega_r) \dto N\left(0, \Omega_{\omega_r}\right),
\end{equation}
%
where
%
\begin{equation}
    \Omega_{\omega_r} \coloneqq \left(\E\left[h_{\omega_r}(\sigma^2_{t}, \sigma^2_{t+1}, \xi_{1})\right]'
    \E[h(\sigma^2_t, \sigma^2_{t+1} \ivert \omega_r)' h(\sigma^2_t, \sigma^2_{t+1} \ivert \omega_r)]^{-1}
    \E\left[h_{\omega_r}(\sigma^2_{t}, \sigma^2_{t+1}, \xi_{1})\right]\right)^{-1}
\end{equation}
%
We estimate this by replacing the population expectations and covariances by their sample counterparts.

\subsection[Covariance Computation Step 2]{Covariance of $\xi_1$}\label{sec:est_xi2}

We estimate $\xi_1$ by weighted least squares, a special case of GMM: 
%
\begin{equation}
    \E\left[r_{t+1} \mvert \sigma^2_t, \sigma^2_{t+1}\right]  = \gamma + \beta \sigma^2_t + \psi \sigma^2_{t+1}.
\end{equation}

The only unusual part is we know that $\Var(r_{t+1} \ivert \sigma^2_t, \sigma^2_{t+1}) = (1-\phi^2) \sigma^2_{t+1}$.  Consequently, the regression results  are more efficient if we adjust for heteroskedasticity.  However, we do not know $\phi$, and so this might seem impossible.  However, since time-invariant parts of heteroskedasticity adjustments cancel, reweighting by the inverse of $\sigma^2_{t+1}$ achieves is equivalent to the optimal reweighting.  Also, since $\sigma^2_{t+1}$ is contained in the conditioning set, the fact that it is viewed as a random variable in other parts  of the regression is irrelevant.  Let $u_t = \frac{r_{t+1} - (\gamma + \beta \sigma^2_{t} + \psi \sigma^2_{t+1}}{\sigma^2_{t+1}}$.

Since this regression is exactly identified, any positive-definite weight matrix, including the identity is optimal. Consequently, we have the following result, where the WLS covariance matrix has the standard form:

\begin{equation}
    \Omega_{\xi_1} = \E\left[\left(1, \sigma^2_{t}, \sigma^2_{t+1}\right) \left(1, \sigma^2_{t},
    \sigma^2_{t+1}\right)'\right]^{-1} \Var\left(u_t\right).
\end{equation}


\subsection[Covariance Computation Step 3]{Covariance of $\xi_2$}

We know that $\Var(r_{t+1} \vert \sigma^2_{t+1} \sigma^2_t) = (1-\phi^2) \sigma^2_{t+1}$.
This implies $\Var(\frac{r_{t+1}}{\sigma_{t+1}} \vert \sigma^2_{t+1}, \sigma^2_t) = 1 - \phi^2$.
Since we consistently estimate the conditional mean of $r_{t+1}$ in \cref{sec:est_xi2}, the residuals ---
$\widehat{u}_t = \frac{r_{t+1} - \widehat{\gamma} - \widehat{\beta}\sigma^2_t - \widehat{\psi}
\sigma^2_{t+1}}{\sigma_{t+1}}$ --- satisfy $\frac{1}{T} \sum_{t=1}^T \hat{u}_t^2 \pto (1 - \phi^2)$.

Define $\widehat{\xi}_2 \coloneqq 1 - \frac{1}{T} \sum_{t=1}^T \hat{u}_t^2 \pto (1 - \phi^2)$.
Then $\widehat{\xi}_2 \dto N(0, \Omega_{\xi_2})$ for some covariance matrix $\Omega_{\xi_2}$, since this is a  GMM
estimator.
What is $\Omega_{\xi_2}$.
Since we are just identified, standard GMM theory says it is the covariance of the moment condition scaled by the
appropriate derivative. 
Since we are estimating a mean shifted by a constant, the derivative equals $1$.
Consequently, $\Omega_{\xi_2} = \Var(\frac{u^2_t}{\sigma^2_{t+1}})$, which can be estimated by
$\frac{1}{T} \sum_{t=1}^T (\frac{\widehat{u}_t^2}{\sigma^2_{t+1}} - \frac{1}{T} \sum_{t=1}^T
\frac{\widehat{u}_t^2}{\sigma^2_{t+1}})^2$, i.e.\@ the sample covariance of the squared residuals. 

\subsection[Combining the Covariance Matrices]{Combining $\Omega_{\omega_r}, \Omega_{\xi_2}$, and $\Omega_{\xi_2}$}

Each of $\Omega_{\xi_i}$ are of the form $(\E[h_{\xi_{i}}(\sigma^2_{t+1}, \sigma^2_t \ivert \xi_i)]' \Var(h(\sigma^2_{t+1}, \sigma^2_t \ivert \xi_i) \ivert \xi_i)^{-1} \E[h_{\xi_{i}}(\sigma^2_{t+1}, \sigma^2_t \ivert \xi_i) \ivert \xi_i])^{-1}$.  Consequently, the off-diagonal blocks of the joint covariance matrix $\Omega$ come from two places: the derivatives and the covariance of the moments.  Since the moments in the first stage do not depend on the parameters in the second stage, and vice-versa, the derivatives to not cause any co-movement.  The other cases are trickier, and so we consider them each in turn.  

Consider the covariance between $h(\sigma^2_{t+1}, \sigma^2_t, \omega_r)$ and $h(\sigma^2_{t+1}, \sigma^2_t, \xi_2)$, which we can rearrange since the moments are mean zero.  
%
\begin{equation}
    \Cov\left(h\left(r_{t+1}, \sigma^2_{t+1}, \sigma^2_t \mvert \omega_{r} \right) ,
      h\left(\sigma^2_{t+1} \sigma^2_t \mvert \xi_2 \right) \right) 
%
    = \E\left[h(r_{t+1},  \sigma^2_{t+1}, \sigma^2_t \mvert \omega_r) h\left(\sigma^2_{t+1}, \sigma^2_t \mvert
       \xi_2 \right) \right]
\end{equation}
%
By the law of iterated expectation, that equals 
%
\begin{equation}
    \E\left[\E[h\left(r_{t+1},  \sigma^2_{t+1}, \sigma^2_t \mvert \omega_r\right) \mvert \sigma^2_{t+1},
       \sigma^2_t] \E\left[h\left(\sigma^2_{t+1}, \sigma^2_t \mvert \xi_2 \right) \mvert \sigma^2_{t+1},
       \sigma^2_t\right] \right].
\end{equation}

The first term in the expression above equals zero, and, hence, so does the entire expression.  In other words, the first two set of moment conditions are independent.  By an identical argument, the first and third moments are independent as well.

We now consider how the second and their sets of moments are related.  Since the derivatives are with respect to different parameters (and constant) no dependence arises from there.  The question is how are the moment conditions in the second and third steps related.  The second stage moment condition is a conditional mean and third stage moment is a conditional covariance.  Let $u_t$ denote the error term in that regression (as it did above). Then
%
\begin{equation}
    \E\left[\E\left[\frac{r_{t+1} - \E\left[r_{t+1}\mvert \sigma^2_{t+1} \sigma^2_t\right]}{\sigma_{t+1}} \right]
    \E\left[\frac{(r_{t+1} - \E\left[r_{t+1} \mvert \sigma^2_{t+1} \sigma^2_t\right])^2}{\sigma^2_{t+1}}\right]
    \right] 
%
    = \E\left[\E\left[\frac{u_t u_t^2}{\sigma^2_{t+1}}\right] \mvert \sigma^2_t, \sigma^2_{t+1} \right]  = 0.
\end{equation}
%
Since $u_t$ is conditionally Gaussian, its conditional third moment equals zero. By the law of iterated expectations, its unconditional moment does as well.

In other words, the second and third set of moments are uncorrelated.  Now, the careful reader might be worried about filling in the population expectations instead of their estimators in the regression above.  However, since the expectations are linear and consistently estimable, this error vanishes in the limit.  Intuitively, OLS mean and variance estimates are asymptotically independent.

In addition, since all three components are asymptotically independent; the inverse of a block-diagonal matrix is block-diagonal, and we using optimal weighting matrices in each part, we are using an optimal weighting matrix for $\xi$, not just its components.

\subsection{Asymptotic Distribution of the Reduced-Form Parameter}

This section gives the asymptotic distribution of the reduced-form parameter.  Write $\omega =(\omega_{1},\omega_{2},\omega_{3}),$ where $\omega_{1}=(\rho ,c)$, $\omega_{2} = (\gamma ,\beta ,\psi)$, and $\omega_{3} = \phi ^{2}$.  Below we describe the estimator $\widehat{\omega}_{1},\widehat{\omega}_{2},\widehat{\omega}_{3}$ and provide the asymptotic distribution of $\widehat{\omega} = (\widehat{\omega}_{1},\widehat{\omega }_{2},\widehat{\omega}_{3})$.  We estimate these parameters separately because $\omega_{1}$ only shows up in the conditional mean and variance of $r_{t+1};$ $\omega_{2}$ only shows up in the conditional mean of $\sigma_{t+1}^{2};$ and $\phi $ only shows up in the conditional variance of $\sigma_{t+1}^{2}.$ 

We estimate $\omega_{1}$ by GMM based on the moment condition: 
%
\begin{eqnarray}
    \mathbb{E}[h_{t}(\omega_{1,0})] & = &0,\text{ where}  \nonumber \\
%
    h_{t}(\omega_{1}) & = &\left(
        \begin{array}{c} 
            \sigma_{t+1}^{2}-\left( c\delta +\rho \sigma_{t}^{2}\right)  \\ 
%
            \sigma_{t}^{2}\left( \sigma_{t+1}^{2}-\left( c\delta +\rho \sigma _{t}^{2}\right) \right)  \\ 
%
            \sigma_{t+1}^{4}-\left( c^{2}\delta +2c\rho \sigma_{t}^{2}+\left( c\delta +\sigma_{t+1}^{2}-\left(
            c\delta +\rho \sigma_{t}^{2}\right) ^{2}\right) \right)  \\ 
%
            \sigma_{t}^{2}\left( \sigma_{t+1}^{4}-\left( c^{2}\delta +2c\rho \sigma _{t}^{2}+\left( c\delta
            +\sigma_{t+1}^{2}-\left( c\delta +\rho \sigma _{t}^{2}\right) ^{2}\right) \right) \right)  \\ 
%
            \sigma_{t}^{4}\left( \sigma_{t+1}^{4}-\left( c^{2}\delta +2c\rho \sigma _{t}^{2}+\left( c\delta
            +\sigma_{t+1}^{2}-\left( c\delta +\rho \sigma _{t}^{2}\right) ^{2}\right) \right) \right) 
        \end{array}\right).
\end{eqnarray}
%
The optimal GMM estimator is
%
\begin{eqnarray}
    \widehat{\omega}_{1} & = & \underset{\omega_{1}\in \Lambda_{1}}{\arg\min} 
%
    \overline{h}_{T}(\omega_{1})^{\prime}W_{T}\overline{h}_{T}(\omega_{1}), \text{ where}  \nonumber \\ 
%
    \overline{h}_{T}(\omega_{1}) & = &T^{-1}\sum_{t = 1}^{T}h_{t}(\omega_{1}), \nonumber \\
%
    W_{T} & = &T^{-1}\sum_{t = 1}^{T}h_{t}(\widetilde{\omega}_{1})h_{t}(\widetilde{\omega}_{1})^{\prime
   }-\overline{h}_{T}(\widetilde{\omega}_{1})\overline{h} _{T}(\widetilde{\omega}_{1})^{\prime},
\end{eqnarray}
%
where $\widetilde{\omega}_{1}$ is the preliminary GMM estimator based on the identify covariance matrix.

We estimate $\omega_{2}$ by the GLS estimator because $\gamma, \beta, \psi$ are the intercept and linear
coefficients of the conditional mean function and the conditional variance is proportional to $\sigma_{t+1}^{2}$. 
Define $x_{t} = \sigma_{t+1}^{-1}(1,\sigma_{t}^{2},\sigma_{t+1}^{2})$ and $y_{t} = \sigma_{t+1}^{-1}r_{t+1}$. 
The GLS\ estimator of $\omega_{2}$ is
%
\begin{equation}
    \widehat{\omega}_{2} = \left( \sum_{t = 1}^{T}x_{t}x_{t}^{\prime}\right) ^{-1}\sum_{t = 1}^{T}x_{t}y_{t}.
\end{equation}

We estimate $\omega_{3}$ by the sample variance estimator. 
Define 
%
\begin{equation}
    \widehat{y}_{t} = x_{t}\widehat{\omega}_{2} = \sigma_{t+1}^{-1}(\widehat{\gamma}+\widehat{\beta
   }\sigma_{t}^{2}+\widehat{\psi}\sigma_{t+1}^{2}).
\end{equation}
%
The estimator of $\omega_{3}$ is 
\begin{equation}
    \widehat{\omega}_{3} = \max \{1-T^{-1}\sum_{t = 1}^{T}\left( y_{t}-\widehat{y}_{t}\right) ^{2},0\}.
\end{equation}%
[**XC. In practice, do we need to impose the estimator is positive?]

The next lemma provides the asymptotic distribution of the estimator $\widehat{\omega}$.  Let $h_{\omega ,t}(\omega_{1})\in \mathbb{R}^{5\times 2}$ denote the derivative of $h_{t}(\omega_{1})$ w.r.t.\@ $\omega_{1}$.  Define 
%
\begin{eqnarray}
    \Omega_{1} & = &\left \{ \mathbb{E}\left[ h_{\omega ,t}\left( \omega _{1,0}\right) \right]^{\prime}
        \mathbb{E}[h_{t}(\omega_{1,0})h_{t}(\omega _{1,0})^{\prime}]^{-1}\mathbb{E}\left[
    h_{\omega,t}\left(\omega_{1,0}\right) \right] \right \} ^{-1},  \notag \\ 
%
    \Omega_{2} & = &\mathbb{E}\left[ x_{t}x_{t}^{\prime}\right] ^{-1}\mathbb{E[(} y_{t}-x_{t}^{\prime
   }\omega_{2,0})^{2}],  \notag \\
%
    \Omega_{3} & = &\mathbb{V[}\left( y_{t}-x_{t}^{\prime}\omega_{2,0}\right)^{2}].
\end{eqnarray}

\begin{lemma}
Suppose Assumptions *** hold. Then

\begin{equation}
    T^{1/2}\left( 
%
    \begin{array}{c}
        \widehat{\omega}_{1}-\omega_{1,0} \\ 
        \widehat{\omega}_{2}-\omega_{2,0} \\ 
        \widehat{\omega}_{3}-\omega_{3,0}%
    \end{array}\right) 
%
    \rightarrow_{d}\xi_{\omega}  = 
%
   \left(\begin{array}{c}
            \xi_{\omega 1} \\ 
            \xi_{\omega 2} \\ 
            \xi_{\omega 3}%
   \end{array}\right) 
%
   \sim N\left( 0,
%
    \begin{array}{ccc}
        \Omega_{1} & 0 & 0 \\ 
        0 & \Omega_{2} & 0 \\ 
        0 & 0 & \Omega_{3}%
    \end{array}\right).
\end{equation}
\end{lemma}


\begin{proof}
    Will be added later.
\end{proof}



\section{Estimating the Structural Parameters}\label{sec:EstimatingStructural}

\subsection{Constructing the Link Function}\label{sec:construct_link_func}

In this stage, we convert the estimates for $\xi$ into estimates for $\omega$.  We do this by specifying a link function.  Since $(\rho, c, \delta)'$ shows up in both $\xi$ and $\omega$, the link function for those parameters is the identify function.  The third set of parameters $\xi_3 = \phi^2$, and so we use that as a link function.  We also use the reduced-form estimates as link functions, as well, i.e.,
%
\begin{equation}
    g(\xi, \omega) = \xi - (\rho, c, \delta, \beta(\rho, c, \phi, \pi, \theta), \gamma(\rho, \delta, c, \phi, \pi,
    \theta), \psi(\rho, c, \phi, \theta), \phi^2)'.
\end{equation}

We specify $g(\xi, \omega)$ in this way because it gives us the correct off-diagonal elements.  If we estimated $(\rho, c, \delta)$ by themselves and just plugged them in here, we would have to relate the off-diagonal terms in a separate step.  In addition, by using the optimal weight matrix for this stage, we estimate all of the parameters as efficiently possible using our moment conditions.

Doing this implies we must be careful and treat the $\rho,c$, and $\delta$ on each sides of the equation as different.  The 2nd-stage sample criterion function is 
%
\begin{equation}
    Q_T(\omega) \coloneqq \frac{1}{2} g(\widehat{\xi}_T, \omega)' \W_{T} g(\widehat{\xi}_T, \omega),
\end{equation}
%
with second stage weight matrix $\W_T$.
We need to estimate $\omega$, and so we differentiate and get the first-order condition at $\omega_0$, where $\W
\coloneqq \lim_{T \to \infty} \E[W_T]$.
This gives
%
\begin{equation}
    \frac{\partial Q_T}{\partial \omega}(\omega_0) =  g_{\omega}\left(\xi_0, \omega_0\right)  \W g\left(\xi_0,
    \omega_0\right) = 0.
\end{equation}
%
We now expand $\widehat{\xi}_T$ around $\xi_0$, for some $\widetilde{\xi}_T$ between $\xi_0$ and $\widetilde{\xi}_T$, 
%
\begin{align}
    \sqrt{T} \frac{\partial Q}{\partial \omega}(\omega_0) 
%
    &= g_{\omega}\left(\omega_0, \widehat{\xi}_T\right) \W_T \left[\sqrt{T} g\left(\omega_{0},\xi_0\right) +
       g_{\xi}\left(\omega_{0}, \widetilde{\xi}_T\right)' \sqrt{T} \left(\widetilde{\xi}_T - \xi_0\right)\right]
%
%
    \intertext{The first term equals zero by assumption, and the derivative is the identity matrix.}
%
%
    &= g_{\omega}\left(\omega, \widehat{\xi}_T\right) \W_T \left[ \sqrt{T} \left(\widetilde{\xi}_T -
       \xi_0\right)\right]
\end{align}
%
We also need to compute the Hessian and evaluate it at $\widehat{\omega}_T$ a consistent estimator for $\omega$.
%
\begin{align}
    \frac{\partial^2 Q}{\partial \omega \partial \omega'}(\widehat{\omega}_T) &=
%
    g_{\omega}\left(\widehat{\omega}_T, \widehat{\xi}_T\right)' \W_T g_{\omega} \left(\widehat{\omega}_T,
    \widehat{\xi}_T\right)+ g_{\omega, \omega}\left(\widehat{\omega}_T, \widehat{\xi}_T\right) \W_T
    g\left(\widehat{\omega}_T, \widehat{\xi}_T\right)' \\
%
    &\pto g_{\omega}\left(\omega, \xi_0\right)' \W g_{\omega} \left(\omega, \xi_0\right) + 0. 
\end{align}
%
if we use the optimal weight matrix --- $\W \coloneqq \E[g_{\omega_s}(\theta_0, \xi_0)]$.  Standard extremum estimator theory gives
%
\begin{equation}
    \sqrt{T} \left(\widehat{\omega}_T - \omega_{0}\right)  \dto N\left(0, \left(\E[g_{\omega}(\theta_0, \xi_0)]'
    \Omega_{\xi}^{-1} \E[g_{\omega}(\theta_0, \xi_0)]\right)^{-1}\right).
\end{equation}
%
The covariance in the middle is GMM-covariance of the reduced-form parameters.  We estimate it by plugging $\widehat{\xi}$ into to the formulas above and their derivatives.  We estimate the asymptotic covariance matrix by replacing its components with their sample counterparts.

\subsection{Robust Inference for Risk Price}

The reduced-form parameters are $\omega  = (\rho ,c,\gamma ,\beta ,\psi ,\phi ^{2})$. 
Using the conditional mean and conditional variance derived in the paper, we estimate $\, \omega_{1} = (\rho ,c)$
by the GMM estimator, estimate $\omega_{2} = (\psi ,\beta ,\gamma )$ by the GLS estimator, and estimate
$\omega_{3} = \phi^{2}$ by the method of moments estimator for the variance.

We can show that the estimator $\widehat{\omega}$ satisfies

\begin{equation}
    T^{1/2}(\widehat{\omega}-\omega_{0})\rightarrow_{d}\upsilon_{\omega}\sim N(0,V).
\end{equation}%

See the next section for details. 
Note that these estimators do not involve the structural parameters $\theta $ and $\pi$.
We do not plug in $\beta ,\gamma ,\psi $ as functions of $\theta $ and $\pi .$ Instead, we treat $\beta ,\gamma
,\psi $ just as linear coefficients and estimate them by GLS.

We estimate the structural parameters $\theta $ and $\pi $ using $\widehat{ \omega}$ and the link functions
specified below. 
First, we know that
%
\begin{equation}
    \label{psi_fn} 
    \psi_{0}=\phi_{0}\left( c_{0}\left( 1+\rho_{0}\right) \right) ^{-1/2} - \left( 1-\phi_{0}^{2}\right)
    /2-(1-\phi_{0}^{2})\theta_{0}
\end{equation}
%
when all parameters are evaluated at the true values. 
This equation strongly identifies $\theta_{0}$ because $\phi_{0}$ is assumed to be negative and bounded away from
1 in magnitude.. 
It follows from (\cref{psi_fn}) that

\begin{equation}
    \theta_{0} = L(\omega_{0})=-(1-\phi_{0}^{2})^{-1}\left[ \psi_{0}-\phi_{0}\left( c_{0}\left( 1+\rho_{0}\right)
    \right)^{-1/2}-\left( 1-\phi_{0}^{2}\right) /2\right] .
\end{equation}
%
Thus, we estimate $\theta_{0}$ by
%
\begin{equation}
    \widehat{\theta}=L(\widehat{\omega}).
\end{equation}
%
By the delta method, we know that
%
\begin{equation}
    T^{1/2}(\widehat{\theta}-\theta_{0})\rightarrow_{d}L_{\omega}(\omega_{0})^{\prime}\upsilon_{\omega},
\end{equation}
%
where $L_{\omega}(\omega )\in R^{d_{\omega}}$ denote the derivative of $L(\omega )$ w.r.t.\@ $\omega$. 
The inference for $\theta $ is standard. 
A confidence interval for $\theta $ can be obtained by inverting the $t$-statistic with a critical value obtained
from the standard normal distribution.

Next, we consider inference for the structural parameter $\pi$. 
This is a non-standard problem because $\pi $ is potentially weakly identified. 
Define
%
\begin{equation}
    g(\pi ,\omega )=\left( 
%
    \begin{array}{c}
        \gamma -\left[ B\left( \pi +C\left( \theta_{L}-1\right) \right) -B\left( \pi +C\left( \theta_{L}\right)
        \right) \right]  \\ 
%
        \beta -\left[ A\left( \pi +C\left( \theta_{L}-1\right) \right) -A\left( \pi +C\left( \theta_{L}\right)
        \right) \right] 
    \end{array}\right),
%
    \text{ where}\ \theta_{L} = L(\omega).
\end{equation}
%
We know 
%
\begin{equation}
    g(\pi_{0}, \omega_{0})=0 \in \mathbb{R}^{2}.
\end{equation}
%
Inference on $\pi $ is based on the function $g(\pi ,\widehat{\omega})$ because $\widehat{\omega}$ is a
consistent estimator of $\omega_{0}$.
By the consistency of $\widehat{\omega}$,
%
\begin{equation}
    T^{1/2}\left[ g(\pi, \widehat{\omega}) - g(\pi,\omega_{0})\right] \Rightarrow \upsilon(\pi) = G(\pi,
    \omega_{0})^{\prime}\upsilon_{\omega},
\end{equation}%

\noindent where $G(\pi ,\omega )$ denote the derivative of $g(\pi ,\omega )$ w.r.t.\@ to $\omega$. 
The Gaussian process $\upsilon(\pi)$ has covariance kernel 
%
\begin{equation}
    \Sigma (\pi_{1},\pi_{2})=G(\pi_{1},\omega_{0})^{\prime}VG(\pi_{2},\omega_{0}).
\end{equation}
%
We can estimate $\Sigma (\pi_{1},\pi_{2})$ by 
%
\begin{equation}
    \widehat{\Sigma}(\pi_{1},\pi_{2})=G(\pi_{1},\widehat{\omega})^{\prime}
    \widehat{V}G(\pi_{2},\widehat{\omega}),
\end{equation}

\noindent where $\widehat{V}$ is a consistent estimator of $V.$

We construct a confidence interval for $\pi $ by inverting tests $H_{0}:\pi =\pi_{0}$ vs $H_{0}:\pi \neq \pi_{0}$. 
The test statistic is the QLR statistic:
%
\begin{equation}
    QLR=Tg(\pi_{0},\widehat{\omega})^{\prime}\widehat{\Sigma}(\pi_{0},\pi _{0})^{-1}g(\pi_{0},\widehat{\omega})
    - \underset{\pi \in \Pi}{\min}Tg(\pi ,\widehat{\omega})^{\prime}\widehat{\Sigma}(\pi,\pi)^{-1}
    g(\pi,\widehat{\omega}).  
\end{equation} 

To obtain the critical value, we follow the conditional inference approach in Andrews and Mikusheva (2016). To
this end, first construct a projection residual process:
%
\begin{equation}
    h(\pi ,\widehat{\omega})=g(\pi ,\widehat{\omega})-\widehat{\Sigma}(\pi ,\pi_{0})\widehat{\Sigma
   }(\pi_{0},\pi_{0})^{-1}g(\pi_{0},\widehat{\omega}).
\end{equation}
%
By construction, $h(\pi ,\widehat{\omega})$ and $g(\pi_{0},\widehat{\omega})$ are independent asymptotically. 
Conditional on $h(\pi ,\widehat{\omega})$, we obtain the $1-\alpha $ quantile of the QLR statistic, denoted
$c_{\alpha}(h)$, by sampling from the asymptotic distributions of $g(\pi_{0},\widehat{\omega})$ under the null.
Specifically, we take independent draws $\upsilon^{\ast}\sim N(0,\Sigma (\pi_{0},\pi_{0}))$ and produce simulated
process:
%
\begin{equation}
    g^{\ast}(\pi ,\widehat{\omega}) = h\left(\pi ,\widehat{\omega}\right) + \widehat{\Sigma} (\pi
    ,\pi_{0})\widehat{\Sigma}(\pi_{0},\pi_{0})^{-1}\upsilon^{\ast}.
\end{equation}%
%
We then calculate
\begin{equation}
    QLR^{\ast}=Tg^{\ast}(\pi_{0},\widehat{\omega})^{\prime}\widehat{\Sigma} (\pi_{0},\pi_{0})^{-1}g^{\ast
   }(\pi_{0},\widehat{\omega})-\underset{\pi \in \Pi}{\min}Tg^{\ast}(\pi ,\widehat{\omega})^{\prime
   }\widehat{\Sigma} (\pi ,\pi )^{-1}g^{\ast}(\pi ,\widehat{\omega}), 
\end{equation}
%
which is a random drawn from the conditional distribution of the $QLR$ statistic given $h_{T}(\pi ,\widehat{\omega
})$, when $g(\pi_{0},\widehat{ \omega})$ is drawn from its asymptotic distribution. 
In practice, we repeat this process for a large number of times and obtain $c_{\alpha}(h)$ by simulation.

We reject the null $H_{0}:\pi =\pi_{0}$ if $QLR\geq c_{\alpha}(h)$. 
The confidence interval for $\pi $ is the collection of null values that are not rejected as the null value. 
Note that the construction of this CI\ does not involve estimation of $\pi$.

\section{Simulations}

\section{Data}\label{sec:data}

\section{Empirical Results}

\section{Conclusion}

\newpage

\phantomsection
\addcontentsline{toc}{section}{References}
\printbibliography
\newpage

\begin{appendices}


\section{Model Characterization}\label{app:model_characterization}

\begin{lemma}[Linearity of $\beta$ and $\gamma$]
  \label{lemma:linearity_of_physical_functions}
  Letting $\sigma^2_{t+1}$ be the integrated volatility of a process with return $r_{t+1}$.
  Assume that $\sigma^2_{t+1}$ and $r_{t+1}$ follow a bivariate CAR(1) process parametrized as in
  \cref{defn:physical_vol_dynamics} and \cref{defn:physical_return_dynamics}. 
  Then $\beta''(0)$ and $\gamma''(0)$ both equal zero.
\end{lemma}

\begin{proof}
  By the \Ito\ Isometry, and the definition of $r_{t+1}$ as an integrated variance, the following holds for the
  returns' predictable information set $\F^r_{t-}$. 

  \begin{equation}
    \Var\left(r_{\tau+1}\mvert \F_{\tau-}\right) \leq \E\left(r^2_{\tau+1} \mvert \F_{\tau-}\right) 
    = \E\left(\sigma^2_{t+1}\mvert \F_{\tau-}\right)
  \end{equation}

  The integrated volatlity is predictable, and so $\sigma^2_{t+1}$ is contained in the return's predictable
  $\sigma$-algebra. 

  Consequently, $\Var\left(r_{t+1} \mvert \sigma^2_t, \sigma^2_{t+1}\right) \leq \sigma^2_{t+1}$
  In addition, variance must always be positive, and so $\Var\left(r_{t+1} \mvert \sigma^2_t,
  \sigma^2_{t+1}\right) \geq 0$.

  Since the second derivative of the log-cumulant funciton evaluted at zero equals the variance, we have the
  following set of inequalities.

  \begin{gather}
    0 \leq (1 - \phi^2) \sigma^2_{t+1} - \beta''(0) \sigma^2_t - \gamma''(0) \leq
    \sigma^2_{t+1} 
%
    \intertext{Dividing through by $\sigma^2_{t+1}$ and pulling the first term outside}
%
    \label{eqn:second_derivative_inequalities}
    \implies \phi^2 - 1 \leq -\frac{1}{\sigma^2_{t+1}} \left(\beta''(0) \sigma^2_t +
    \gamma''(0)\right) \leq \phi^2 
%
  \end{gather}

  On the outside of the two inequalities we have constants, and the distribution of $\sigma^2_{t+1}$ given
  $\sigma^2_t$ is not bounded away from zero.
  Consequently, the only way for \cref{eqn:second_derivative_inequalities} to hold for all $\sigma^2_{t+1}$ is
  if the term inside the parantheses equals zero.

  \begin{equation}
    0 = \beta''(0) \sigma^2_t + \gamma''(0)
  \end{equation}

  However, the only way for this to hold is for both $\gamma''(0)$ and $\beta''(0)$ to equal zero.
  This plus the condtional gaussianity of the returns implied by the paramtric model implies that $\gamma$
  and $\beta$ are both linear.

\end{proof}


\sdfConstants*

\begin{proof}

\begin{equation}
  \label{eqn:reweigted_sdf}
  \E\left[\exp\left(m_0(\theta, \pi) + m_1(\theta, \pi) \sigma^2_t - \pi \sigma^2_{t+1} - \theta r_{t+1}
  \right) \mvert \F_t \right] = 1 
\end{equation}

We can use \cref{eqn:reweigted_sdf} to relate $m_0(\theta, \pi)$ and $m_1(\theta, \pi)$ to the physical measure
functions. 

\begin{gather}
  \E\left[\exp\left(m_0(\theta, \pi) + m_1(\theta, \pi) \sigma^2_t - \pi \sigma^2_{t+1} - \theta r_{t+1}
  \right) \mvert \F_t \right] = 1 \\
%
  \intertext{By the law of iterated expectations.}
%
  \E\left[ \E\left[\exp\left(m_0(\theta, \pi) + m_1(\theta, \pi) \sigma^2_t - \pi \sigma^2_{t+1}\right)
    \exp\left( - \theta r_{t+1}\right) \mvert \F_t, \sigma^2_{t+1} \right]\right] = 1 \\
%
  \intertext{The second term is the Laplace transform of $r_{t+1}$.}
%
  \E\left[\exp\left(m_0(\theta, \pi) + m_1(\theta, \pi) \sigma^2_t - \pi \sigma^2_{t+1} \right)
    \exp(-C(\theta) \sigma^2_{t+1} - D(\theta) \sigma^2_{t} - E(\theta_2) \mvert \F_t \right] = 1 \\
%
  \intertext{Reorganizing terms.}
%
  \E\left[\exp\left(m_0(\theta, \pi) + m_1(\theta, \pi) \sigma^2_t - D(\theta) \sigma^2_{t} - E(\theta_2)
  \right) \exp(-\left(\pi + C(\theta)\right) \sigma^2_{t+1}) \mvert \F_t \right] = 1 \\ 
%
  \intertext{Substituting in the Laplace transform for $\sigma^2_{t+1}$.} 
%
  \label{eqn:expected_sdf_wrt_PP}
  \E\left[\exp(m_0(\theta, \pi) + m_1(\theta, \pi) \sigma^2_t - D(\theta) \sigma^2_{t} - E(\theta_2) - A(\pi +
  C(\theta)) - B(\pi + C(\theta)) \mvert \F_t \right] = 1 
\end{gather}

\end{proof}

\leverageVersusMeasureChange*


\begin{proof}

  We start by considering the expectation of $\widetilde{r}_{t+1}$ and show that it equals zero.

  \begin{align}
    \E\left[\widetilde{r}_{t+1} \mvert \sigma^2_{t} \right] &= \E\left[\E\left[r_{t+1} - \frac{1 - \phi^2}{2}
    \sigma^2_{t+1} + (1 - \phi^2) \theta \sigma^2_{t+1}\mvert \sigma^2_{t}, \sigma^2_{t+1} \right] \mvert
    \sigma^2_t\right] \\
%
    \intertext{By the conditional Gaussianity of $r_{t+1}$, we can absorb the convexity correction into an
    exponential. Normally, it would introduce a $1/2$ variance term, but that cancels. The second term is a
    measurable with respect to the conditioning information and so is not affected.}
%
    &= \E\left[\log \E\left[\exp(r_{t+1}) \exp((1 - \phi^2) \theta \sigma^2_{t+1})\mvert \sigma^2_{t},
      \sigma^2_{t+1} \right] \mvert \sigma^2_t\right] \\
%
    \intertext{We note that normally, the covariance term would cause the mean to fall, but again that term
    cancels with the second term. We can divide through by $\exp(r_t)$ and $m_{t-1,t}$ because they are
    measurable with respect to $\F_t$, and the remainder of the SDF is also contained within the information
    set, and so we can add it as well (\cref{eqn:log_sdf}).} 
%
    &\propto \E\left[\log \E\left[\frac{\exp(r_{t+1})}{\exp(r_t)} \frac{M_{t,t+1}}{M_{t-1,t}} \mvert
     \sigma^2_t, \sigma^2_{t+1} \right] \mvert \sigma^2_t \right]
%
    \intertext{We can pull the log outside of the outer expectation while adding at most a
    $\sigma^2_t$-measurable term.}
%
    &\propto \log \E\left[\frac{\exp(r_{t+1})}{\exp(r_t)} \frac{M_{t,t+1}}{M_{t-1,t}} \mvert \sigma^2_t
     \right]
%
    \intertext{This is the log expectation of a price change in the price discounted by the change in the
    SDF.}
%
    &= 0
  \end{align}

Because the mean of $\tilde{r}_{t+1}$ given $\sigma^2_t$ does not change in expectation when we condition on
$\sigma^2_{t+1}$ we can apply the \Ito\ Isometry.

\begin{equation}
  \label{eqn:return_var_versus_expected_vol_pp}
  \E\left[\Var\left(\tilde{r}_{t+1} \mvert \sigma^2_t\right) \right] = \E[ \sigma^2_{t+1}]
\end{equation}

Intuitively, volatilities are squared returns, and so they are variances.
The tricky part here is that variances are centered second moments, not the second moments themselves.
The volatilities are also centered second moments, but the centering is not the same in general.
In continuous time, they would only differ by a drift term, which can be ignored, which is why the Ito\ Isometry
usually is used in that context.
Using discrete-time return, as we do here, we first have to appropriately recenter the variables, which is why it
applies to $\widetilde{r}_{t+1}$ but not to $r_{t+1}$.

\end{proof}


\section{Identification Proofs}


\identifiedSet*

\begin{proof}
  Since $Q_{T}$ is a quadratic in terms of deviations between sample and population moment conditions as long as
  the population moment can be inverted to solve for the parameters, the $Q_T$ process identifies them as well. 
  In addition, since we have a sufficient number of exogenous valid instruments the conditioning implied by
  projecting on the instruments does not affect the arguments above. 

  Identifying the four parameters that govern the volatility dynamics is not particularly complicated. 
  We have four parameters and four non-redundant moment conditions.
  The first two equations in  \cref{defn:equilibrium_moment_conditions} identify $\rho$ and $c \delta$.
  \cref{eqn:cond_vol_var} identifies $\rho c$ and $c^2 \delta$. 
  This allows us to separately identify $c$ and $\delta$.
  
  Identifying $\phi$ is also relatively straightforward. Since $r_{t+1}$ and $\sigma^2_{t+1}$ are known, as long
  as we know the conditional mean of $r_{t+1}$, then identifying $\phi$ is identified by \cref{eqn:cond_rtn_var}.
  Identifying the conditional mean of $r_{t+1}$ is straightforward because we observe volatility and the
  conditional mean is a linear equation in these variables. 
  

  Identifying the risk prices $\pi$ and $\theta$ is more complicated.
  We have to identify both parameters off of \cref{eqn:cond_expected_rtn_moment}. 
  This is in principle possible because we now have two non-redundant sources of variation in the data ---
  $\sigma^2_t$ and $\sigma^2_{t+1}$.

  The only place that \cref{defn:equilibrium_moment_conditions} that the risk-prices occurs in
  \cref{eqn:cond_expected_rtn_moment}. 
  We showed that $\gamma, \beta$ and $\psi$ functions are independent of $\pi$ if $\phi = 0$ in the discussion
  leading up \cref{eqn:alpha_difference}.
  We further showed that they are not independent if $\phi \neq 0$.
  In addition the dependence of the identification in terms of the non-singularity of the derivatives
  equilibrium conditions in terms of $\pi$ and $\theta$ depends smoothly on $\phi$. 
  This will be important later.
  
\end{proof}



\end{appendices}


\end{document}



\documentclass[11pt, letterpaper, twoside, final]{article}
\usepackage{risk_price_inference}
\addbibresource{riskpriceinference.bib}

\author{Xu Cheng\thanks{University of Pennsylvania, The Perelman Center for Political Science and
    Economics, 133 South 36th Street, Philadelphia, PA 19104, \href{mailto:xucheng@upenn.edu}{xucheng@upenn.edu}}
    \and 
    Eric Renault\thanks{Brown University, Department of Economics -- Box B, 64 Waterman Street, Providence, RI
    02912, \href{mailto:eric_renault@brown.edu}{eric\_renault@brown.edu}}
    \and 
    Paul Sangrey\thanks{University of Pennsylvania, The Perelman Center for Political Science and
    Economics, 133 South 36th Street, Philadelphia, PA 19104, \href{mailto:paul@sangrey.io}{paul@sangrey.io}}}
    
\title{Inference for Risk Prices using Equity Data}

\date{\today}

\begin{document}

\begin{titlepage}


    \maketitle
    \thispagestyle{empty}
    \addtocounter{page}{-1}

    \begin{abstract} \singlespacing \noindent 
        How risks and returns are traded off is arguably the central object of study in modern asset pricing. 
        Even though the theoretical literature cohesively argues there should be a strong positive correlation
        between expected returns and volatility, empirically measuring it has proven difficult. 
        In addition, the option pricing literature shows that volatility enters as its own risk factor, not just
        as a predictor of returns.
        However, because as there is a negative contemporaneous correlation between the volatility and return
        processes and the return and volatility relationship is inherently nonlinear measuring these risk prices
        have proven delicate. 
        We develop methods to provide  valid inference for the prices of equity and volatility risk using only
        equity data that directly handles this identification problem. 
        We do this by adapting the results in weak identification literature that uses drifting sequences in order
        to be robust to the various parameters' identification strength. 
    \end{abstract}

    \jelcodes{C12, C14, C38,  C58, G12}

    \keywords{identification, robust inference, stochastic volatility, leverage, equity risk premium, volatility
    risk premium, risk price, confidence set, asymptotic size}

\end{titlepage}

\tableofcontents
\clearpage

\phantomsection
\addcontentsline{toc}{section}{Introduction}

Modern finance is all about the risk return trade offs that investors face and how to optimally respond to them. 
In particular, the central question of asset pricing is what drives expected returns.
Standard economic theory predicts you must compensate investors with higher expected returns when they face more
risk.
In other words, we would expect a positive relationship between the mean and volatility of returns.
In \gentextcites{sharpe1964capital,lintner1965security} capital asset pricing model (CAPM) the expected
return varies proportionally with the volatility of the market return.
However, to price option prices well, we need not only a price of equity risk that loads upon the market variance,
but also a price of volatility risk as well \parencite{christoffersen2013capturing}.
Investors care not just about how their returns co-move with the market return but also how they co-move with the
market volatility.

This implies that current volatility will affect expected returns in two ways. 1) Through investors' preferences
over future market returns. 2) Through investors' preferences over future volatility. 
As one might expect distinguishing these two effects is quite difficult. 
The difference between how the two effect risk premia comes through their different nonlinear relationships in the
presence of contemporaneous correlation between the volatility and return processes. 
We adopt the framework of \textcite{khrapov2016affine}, which is a discrete-time exponentially-affine stochastic
volatility model.
They mention a potential identification strategy in that paper. 
We develop this strategy,  characterizing how the identification strength of the risk prices varies over the
parameter space, and show how to perform uniformly valid inference.

To take a step back and more fully develop empirical structure at hand, we consider how to measure the
relationship between market volatility and expected returns if we assume that this relationship is constant and
linear. 
Even in this simple case, unlike the consensus in the theoretical literature, the empirical literature has found
pinning down this relationship quite difficult.
Not only has its magnitude proven difficult to determine, but various estimates even differ in sign,
\parencite{lettau2010measuring}.

The empirical literature, which we examine in more detail in the literature review, has focused on point estimates
of this magnitude. 
However, if individual investors are ambiguity averse as in \textcite{hansen2001robust, jiu2012ambiguity}, they
will care not just about how the representative investor prices volatility but also their uncertainty regarding
this estimate. 
Furthermore, when economists calibrate models, they must know how precisely the data determine these parameters. 
If the need to alter the parameter value slightly in order to make their model perform well, are they bringing in
more restrictions or data to more precisely determine the parameter of interest or are they using a value that the
data tell us is incorrect?

Clearly, as obtaining precise believable point estimates of the price of volatility risk has proven quite
difficult, we should expect doing valid inference to be even more delicate.
To the best of our knowledge, this is the first paper to directly tackle this question.
Various authors report confidence intervals as well as their point estimates.
However, they do not take into account the weak identification that makes getting the point estimate difficult in
computing these estimates.

Why is it that measuring this price is so difficult when the theoretical literature is so cohesive?
Econometrically, it is because the volatility price is weakly identified, as in \textcite{andrews2012estimation},
in that the strength of the identification of the price depends upon the value of other parameters. 
This obviously begs the question --- what are these parameters? 

To estimate the risk prices, there are three different phenomena that must be distinguished.
First, the econometrician must disentangle the volatility feedback effect (leverage) which is a contemporaneous
relationship between the volatility and returns from the risk premium, which is a relationship between volatility
and expected returns. 
It is not a contemporaneous relationship, but rather a predictable one. 

Second, and just as important.
We must separate the equity risk price and the volatility risk price. 
In general, and we will show this below, the equity risk price is strongly identified even in the presence of the
volatility risk price. 
There is a simple linear relationship that we can use between the equity risk price and the expected return.
However, volatility risk being another priced factor will introduce nonlinear nuisance terms into this regression. 
They will not matter asymptotically, but in finite samples, they likely do.

Returning to identifying the price of volatility risk, we know that when the strength of identification varies over
the parameter space and we lack identification entirely for some parameter values, the finite sample distributions
are highly nonstandard. 
Consequently, the usual asymptotic approximations do not perform well. 

In addition, the finite-sample distributions are the relevant ones here. 
Even though we often have thousands of observations, since the variation in the expected return is so much smaller
than variation in the return itself, we have a very low signal-to-noise ratio.
Consequently, our estimators will continue to behave as if they were taken from a \textquote{small} sample even in
\textquote{large} samples.

The obvious next step is considering how we need to do this in practice.
Since the contribution of this paper is in terms of methodology and empirical results, we will take a model from
the literature that has the various components, instead of developing our own pricing model.
In particular, we take the model from \textcite{khrapov2016affine} and use it to estimate the relevant parameters. 

This model has a few nice features. 
First, it has both equity and volatility prices and a leverage effect. 
As such, it is the natural discrete-time analogue of the \textcite{heston1993closedform} option pricing model. 
It has an exponentially affine stochastic discount factor  and shares with \textcite{heston1993closedform} the
advantage of having a structure preserving change of measure between the physical and risk-neutral models.
By doing our analysis in discrete-time we are able to more directly compare our results to risk-premia estimates
outside of the option pricing literature and the jumps in high-frequency innovations will not dramatically affect
our results.  
If we were to use a diffusion process in continuous time, we would be severely counterfactually constraining the
higher-order  moments of the process in way that would likely bias our inference. 

As far as estimation is concerned, we derive a series of conditional means and variances.
We then take these means and variances and plug them into a general method of moments (GMM) criterion.
The data we use are the bivariate series $\begin{pmatrix} r_{t+1}, \sigma^2_{t+1} \end{pmatrix}$.
$r_{t+1}$ is the daily return on some asset, and we use its associated realized volatility for $\sigma^2_{t+1}$.
We go into further detail in \cref{sec:data} regarding how we obtained it, the time-span covered, and so on.

\section{Literature Review}\label{sec:lit_review}


\section{The Model}\label{sec:model}

\addtocounter{subsection}{1}

We estimate the prices of some factors using moment conditions derived from a pricing model. 
As is standard in that literature, we will do  this by specifying the physical and risk-neutral measures and their
relationship, i.e.\@ the stochastic discount factor or pricing kernel.
This is non-trivial because we only observe equity data and so can only use moments with respect to the physical
measure to estimate the parameters. 
However, as will be seen in detail below the risk prices govern the stochastic discount factor (SDF), not the
physical measure directly. 
Consequently, we need to relate the physical and risk-neutral measures through SDF closely in order to get
restrictions on the behavior of the physical measure in terms of the risk prices. 

Let $\F_t$ be the representative investor's information set at time $t$, and $P_t$ be the price on the asset in
question, with associated return $r_{t+1}$ and volatility  $\sigma^2_{t+1}$.
Given $\F_{t}$, the vector $\left( r_{t+1},  \sigma^2_{t+1}\right)$ is drawn from some process --- $\PP$ --- the
physical measure. 
We can further define the risk neutral measure --- $\QQ$ ---  as the process that makes $P_t$ a martingale.
The advantage of defining $\QQ$ is that for some function --- $f$ -- of the future excess return --- $r_{t+1}$  --
and volatility $\sigma^2_{t+1}$ and potentially the current information available -- $\F_t$, we can price this
payoff as its expectation with respect to $\QQ$.
In other  words, the price of $f(r_{t+1}, \sigma^2_{t+1}, \F_t)$ satisfies \cref{eqn:risk_neutral_measure_defn}
for all $t$ and for all $f$.  
This is useful because we can choose $f$ to make our estimation convenient.

\begin{equation}
    P_t(f) = \E_{\QQ}\left[ f\left(r_{t+1}, \sigma^2_{t+1}, \F_{t}\right)  \mvert \F_{t}\right]
    \label{eqn:risk_neutral_measure_defn}
\end{equation}

Since $P_t(f), r_{t+1}$ and $\sigma_t^2$ are observable, if we specify a model for $\F_t$ in terms of observable
(to the econometrician) variables, this provides a moment condition that we can use. 
However, this condition does not identify everything we wish to estimate, in particular it does not identify the
risk prices because $\QQ$ is not observable from equity data.

To resolve this we complete the model by defining the stochastic discount factor --- $M_{t, t+1}$ --- as the
Radon-Nikodym derivative between the $\PP$ and $\QQ$ measures. 
No arbitrage guarantees that this will exist, \parencite{harrison1978martingales}.
Since risk prices arise from investors' demand for compensation to hold risk, the risk price show up here 
in the $\QQ$ measure. 
(We collect the parameter of interest into a vector --- $\omega$.)


\begin{defn}{Asset Pricing Moments}
    \begin{equation}
        P_t(f)  = \E_{\QQ} \left[f\left(r_{t+1}, \sigma^2_{t+1} , \F_t\right) \mvert \F_t \right] =
        \E\left[M_{t,t+1}(\omega)f\left(r_{t+1}, \sigma^2_{t+1}, \F_t\right) \mvert \F_t \right] 
    \end{equation}
\end{defn}

We specify the model by parameterizing the $\PP(\omega)$ and $M_{t, t+1}(\omega)$
We then solve for the $\QQ$ more in terms of these moments.
We start by specifying the $\PP$ measure.

Following \textcite{khrapov2016affine}, we assume that the variables are first-order Markov and there is no
Granger-causality from return to the volatility and that returns are serially independent given the volatility
path.
In other words, the volatility drives all of the dynamics of the process.
Note, we do allow $\sigma^2_{t+1}$ and $r_{t+1}$ to be contemporaneously correlated, which they are in the data. 

We construct the following conditional Laplace transforms as follows.
This is well-defined because we can define the true Laplace transform as the expectation of a known function,
and then integrate out all of the unknown variables using their marginal distributions.
Hence, we can represent out model under the physical measure as follows for some functions $a, b,
\alpha, \beta$, and $\gamma$ for all $v$ in its domain.


\begin{restatable}[The Limited Information Model under the Physical Measure]{defn}{physicalMeasureModel}
    \label{defn:physical_model}
    \begin{align}
        \E \left[\exp(-v \sigma^2_{t+1}) \mvert \sigma^2_t \right] &= \exp\left( - a(v) 
        \sigma^2_t - b(v) \right) \\
        \E\left[\exp(-v r_{t+1}) \mvert \sigma^2_t,  \sigma^2_{t+1}\right] &= \exp\left(- \alpha(v)
        \sigma^2_{t+1} - \beta(v)\sigma^2_t - \gamma(v) \right) 
    \end{align}
\end{restatable}


The difficult part moving forward is that we are identifying risk prices, which are  parameters of the stochastic
discount factor (SDF), but since we only observe equity data, we only observe the physical measure.
Consequently, we need to solve for the risk-prices as a function of the parameters governing the physical measure
an then invert this mapping.
In other words, we have some structural parameters --- the risk prices --- and we need to relate them to the
reduced form parameters --- the physical measure parameters.
We will do this by parameterizing the physical measure and the stochastic discount factor in terms of the first
few moments.
If higher moments, such as skewness and kurtosis are also priced factors, as in \textcites{harvey2000conditional,
conrad2012exante, chang2013market},  and we used higher sample moments as well to determine the price of our
risk-factors our resulting estimates would be biased, likely substantially so. 

As discussed in the introduction, this identification strategy is rather fragile.
Consequently, we need to take this weak identification into account when we construct the confidence sets.
We will do this by first deriving the joint distributions of the return and volatility in terms of the physical
measure parameters.
This is not trivial because we only modeled the conditional distribution of the $\left. r_{t+1} \mvert \sigma^2_{t},
\sigma^2_{t+1} \right.$  and $\sigma^2_{t+1}$ is not known at time $t$, and so we need to solve for it.


\subsection{Parameterizing the Physical Measure Dynamics}

We now introduce the data generating process for the volatility.
We use a conditional autoregressive gamma process as in \textcite{gourieroux2006autoregressive, khrapov2016affine}
for the volatility.
This implies that we can parameterize the process using the $a$ and $b$ defined as follows.

\begin{defn}{Volatility Dynamics Functions}
    \label{defn:physical_vol_dynamics}
    \begin{align}
        \label{defn:a_PP}
        a(v) &= \frac{\rho v}{1 + c v} \\
        \label{defn:b_PP}
        b(v) &= \delta \log(1 + c v)
    \end{align}
    
    with 
    \begin{equation}
        \rho \in [0,1), \quad c > 0, \quad \delta > 0
    \end{equation}

\end{defn}

$\rho$ is a persistence parameter.
$c$ is a scaling parameter.
We can see this this clearly from the following forms of the moment conditions.

\begin{remark}[Volatilty Moment Conditions] 
    \label{remark:vol_moment_conditions}
    \begin{align}
        \E\left[\sigma^2_{t+1} \mvert \sigma^2_t \right]  &= \rho \sigma^2_t  + c \delta\\
%
        \Var\left[\sigma^2_{t+1} \mvert \sigma^2_t \right]  &=  2 c \rho \sigma^2_t  + c^2
        \delta 
%
    \end{align}
\end{remark}

Since these two moment conditions are sufficient to derive the unconditional moments, all of the parameters are
identified as long as they are in the interior of their appropriately specified domains.
Intuitively, we are using linear regression to estimate the slope and intercept parameters.


\subsubsection{Return Dynamics}

We then turn to computing the moments of the return distribution. 
This is more subtle than computing the moments of the volatility dynamics because we have to relate the dynamics
of the returns to that of the volatility and to the stochastic discount factor, which is not observed. 
We use the conditional autoregressive CAR(1) model here, which we take from
\textcite{darolles2006structural,khrapov2016affine}
This model  specifies the conditional Laplace transform of the return as a function of $\left. r_{t+1} \mvert
\sigma^2_{t+1}, \sigma^2_t \right.$

We start by specifying a parametric form for $\alpha(v)$ and $\beta(v)$, and $\gamma(v)$.
We parameterize $\alpha(v)$ in the way that we do because it gives $\phi$ a nice interpretation.
It is a leverage effect parameter.

\begin{defn}{Return Dynamics Functions}
    \label{defn:physical_return_dynamics}
    \begin{align}
        \alpha(v) &\coloneqq \psi v + \frac{1 - \phi^2}{2} v^2 \\
        \beta(v) &\coloneqq \beta v  \\
        \gamma(v) &\coloneqq \gamma v  
    \end{align}
\end{defn}

Since $\alpha$ is quadratic, the return are conditionally Gaussian.
In addition, as these functions parameterize the Laplace transform, they equal zero at at $v=0$.
It might seem as first, that imposing linearity for $\beta(v)$ and $\gamma(v)$ is a strong restriction.
This is not actually the case.
Since we assumed that $\sigma^2_{t+1}$ is an integrated volatility and hence greater than $\Var\left(r_{t+1}
\mvert \sigma^2_{t+1}, \sigma^2_{t}\right)$ and variances must be positive, the model implies that they must be
linear, (\cref{lemma:linearity_of_physical_functions}).


Again, we start by computing the moments of the conditional distributions of $\left. r_{t+1} \mvert
    \sigma^2_{t+1}, \sigma^2_t \right.$.
As we did above, we do this by evaluating the derivative the log-cumulant function at zero.

\begin{align}
    \E\left[r_{t+1} \mvert \sigma^2_t, \sigma^2_{t+1}\right]  &= \psi \sigma^2_{t+1}  + \beta \sigma^2_t +
    \gamma \\
    \Var\left[r_{t+1} \mvert \sigma^2_t, \sigma^2_{t+1}\right]  &= (1 - \phi^2) \sigma^2_{t+1}  
    \label{eqn:rtn_cond_vol}
\end{align}


\subsection{The Stochastic Discount Factor}\label{sec:deriving_sdf_functions}

We now introduce the stochastic discount factor in order to relate the physical moments above to moments in terms
of the risk-prices.
Since the Laplace transform for $r_{t+1}$  is exponentially affine under both $\PP$ and $\QQ$, the measure change
between them is also exponentially affine.
If we let $\pi$ be the price of volatility risk and $\theta$ be the price of equity risk, we can write down the
change of measure as follows.

\begin{defn}{The Stochastic Discount Factor}
    \label{defn:SDF}
    \begin{equation}
        M_{t,t+1}(\pi, \theta) = \exp\left(m_{0}(\pi, \theta) + m_1(\pi, \theta) \sigma_t^2 - \pi \sigma^2_{t+1} -
        \theta r_{t+1}\right) 
    \end{equation}
\end{defn}


Before we show how the risk prices $\pi, \theta$ show up in the equations above, we will introduce some parameters
that govern the dynamics of the volatility process.
$M_{t,t+1}$ must integrate to $1$ for all $\sigma^2_t$.
We can view $m_{0}$ and $m_1$ as integration constants.
By plugging in the Laplace transforms for $\sigma^2_{t+1}$ and $r_{t+1}$ we can derive the following.

\begin{restatable}[Uniform Convergence under Strong Identification]{lemma}{sdfConstants}
    Let the SDF be given as in \cref{defn:SDF}, and the model be paramterized as in
    \cref{defn:physical_vol_dynamics} and \cref{defn:physical_return_dynamics}.

    Then the SDF constants follow the following equations.\footnote{This is Equation $3.4$ in
    \textcite[3.4]{khrapov2016affine}.}

    \begin{align}
        \label{eqn:sdf_functions_vs_physical_functions}
        m_0(\theta, \pi)  &= \gamma(\theta) + b(\alpha(\theta) + \pi) \\
        m_1(\theta, \pi)  &= \beta(\theta) + a(\alpha(\theta) + \pi) \nonumber
    \end{align}

\end{restatable}


Since the right-hand side of \cref{eqn:sdf_functions_vs_physical_functions} are entirely in terms of the physical
measure functions, which are in principle observable, we can estimate the SDF functions entirely using equity
data.
In other words, we do not need, at least in principle, option data to estimate the SDF.
In particular, we can determine the risk prices from equity prices if the identified parts of the physical measure
functions ($\alpha$, $\beta$, etc.) are \textquote{sufficiently} invertible.
(I.e.\@ the matrix of their derivatives satisfies the appropriate non-singularity conditions.)

\subsection{Relating the Physical Measure Functions and the Risk Prices}

\subsubsection{Deriving \texorpdfstring{$\psi$}{psi}}

The goal moving forward is to solve for $\gamma$, $\beta$, and $\psi$ in terms of the reduced-form parameters and
risk prices. 
Once we do this, we can use the difference in expected returns between the distribution given $\sigma^2_t$ and the
distribution given $\sigma^2_{t+1}$ and $\sigma^2_t$ to separately identify the two risk prices.
In this section, we focus on $\psi$ because it controls this difference in means and hence controls the strength
of identification.

In general, $\psi$ will have three parts.
First, we have the Jensen's inequality term --- the mean will shift by a value proportional to the variance.
Second, the reduction in variance will shift the price as consumers are risk averse.
Third, since $\sigma^2_{t+1}$ and $r_{t+1}$ are correlated, (the leverage effect) the drift will change directly. 

Intuitively, if volatility and returns are uncorrelated, we cannot disentangle the shift in the mean because the
price of volatility risk from the shift in the mean induced by price of equity risk.
They both show up in the same way in $\psi$.
However, if we have a leverage effect, when we condition on $\sigma^2_{t+1}$ two different components of the
return are moving -- the mean and variance.
Since we now have two moments, we can identify both parameters.

The discussion in the previous paragraph was at very high level, and so we now turn to the details.
The goal is to separate out the three components.
The way we do this is by constructing a pseudo-return --- $\widetilde{r}_{t+1}$ --- whose mean is not affected by
the change in variance caused by adding $\sigma^2_{t+1}$ to the information set.

We do this by exploiting the fact that $\E\left[M_{t,t+1} \exp(r_{t+1}) \mvert \sigma^2_t \right] = 0$.
We want to relate $\E\left[\exp(r_{t+1}) \mvert \sigma^2_{t+1}, \sigma^2_t \right]$  to this expression.
Clearly, there are two differences between these expressions.
First, we have the SDF in the first expression.
Second, the conditioning information differs.
We can view these differences as two measure changes.
Since prices here are conditionally log-Gaussian, the measure change  can be paramterized in terms of the
covariance between the log SDF --- $m_{t,t+1} \coloneqq \log M_{t,t+1}$ --- and the return.

If we apply the logarithm to \cref{defn:SDF}, we get the following expression for $m_{t,t+1}$.

\begin{equation}
    \label{eqn:log_sdf}
        m_{t,t+1}(\pi, \theta) = m_{0}(\pi, \theta) + m_1(\pi, \theta) \sigma_t^2 - \pi \sigma^2_{t+1} - \theta
        r_{t+1}
\end{equation}

All of the terms on the right are constant given $\sigma^2_t, \sigma^2_{t+1}$ except for $\theta r_{t+1}$, and so
they do not affect the conditional covariance between $m_{t,t+1}$ and $r_{t+1}$.

\begin{equation}
    \Cov\left(m_{t,t+1}, r_{t+1} \mvert \sigma^2_{t+1}, \sigma^2_t \right)   
%
    = \Cov\left(-\theta r_{t+1}, r_{t+1} \mvert \sigma^2_{t+1}, \sigma^2_t \right)   
%
    = -\theta \Var\left(r_{t+1} \mvert \sigma^2_{t+1}\right)   
\end{equation}

Once we have plugged in the covariance, we can simplify slightly, deriving the following expression.

\begin{equation}
    \label{eqn:return_covarinace}
    \Cov\left(m_{t,t+1}, r_{t+1} \mvert \sigma^2_{t+1}, \sigma^2_t \right)   
%
    = -\theta (1 - \phi^2) \sigma^2_{t+1} 
\end{equation}

The expression in \cref{eqn:return_covarinace} is the change in the mean driven by investors' risk aversion, and
is controlled by their equity risk price.
To see this, note it equals zero if $\theta = 0$.

The second term we need is the Jensen's effect term.
The mean of a conditionally log-Gaussian price depends on both the mean and variance of the underlying return.
It has the standard form: minus one-half the conditional variance $\left(-\frac{1 - \phi^2}{2}
\sigma^2_{t+1}\right)$. 

Having done this we can define a pseudo-return that only changes by an amount proportional to the leverage
effect, and is not affected by the changes in the return arising from the reduction in risk.
Intuitively, we know that $\E\left[r_{t+1} \mvert \sigma^2_t, \sigma^2_{t+1} \right] = \psi \sigma^2_{t+1}$
plus some function  of $\sigma^2_t$.
However, this $\psi$ contains all three effects.
We want a pseudo-return $\widetilde{r}_{t+1}$ where $\E\left[\widetilde{r}_{t+1} \mvert \sigma^2_t,
\sigma^2_{t+1} \right] = \widetilde{\psi} \sigma^2_{t+1}$
Once we have this we only have to worry about the change in the mean, and can solve for $\tilde{\psi}$.

\begin{restatable}[Separating the Leverage Effect from the Measure Changes]{lemma}{leverageVersusMeasureChange}

    Let $\tilde{r}_{t+1} \coloneqq r_{t+1} - \frac{1 - \phi^2}{2} \sigma^2_{t+1} - (1 - \phi^2) \theta
    \sigma^2_{t+1}$, and let the log SDF have the form given by \cref{eqn:log_sdf}.
    
    Then 
    
    \begin{equation}
        \E\left[\Var\left[\tilde{r}_{t+1} \mvert \sigma^2_t, \sigma^2_{t+1}\right] \mvert \sigma^2_t\right]   =
        \E\left[\sigma^2_{t+1} \mvert \sigma^2_t\right]
    \end{equation}
\end{restatable}

This is not the only way we can compute $\Var\left[r_{t+1} \mvert \sigma^2_t \right]$.
We can also use the law of total variance.

\begin{equation}
    \Var\left[\widetilde{r}_{t+1} \mvert \sigma^2_t\right]  =
    \E\left[\Var\left[\widetilde{r}_{t+1} \mvert \sigma^2_{t+1}\right] \mvert \sigma_t^2 \right] +
    \Var\left[\E\left[\widetilde{r}_{t+1}\mvert \sigma_{t+1}^2\right] \mvert \sigma^2_t\right]
\end{equation}

Since $\sigma^2_{t+1}$ is constant given $\sigma^2_{t+1}, \sigma^2_t$, the conditional variance of
$\widetilde{r}_{t+1}$ is the same as $r_{t+1}$.
We can now fill in the values for both of the inside variables on the right-hand side and
\cref{eqn:return_var_versus_expected_vol_pp} on the left-hand side and taking unconditional expectations of all of
the variables.

\begin{equation}
    \E\left[\sigma^2_{t+1} \right]  = \E[ \E\left[(1 - \phi^2) \sigma^2_{t+1} \right]] +
    \E\left[\Var\left[\widetilde{\psi} \sigma^2_{t+1} \mvert \sigma^2_t\right]\right] 
\end{equation}

Solving for $\widetilde{\psi}$ gives the following.

\begin{equation}
    \label{eqn:psi_tilde_eqn}
    \widetilde{\psi} = \phi \sqrt{\frac{\E\left[\sigma^2_{t+1} \right]}{\E
    \left[\Var\left(\sigma^2_{t+1} \mvert \sigma^2_t\right)\right]}}  
\end{equation}

We can substitute back in the definition of $\tilde{\psi}$ in terms of $\psi$ and derive the moments in
\cref{eqn:psi_tilde_eqn} from \cref{remark:vol_moment_conditions} through some elementary calculations. 

\begin{equation}
    \label{eqn:psi_pp_as_func_of_params}
    \psi = \phi \sqrt{\frac{c \delta / (1 - \rho)}{2 c \rho \left(c \delta /
    (1 - \rho)\right) + c^2 \delta}} - \frac{1- \phi^2}{2} - (1 - \phi^2) \theta = \frac{\phi}{\sqrt{c
    (1 + \rho)}} - \frac{1 - \phi^2}{2}  + (1 - \phi^2) \theta
\end{equation}

Now that we have a formula for $\psi$, we can derive the formulas for $\beta$ and $\gamma$ in terms of the
other reduced-form parameters and the risk prices. 
I use subscript $\QQ$ notation to refer the risk-neutral analogues of the parameters.

\begin{restatable}[Reparamaterizing the Physical Distribution]{lemma}{physicalMeasureFunctions}

    
    \label{lemma:reparamterizing_physical_dist}
    Let the SDF be given as in \cref{defn:SDF}, and the model be paramterized as in
    \cref{defn:physical_vol_dynamics} and \cref{defn:physical_return_dynamics}, and assume that the risk-neutral
    measures have distributions with the same parametric forms.
    Let $\mu(\pi, \theta) \coloneqq -\psi - (1-\phi^2)\left(\theta - \frac{1}{2}\right)$, and $\chi(\pi, \theta)
    \coloneqq 1 + c (1+ \alpha(\theta))$. 
    Then the following three equations hold.


    \begin{align}
        \label{eqn:psi_function}
        \psi(\pi, \theta) &= \frac{\phi}{\sqrt{c \cdot (1 + \rho)}} + \frac{1 - \phi^2}{2} - (1 - \phi^2)
        \theta  \\
%
        \label{eqn:beta_function}
        \beta(\pi, \theta) &\coloneqq \frac{\rho_{Q} \cdot \mu(\pi, \theta)}{1 + c_{\QQ} \cdot \mu(\pi, \theta)} =
        \frac{\rho}{\chi(\pi, \theta)} \cdot \frac{\mu}{\chi\left(\pi, \theta\right) + c \cdot \mu(\pi,
        \theta)}  \\
    %
        \label{eqn:gamma_function}
        \gamma(\pi, \theta) &\coloneqq \delta \log\left(1 + c_{\QQ} \cdot \mu(\pi, \theta)\right) = \delta
        \log\left(1 + \frac{c}{\chi(\pi,\theta)} \cdot \mu(\pi,\theta) \right) 
    \end{align}

\end{restatable}

As can be seen in \cref{lemma:reparamterizing_physical_dist}, $\psi$ has the form we derived above.
$\beta(\pi, \theta)$ and $\gamma(\pi, \theta$ have the form of $\beta(v)$ and $\gamma(v)$, but where we use the
risk-neutral versions and evaluate them at $\mu(\pi, \theta)$.

\subsection{Identification of the Risk Prices}

The final goal is to identify the risk-prices $\theta$ and $\pi$.
In the previous sections, we have derived a series of moment conditions in terms of the parameters.
We now need to analyze when these moment conditions allow to identify the risk prices. 
The information that return data contain about equity pricing data is entirely encapsulated by the asset pricing
equation for excess returns.  
In what follows, we will use $rx_{t+1}$ as the excess log-return.
The definition of $M_{t,t+1}$ as a change of measure means that the following holds.

\begin{equation}
    \E\left[ M_{t,t+1}(\theta, \pi) \exp(r_{t+1}) \mvert \F_{t} \right] = 1
\end{equation}

We now characterize, the information in this set of moment conditions regarding the risk prices.
The difficult part is identifying the volatility risk price --- $\pi$, and so we will focus first on the
information regarding that parameter.
We start by substituting in the SDF formula from above, and we also replace all of the dependence on $\F_t$ with
$\sigma^2_t$.

%Where exactly should I mention that I focusing on pi?

\begin{gather}
    \E \left[ \exps*{ - \pi \sigma^2_{t+1} - (\theta - 1) rx_{t+1} } \mvert \sigma^2_t \right]
        = \exps*{- m_0(\theta, \pi) - m_1(\theta, \pi) \sigma^2_t}
%
    \intertext{Similar to above, we used the law of iterated expectations to substitute in the conditional Laplace
        transforms of $r_{t+1}$ and $\sigma^2_{t+1}$.}
%
    \E\left[\exps*{- a\left(\pi + \alpha(\theta -1)\right) \sigma^2_t - b(\pi) -
    \beta(\theta-1) \sigma^2_t - \gamma(\theta-1)} \mvert \sigma^2_t \right] = \exps*{- m_0(\theta,
    \pi) - m_1(\theta, \pi) \sigma^2_t} 
\end{gather}

We now match the coefficients of the $\F_t$-measurable variables in the above moment condition. 
We also substitute in the formulas for $m_0(\theta, \pi)$ and $m_1(\theta, \pi)$ that we derived in
\cref{sec:deriving_sdf_functions}, and rearrange to solve for $\gamma$ and $\beta$.

\begin{align}
   \label{eqn:identification_eqn_1}
   \gamma  &= b(\pi + \alpha(\theta - 1) -  b(\pi + \alpha(\theta))  \\
%
    \label{eqn:identification_eqn_2}
    \beta &=   a(\pi + \alpha(\theta -1)) - a(\pi + \alpha(\theta)) 
\end{align}

We can characterize the identification restrictions in \cref{eqn:identification_eqn_1} and
\cref{eqn:identification_eqn_2} in two different cases.\footnote{These equations are Equation 3.7 in
\textcite{khrapov2016affine}.}

\begin{enumerate}
    \item[Case 1:] The price of equity risk $\theta$ satisfies the following equations. 
        \begin{equation}
            \alpha(\theta - 1) = \alpha(\theta)
            \label{eqn:lack_of_id_condition}
        \end{equation}

        If \cref{eqn:lack_of_id_condition} holds, then some simple algebra shows that $\gamma(\theta) =
        \gamma(\theta-1)$ and $\beta(\theta) = \beta(\theta-1)$.
        In this situation, we can satisfy these \cref{eqn:identification_eqn_1} and
        \cref{eqn:identification_eqn_2}, regardless of the value of $\pi$.
        In other words, the asset pricing equation does not identify $\pi$. 
        As noted by \textcite{khrapov2016affine}, this is in line with the common belief that the econometrician
        needs options data to be able to identify the price of volatility risk. 

    \item[Case 2:] 
        In general, there is no reason to expect the \cref{eqn:lack_of_id_condition} to hold.
        If it does not, it might seem reasonable to expect that we should be able to identify both $\theta$ and
        $\pi$.
        In other words, we should, in principle, at least be able to identify $\pi$ from the difference between the
        functions in the previous case when evaluated at $\theta-1$ and $\theta$.
\end{enumerate}

We now show that if $\phi = 0$, then \cref{eqn:lack_of_id_condition} will hold, but it will fail to hold if $\phi
\neq 0$.
In other words, as mentioned in \textcite[13]{khrapov2016affine}, a leverage effect will allow us to separately
identify $\theta$ and $\pi$.
To see why this is the case note the following.

\begin{align}
    \alpha(\theta) - \alpha(\theta - 1) &= \alpha'(0)  + \alpha''(0) \left(\theta - \frac{1}{2}\right) \\
%
    &=  \psi + (1 - \phi^2) \left(\theta - \frac{1}{2}\right)  \\
%
    \intertext{Then filling in the definition of $\psi$ from \cref{eqn:psi_pp_as_func_of_params}.}
%
    &= \frac{\phi}{\sqrt{c (1 + \rho )}} + \frac{1 - \phi^2}{2} - (1 - \phi^2) \theta +  (1 - \phi^2)
       \left(\theta - \frac{1}{2}\right) 
%
    \intertext{Simplifying we have the following.}
%
    \label{eqn:alpha_difference}
    &= \frac{\phi}{\sqrt{c (1 + \rho )}} 
\end{align}

In other words, the leverage effect leads to a wedge between $\alpha(\theta)$ and $\alpha(\theta-1)$ proprotional
to $\frac{\phi}{\sqrt{c (1 + \rho )}}$.
Clearly, this equals zero if and only if $\phi = 0$.
If $\phi \neq 0$,on the other hand, the derivatives of \cref{eqn:identification_eqn_1} and
\cref{eqn:identification_eqn_2}  do not equal at $\theta$ and $\theta-1$ since $a$ and $b$ are not linear
functions.
Consequently, we can separately identify $\pi$ and $\theta$.

\section{GMM}\label{sec:GMM}

To estimate the model we use the various moment conditions that we have derived.
The one that we have yet to derive is the ones implied by the continuous-time model that allow us to estimate
$\phi$ and $\psi$.
Deriving them is straightforward because we constructed what the discrete-time equations, and they are linear.

\subsection{Moment Conditions}\label{sec:moment_conditions}

First, we define two functions that we use to reparameterize the moment conditions.
We do this because the model creates some cross-equation equations to eliminate two of the redundant parameters.
By doing this we are able to avoid the econometric complications that we would have to handle in the general
case.
Note, we are only specifying values for the return conditional on both $\sigma^2_{t+1}$ and $\sigma^2_t$ because
the other moments can be derived from them and the volatility moments.
Also, it is worth noting that we consider \cref{eqn:beta_function}, \cref{eqn:gamma_function}, and
\cref{eqn:psi_function} as implicitly defining those parameters as functions of the other parameters.
\cref{eqn:cond_expected_rtn_moment} can be derived from those equation and the first derivative of the conditional
log-cumulant function equaling the conditional mean.


\begin{defn}{Equilibrium Moment Conditions}
    \label{defn:equilibrium_moment_conditions}
    \begin{align}
        \label{eqn:cond_vol_mean}
        \E\left[\sigma^2_{t+1} \mvert \sigma^2_t \right]  &= \rho \sigma^2_t  + c \delta\\
%
        \label{eqn:cond_vol_var}
        \Var\left[\sigma^2_{t+1} \mvert \sigma^2_t \right]  &=  2 c \rho \sigma^2_t  + c^2 \delta \\
%
        \label{eqn:cond_expected_rtn_moment}
        \E\left[r_{t+1} \mvert \sigma^2_t, \sigma^2_{t+1}\right] &= \gamma(\pi, \theta) + \beta(\pi, \theta)
        \sigma^2_t + \psi(\pi, \theta) \sigma^2_{t+1} \\
%
        \label{eqn:cond_rtn_var}
        \Var\left[r_{t+1} \mvert \sigma^2_t, \sigma^2_{t+1}\right] &= (1 - \phi^2) \sigma^2_{t+1} 
\end{align}
\end{defn}

\subsection{Instruments}\label{sec:instruments}

To fully specify the GMM conditions, we need to also specify the instruments we are using.
Doing this correctly is somewhat subtle because we need to only use $\F_{t-1}$-measurable variables in order to
satisfy the necessary exogeneity conditions, while providing instruments for both $\sigma^2_t$ and
$\sigma^2_{t+1}$. 
The natural instrument to use in both cases is $\sigma^2_t$.
This comes from the first-order Markov assumption that we made.
However, we need more than one instrument in order to be identified.
The way around this is to notice that although we assumed first-order Markov, we did not assume that the
transition was linear. 

To see this more fully, note that the volatility risk price is being identified through the part of $\psi$
above and beyond a Jensen inequality effect.
Consequently, we use lags of the volatility and powers of those lags as instruments.\footnote{The parametric model
we are using is determined entirely by its means and variances which are linear in $\sigma^2_t$ and
$\sigma^2_{t+1}$, and so the instruments given in \cref{defn:instruments} span the relevant information set and
hence are optimal.} 

\begin{defn}{Instruments}
    \label{defn:instruments}
    \begin{equation}
        Z_t \coloneqq 1, \sigma^2_{t+1}, \sigma^4_{t+1}, \sigma^2_{t}, \sigma^4_{t}
    \end{equation}
\end{defn}

\subsection{Identified Sett}\label{sec:identified_set}

Having constructed the moments and the instruments, we can use GMM to estimate the parameters.
We do need to run a constricted optimization though, because only certain values of the parameters are valid. 
Having done that we characterize the set where the GMM criterion strongly identifies the parameters.

\begin{restatable}[Identified Set]{lemma}{identifiedSet}

    Assume that the moment conditions specified in \cref{defn:equilibrium_moment_conditions} have the correct form
    and that the instruments we are using satisfy the standard exogeneity and relevant conditions. 

    Let the true parameter vector $\omega \coloneqq (\rho, c, \delta, \phi, \theta, \pi) \in [-1+\epsilon_1,
    1 - \epsilon_2] \times [M_1, M_2] \times [\epsilon_4, M_4]\times [M_5, M_6]\times \times [-1 +
    \epsilon_4, 1 - \epsilon_5] \times [M_7, M_8] \times [M_12, M_13]$, where the $M_{\ast}$ are some large (in
    magnitude) known constants and the $\epsilon_{\ast}$ are some small positive constants.  

    Let $Q_T(\omega, X)$  be the GMM objective function with moment conditions given in
    \cref{defn:equilibrium_moment_conditions} and instruments given in  \cref{defn:instruments}.

    If there exists a $\epsilon$ such that $\abs{\phi} > \epsilon > 0$, then all of the parameters are identified. 
    If $\phi = 0$, then the objective function is independent of $\pi$. 
    Hence, $\pi$ is not identified but all of the other parameters are still identified.

\end{restatable}



\section{Weak Identification Setup}

In this section, take the model described in the previous sections and place it in the setup of
\textcite{andrews2014Gmm} so that we ca analyze the effects of possible lack of identification in the model in a
nice clean way.
The goal here is to perform valid inference for $\pi, \theta$ even when $\phi$ might be zero. 


From the discussion above, we can collect the parameters discussed above into a parameter vector of the following
form,i.e.\@ recall the following: $\omega = \lbrace \rho, c, \delta, \phi, \pi, \theta \rbrace$
To write it in the notation of \textcite{andrews2014Gmm}, we partition $\omega$ into three subsets.

\begin{align}
    \phi &\coloneqq \phi  \in (-1, 1) \\ 
    \zeta &\coloneqq \lbrace \rho, c, \delta, \theta \rbrace \in [0,1) \times \R_{++} \times \R_{++} \times
    \R  \\
    \pi &\coloneqq \pi \in \R
\end{align}

Let $\omega$ be the set of possible $\omega$, that as defined above.
It is worth noting that the parameter space has a product form, i.e.\@ the values do not affect the valid values
of the other parameters.

In this environment, $\pi$ is not identified when $\phi = 0$.
Both $\phi$ and $\zeta$ are always identified, and $\zeta$ does not affect the identification of $\pi$.

Let $Q_T(\omega)$ be the GMM criterion function, then the GMM estimator $\hat{\omega}_T$ satisfies the following.


\begin{equation}
    \widehat{\omega}_T \in \omega\ \text{and}\ Q_T(\hat{\omega}_T) = \inf_{\omega \in \Omega} Q_T(\omega) +
    o\left(T^{-1}\right) 
\end{equation}


Now that we have defined the parameters, we can characterize the set of assumptions necessary for valid inference.
We will work through the assumptions described in \textcite{andrews2014Gmm}.
The set of necessary assumptions is relatively complicated because we have to characterize the asymptotic
distribution under several different estimation strengths simultaneously, and the assumptions required to do that
  differ in the various cases. 
In what follows, we will use 

The first assumption specifies the basic identification
problem. It also provides conditions that are used to determine the
probability limit of the GMM estimator, when it exists, under all categories
of drifting sequences of distributions.
Let $\xi$ index the part of the distribution of the data $r_{t+1}, \sigma^2_{t+1}$ that is not determined by the
moment equations.
In general, it is a (likely infinite-dimensional) nuisance parameter that affects the distribution of the data. 


We collect the parameters that we are estimating $\omega$ and the nuisance parameter $\xi$ into one parameter,
$\gamma$ and associated parameter space $\Gamma$.
In the previous discussion we characterized the parameter spaces in a non-compact fashion, let $\omega^{*}$ be a
compact subset of $\omega$, where the true parameter values live.

\begin{defn}{Complete Parameter Space}
    \begin{equation}
        \Gamma \coloneqq \left\lbrace \gamma = (\omega, \xi) \mvert \omega \in \Omega, \xi \in \Xi \right\rbrace 
    \end{equation}
\end{defn}

We characterize these drifting sequences of distributions by sequences of true parameters $\gamma_T \coloneqq
(\omega_T, \phi_T)$.

\purple{TODO Add discussion of the limiting process.}
\purple{Verify that the assumptions on the parameter space hold.}
\purple{Discuss what happens if we lack identification and hence cannot consistently estimate the parameter.}


\begin{restatable}[Inference for $\omega$ under Weak Identification]{theorem}{InferenceWeakID}
    Let that $\phi_0  \in \left(\underline{\phi}_0,1\right)$, for some $\underline{\phi}_0 > -1$. 
    $\rho_0 \in \left[0,1\right)$, and $c_0 > 0$. 

    \purple{TODO  Add Conclusion}
\end{restatable}


\section{Simulations}

\section{Data}\label{sec:data}

\section{Empirical Results}

\section{Conclusion}

\clearpage

\phantomsection
\addcontentsline{toc}{section}{References}
\printbibliography
\clearpage

\begin{appendices}


\section{Model Characterization}

\begin{lemma}[Linearity of $\beta$ and $\gamma$]
    \label{lemma:linearity_of_physical_functions}
    Letting $\sigma^2_{t+1}$ be the integrated volatility of a process with return $r_{t+1}$.
    Assume that $\sigma^2_{t+1}$ and $r_{t+1}$ follow a bivariate CAR(1) process parametrized as in
    \cref{defn:physical_vol_dynamics} and \cref{defn:physical_return_dynamics}. 
    Then $\beta''(0)$ and $\gamma''(0)$ both equal zero.
\end{lemma}

\begin{proof}
    By the \Ito\ Isometry, and the definition of $r_{t+1}$ as an integrated variance, the following holds for the
    returns' predictable information set $\F^r_{t-}$.  

    \begin{equation}
        \Var\left(r_{tau+1}\mvert \F_{tau-}\right) \leq \E\left(r^2_{tau+1} \mvert \F_{tau-}\right) 
        = \E\left(\sigma^2_{t+1}\mvert \F_{\tau-}\right)
    \end{equation}

    The integrated volatlity is predictable, and so $\sigma^2_{t+1}$ is contained in the return's predictable
    $\sigma$-algebra. 

    Consequently, $\Var\left(r_{t+1} \mvert \sigma^2_t, \sigma^2_{t+1}\right) \leq \sigma^2_{t+1}$
    In addition, variance must always be positive, and so $\Var\left(r_{t+1} \mvert \sigma^2_t,
    \sigma^2_{t+1}\right) \geq 0$.

    Since the second derivative of the log-cumulant funciton evaluted at zero equals the variance, we have the
    following set of inequalities.

    \begin{gather}
        0 \leq (1 - \phi^2) \sigma^2_{t+1} - \beta''(0) \sigma^2_t - \gamma''(0) \leq
        \sigma^2_{t+1} 
%
        \intertext{Dividing through by $\sigma^2_{t+1}$ and pulling the first term outside}
%
        \label{eqn:second_derivative_inequalities}
        \implies \phi^2 - 1 \leq -\frac{1}{\sigma^2_{t+1}} \left(\beta''(0)  \sigma^2_t +
        \gamma''(0)\right) \leq \phi^2 
%
    \end{gather}

    On the outside of the two inequalities we have constants, and the distribution of $\sigma^2_{t+1}$ given
    $\sigma^2_t$ is not bounded away from zero.
    Consequently, the only way for \cref{eqn:second_derivative_inequalities} to hold for all $\sigma^2_{t+1}$ is
    if the term inside the parantheses equals  zero.

    \begin{equation}
        0 = \beta''(0) \sigma^2_t + \gamma''(0)
    \end{equation}

    However, the only way for this to hold is for both $\gamma''(0)$ and $\beta''(0)$ to equal zero.
    This plus the condtional gaussianity of the returns implied by the paramtric model implies that $\gamma$
    and $\beta$  are both linear.

\end{proof}


\sdfConstants*

\begin{proof}

\begin{equation}
    \label{eqn:reweigted_sdf}
    \E\left[\exp\left(m_0(\theta, \pi) + m_1(\theta, \pi) \sigma^2_t - \pi \sigma^2_{t+1} - \theta r_{t+1}
    \right) \mvert \F_t \right] = 1 
\end{equation}

We can use \cref{eqn:reweigted_sdf} to relate $m_0(\theta, \pi)$ and $m_1(\theta, \pi)$ to the physical measure
functions. 

\begin{gather}
    \E\left[\exp\left(m_0(\theta, \pi) + m_1(\theta, \pi) \sigma^2_t - \pi \sigma^2_{t+1} - \theta r_{t+1}
    \right) \mvert \F_t \right] = 1 \\
    \intertext{By the law of iterated expectations.}
    \E\left[ \E\left[\exp\left(m_0(\theta, \pi) + m_1(\theta, \pi) \sigma^2_t - \pi \sigma^2_{t+1}\right)
        \exp\left( - \theta r_{t+1}\right) \mvert \F_t, \sigma^2_{t+1} \right]\right] = 1 \\
    \intertext{The second term is the Laplace transform of $r_{t+1}$.}
    \E\left[\exp\left(m_0(\theta, \pi) + m_1(\theta, \pi) \sigma^2_t - \pi \sigma^2_{t+1} \right)
        \exp(-\alpha(\theta) \sigma^2_{t+1} - \beta(\theta) \sigma^2_{t} - \gamma(\theta_2)
        \mvert \F_t \right] = 1 \\
    \intertext{Reorganizing terms.}
    \E\left[\exp\left(m_0(\theta, \pi) + m_1(\theta, \pi) \sigma^2_t - \beta(\theta) \sigma^2_{t} -
        \gamma(\theta_2) \right) \exp(-\left(\pi + \alpha(\theta)\right) \sigma^2_{t+1}) \mvert \F_t
        \right] = 1 \\ 
    \intertext{Substituting in the Laplace transform for $\sigma^2_{t+1}$.} 
    \label{eqn:expected_sdf_wrt_PP}
    \E\left[\exp(m_0(\theta, \pi) + m_1(\theta, \pi) \sigma^2_t - \beta(\theta) \sigma^2_{t} -
        \gamma(\theta_2)  - a(\pi + \alpha(\theta)) - b(\pi + \alpha(\theta))
        \mvert \F_t \right] = 1 
\end{gather}

\end{proof}

\leverageVersusMeasureChange*


\begin{proof}

\begin{align}
    &\phantom{=} \log \E\left[\widetilde{r}_{t+1} \mvert \sigma^2_t, \sigma^2_{t+1} \right]
%
    \intertext{We can expand $\widetilde{r}_{t+1}$.}
%
    &= \log \E\left[r_{t+1} - \frac{1 - \phi^2}{2} \sigma^2_{t+1} - (1 - \phi^2) \theta \sigma^2_{t+1}
       \mvert \sigma^2_t, \sigma^2_{t+1} \right]
%
    \intertext{The first term variance term is the appropriate Jensen convexity correction, and the second term is
    measurable with respect to the conditioning information.}
%
    &= \log \E\left[\exp(r_{t+1})  \exp\left(- (1 - \phi^2) \theta \sigma^2_{t+1}\right) \mvert \sigma^2_t,
       \sigma^2_{t+1} \right]
%
    \intertext{We can divide through by $\exp(r_t)$  and $m_{t-1,t}$ because they are contained within the
    conditioning set, and the remainder of the SDF is also contained within the information set, and so we can add
it  as well (\cref{eqn:log_sdf}).}
%
    &\propto \log \E\left[\frac{\exp(r_{t+1})}{\exp(r_t)} \frac{M_{t,t+1}}{M_{t-1,t}} \mvert \sigma^2_t,
      \sigma^2_{t+1} \right]
%
    \intertext{This is the expectation of the change in the price discounted by the change in the SDF, and so it
    equals zero after we take logs.}
%
    &= 0
\end{align}


Because the mean of $\tilde{r}_{t+1}$ does not change in expectation when we condition on $\sigma^2_{t+1}$ we can
apply the \Ito\ Isometry.

\begin{equation}
    \label{eqn:return_var_versus_expected_vol_pp}
    \E\left[\Var\left(\tilde{r}_{t+1} \mvert \sigma^2_t\right)  \right] = \E[ \sigma^2_{t+1}]
\end{equation}

Intuitively, volatilities are squared returns, and so they are variances.
The tricky part here is that variances are centered second moments, not the second moments themselves.
The volatilities are also centered second moments, but the centering is not the same in general.
In continuous time, they would only differ by a drift term, which can be ignored, which is why the Ito\ Isometry
usually is used in that context.
Using discrete-time return, as we do here, we first have to appropriately recenter the variables, which is why it
applies to $\widetilde{r}_{t+1}$ but not to $r_{t+1}$.

\end{proof}



\physicalMeasureFunctions*

\begin{proof}

The conditions in \cref{eqn:sdf_functions_vs_physical_functions} are not yet usable in estimation because they
relate a series of unknown functions.
To simplify these equations we impose some restrictions that must hold in the risk-neutral model. 
We know that risk-neutral expected returns must equal zero.
Consequently, any expected returns in the physical measure must be caused by the change of measure.

As we did above, we will use conditional Laplace transforms to parameterize the distributions.
In this case, the Laplace functions maintain the same structure for some functions $a_{\QQ}, b_{\QQ},
\alpha_{\QQ}, \beta_{\QQ}$, and $\gamma_{\QQ}$.

\begin{restatable}[The Limited Information Model under the Risk-Neutral Measure]{defn}{riskNeutralMeasureModel}
    \label{defn:risk_neutral_model}
    \begin{align}
        \E_{\QQ} \left[\exp(-u \sigma^2_{t+1}) \mvert \sigma^2_t \right] &= \exp\left( - a_{\QQ}(u) \sigma^2_t -
        b_{\QQ}(u) \right) \\
        \E_{\QQ}\left[\exp(-u r_{t+1}) \mvert \sigma^2_t,  \sigma^2_{t+1}\right] &= \exp\left(- \alpha_{\QQ}(u)
        \sigma^2_{t+1} - \beta_{\QQ}(u)\sigma^2_t - \gamma_{\QQ}(u) \right) 
    \end{align}
\end{restatable}


Since the change of measure preserves the structure, the parametric forms and moments from above also hold in the
risk-neutral measure.
We will use the same notation, but use subscript $\QQ$ to differentiate the two measure. 
For example, $\psi_{\QQ} \coloneqq \alpha_{\QQ}'(0)$ and $\phi_{\QQ} = \sqrt{1 - \abs*{\alpha''(0)}}$.
We start by characterizing $\beta_{\QQ}$ and $\gamma_{\QQ}$.
Before we do that, it is worth noting that as is standard in these models, the measures' absolute continuity
implies $\phi = \phi_{\QQ}$, \parencite[17]{khrapov2016affine}.
Since we have a risk-neutral distribution the only change between $\E_{\QQ}\left[\exp(r_{t+1}) \mvert
\sigma^2_t\right]$ and $\E{\QQ}\left[\exp(r_{t+1}) \mvert \sigma^2_{t+1}, \sigma^2_t\right]$ comes from the
increased in information.

We characterize $a_{\QQ}$ and $b_{\QQ}$ by deriving the restrictions that risk-neutrality implies upon the
risk-neutral measure functions.
Risk-neutrality means that the expectation of $\exp(r_{t+1}) = 1$.

\begin{align}
    1 &= \E_{\QQ}\left[\E_{\QQ}\left[\exp(r_{t+1}) \mvert \sigma_t^2, \sigma^2_{t+1}\right]  \mvert \sigma^2_t
        \right] \\
%
    \intertext{This is the conditional Laplace transform of $r_{t+1}$ evaluated at $-1$.}
%
      \label{eqn:risk_neutral_at_neg_1}
    &= \E_{\QQ}\left[ \exp\left(-\alpha_{\QQ}(-1) \sigma^2_{t+1}\right)  \mvert \sigma^2_t \right]
       \exp\left(-\beta_{\QQ}(-1) \sigma^2_t - \gamma_{\QQ}(-1)\right)   \\
%
    \intertext{The first term is the Laplace transform of $\sigma^2_{t+1}$ evaluated at $\alpha(-1)$.}
%
      &= \exp(-a_{\QQ}(\alpha_{\QQ}(-1)) \sigma^2_t - b_{\QQ}(\alpha_{\QQ}(-1))) \exp\left(-\beta_{\QQ}(-1)
         \sigma^2_t - \gamma_{\QQ}(-1)\right)  
\end{align}

Since this equation must hold for all $\sigma_t^2$ and $\exp(0) = 1$, we have the following two equations.

\begin{align}
    a_{\QQ}(\alpha_{\QQ}(-1)) = - \beta_{\QQ}(-1) \\
    b_{\QQ}(\alpha_{\QQ}(-1)) = - \gamma_{\QQ}(-1) 
\end{align}


The linearity of $\beta$ and $\gamma$ as shown by \cref{lemma:linearity_of_physical_functions} implies that the
physical and risk-neutral forms of these functions coincide \textcite[Proposition 5]{khrapov2016affine}. 
Hence we can substitute in the physical forms for $\beta_{\QQ}$ and $\beta$.

\begin{gather}
    \label{eqn:beta_qq_at_minus_one}
    \phantom{\implies}   - \beta(-1) = a_{\QQ}(\alpha_{\QQ}(-1))\\
%
    \intertext{Filling in the formula for $\alpha_{\QQ}$ and recalling that $\phi_{\QQ} = \phi$ gives.}
%
    \implies \beta'(0) = a_{\QQ}\left(-\psi_{\QQ} + \frac{1}{2} (1- \phi^2)\right)  \\
%
    \intertext{Then we can fill in the formula for $a_{\QQ}$ from \cref{defn:a_PP} because the change of measure
    preserves the structure.}
%
    \implies \beta'(0) = \frac{\rho_{\QQ} \left(-\psi_{\QQ} + \frac{1}{2} (1- \phi^2)\right)}{1 + c_{\QQ}
    \left(-\psi_{\QQ} + \frac{1}{2} (1- \phi^2)\right)} 
    \label{eqn:beta_function_QQ}
\end{gather}

Before we derive $\rho_{\QQ}$, $c_{\QQ}$, and $\psi_{\QQ}$ in terms of the physical measure functions, we perform
a similar calculation for $\gamma_{\QQ}(-1)$.

\begin{equation}
    \label{eqn:gamma_qq_at_minus_one}
    \gamma'(0) = b_{\QQ}(\alpha_{\QQ}(-1))  = b_{\QQ}\left(-\psi_{\QQ} + \frac{1}{2} (1- \phi^2)\right)  
\end{equation}

We can fill in the formula for $b_{\QQ}$ from \cref{defn:b_PP} because that the measure change is structure
preserving.

\begin{equation}
    \implies \gamma'(0) = \delta \log\left(1 + c_{\QQ} \left(-\psi_{\QQ} + \frac{1}{2} (1-
    \phi^2)\right)\right)
    \label{eqn:gamma_function_QQ}
\end{equation}

To simplify \cref{eqn:beta_function} and \cref{eqn:gamma_function} further, we need to solve for $c_{\QQ}$,
$b_{\QQ}$, and $\psi_{\QQ}$  in terms of their physical counterparts.
We can take the equations in \textcite[Proposition 5]{khrapov2016affine} and differentiate and derive the
following relationship between $a_{\QQ}, b_{\QQ}$ and $a, b$.

\begin{align}
    a_{\QQ}'(v) &= a'(v + \pi + \alpha(\theta))  \nonumber \\
%
    b_{\QQ}'(v) &= b'(v + \pi + \alpha(\theta))  \\
%
    b_{\QQ}''(v) &= b''(v + \pi + \alpha(\theta))  \nonumber \\
%
    \intertext{Filling in parameterization of the right-hand sides, and evaluating the equations at $0$ gives the
    following.} 
%
    a_{\QQ}'(0) &= \frac{\rho}{1 + c(\pi + \alpha(\theta_2))} \nonumber \\
%
    b_{\QQ}'(0) &= \frac{c \delta}{1 + c(\pi + \alpha(\theta_2))} \\
%
    b_{\QQ}''(0) &= -\frac{c^2 \delta}{(1 + c(\pi + \alpha(\theta_2)))^2} \nonumber \\
%
    \intertext{Then since the left hand sides of the equation above are $\rho_{\QQ}$ and $c_{\QQ} \delta$, we
    have the following expressions.}
%
    \rho_{\QQ} &= \frac{\rho}{1 + c(\pi + \alpha(\theta_2))} \nonumber \\
%
    \label{eqn:c_rho_delta_QQ}
    c_{\QQ} \delta_{\QQ} &= \frac{c \delta}{1 + c(\pi + \alpha(\theta_2))}  \\
%
    -c_{\QQ}^2 \delta_{\QQ} &= -\frac{c^2 \delta}{(1 + c(\pi + \alpha(\theta_2)))^2}
    \nonumber
\end{align}

To simplify notation, define the following rescaling function.

\begin{equation}
    \chi(\pi, \theta) \coloneqq 1 + c(\pi + \alpha(\theta))
\end{equation}

We can use \cref{eqn:c_rho_delta_QQ} and substitute in $\chi(\pi, \theta)$ to get the following.

\begin{align}
    \delta_{\QQ} &= \delta \\
    \rho_{\QQ} &= \frac{\rho}{\chi^2(\pi, \theta)} \\
    c_{\QQ} &= \frac{c}{\chi(\pi, \theta)} 
\end{align}

The one value we have yet to derive is $\psi_{\QQ}$.
Again we use \textcite[Proposition 5]{khrapov2016affine}, to relate $\psi$ and $\psi_{\QQ}$. 
Since the $\alpha$ functions are quadratic, we have the following.

\begin{align}
    \alpha_{\QQ}(v) = \psi_{\QQ} v + \frac{1- \phi^2_{\QQ}}{2} v^2  \\
%
    \alpha(v) = \psi v + \frac{1- \phi^2}{2} v^2  
\end{align}

As in \cref{eqn:risk_neutral_at_neg_1}, $\alpha_{\QQ}(-1) = \alpha(\theta)$, recalling that $\phi =
\phi_{\QQ}$, we can differentiate and get the following.

\begin{align}
    \implies \alpha_{\QQ}'(0) = \alpha'(0) + (1 - \phi^2) \theta
%
    \implies \psi_{\QQ} = \psi + (1 - \phi^2) \theta
\end{align}

\end{proof}

\section{Identification Proofs}


\identifiedSet*

\begin{proof}
    Since $Q_{T}$ is a quadratic in terms of deviations between sample and population moment conditions as long as
    the population moment can be inverted to solve for the parameters, the $Q_T$ process identifies them as well. 
    In addition, since we have a sufficient number of exogenous valid instruments the conditioning implied by
    projecting on the instruments does not affect the arguments above. 

    Identifying the four parameters that govern the volatility dynamics is not particularly complicated. 
    We have four parameters and four non-redundant moment conditions.
    The first two equations in    \cref{defn:equilibrium_moment_conditions} identify $\rho$ and $c \delta$.
    \cref{eqn:cond_vol_var} identifies $\rho c$ and $c^2 \delta$. 
    This allows us to separately identify $c$ and $\delta$.
    
    Identifying $\phi$ is also relatively straightforward. Since $r_{t+1}$ and $\sigma^2_{t+1}$ are known, as long
    as we know the conditional mean of $r_{t+1}$, then identifying $\phi$ is identified by \cref{eqn:cond_rtn_var}.
    Identifying the conditional mean of $r_{t+1}$ is straightforward because we observe volatility and the
    conditional mean is a linear equation in these variables. 
    

    Identifying the risk prices $\pi$ and $\theta$ is more complicated.
    We have to identify both parameters off of \cref{eqn:cond_expected_rtn_moment}. 
    This is in principle possible because we now have two non-redundant sources of variation in the data ---
    $\sigma^2_t$ and $\sigma^2_{t+1}$.

    The only place that \cref{defn:equilibrium_moment_conditions} that the risk-prices occurs in
    \cref{eqn:cond_expected_rtn_moment}. 
    We showed that $\gamma, \beta$ and $\psi$ functions are independent of $\pi$ if $\phi = 0$ in the discussion
    leading up \cref{eqn:alpha_difference}.
    We further showed that they are not independent if $\phi \neq 0$.
    In addition the dependence of the identification in terms of the non-singularity of the derivatives
    equilibrium conditions in terms of $\pi$ and $\theta$ depends smoothly on $\phi$. 
    This will be important later.
    
\end{proof}

\section{Inference Assumptions}

    In what follows, three sets of drifting sequences $\lbrace \gamma_T \rbrace$ are key. 
    
    \begin{defn}{Drifting Sequence Parameter Spaces}
        \begin{align}
            \Gamma\left(\gamma_0\right) &\coloneqq \left\lbrace \left\lbrace \gamma_T \in \Gamma \right\rbrace
            \mvert \gamma_T \to \gamma_0 \in \Gamma \right\rbrace\\ 
            \Gamma(\gamma_0, 0, b) &\coloneqq \left\lbrace \lbrace \gamma_T \rbrace \in \Gamma(\gamma_0) \mvert
            \phi_0 = 0\ \text{and}\ \sqrt{T} \phi_T \to b \in (\R \cup \lbrace \pm \infty) \right\rbrace \\
            \Gamma(\gamma_0, \infty, b_0) &\coloneqq \left\lbrace \lbrace \gamma_T \rbrace \in \Gamma (\gamma_0)
            \mvert \sqrt{T} \norm{\phi_T} \to \infty\ \text{and}\ \frac{\phi_T}{\norm{\phi_T}} \to b_0
            \right\rbrace 
        \end{align}
    \end{defn}
    
    These are the standard GMM regularity conditions appropriately adjusted for the lack of identification when
    $\phi =0$.
    
    \begin{assump}[GMM 1]\label{ass:GMM1}
    \begin{assumplist}
        \item If $\phi_0=0$, $\sampmom(\omega)$ and $\W_{T}(\omega)$ do not depend on $\pi$ for all $\omega \in \Omega$,
            for all $T \geq 1$, and for all $\gamma^{*}\in \Gamma.$ 
            \label{ass:GMM1a}
        \item If $\lbrace \gamma_{T} \rbrace \in \Gamma\left(\gamma_0\right)$, $\sup_{\omega \in \Omega}
            \norm*{\sampmom(\omega) - \E\left[g\left(\omega \mvert \gamma_0\right)\right]} \pto 0$ and $\sup_{\omega
            \in \omega} \norm{\W_{T}(\omega)-\E\left[\W\left(\omega \mvert \gamma_0\right)\right]} \pto 0$.
            \label{ass:GMM1b}
        \item When $\phi_0 = 0$,  $g_0\left(\phi, \zeta ,\pi \mvert \gamma_0\right) = 0$ if and only if $\phi
            =\phi_0$ and $\zeta = \zeta_0$ for all $\pi \in \Pi$ and for all $\gamma_0 \in \Gamma.$
            \label{ass:GMM1c}
        \item When $\phi_0 \neq 0$, $g_0\left(\omega \mvert \gamma_0\right)=0$ if and only if $\omega =\omega_0$ for all
            $\gamma_0 \in \Gamma.$
            \label{ass:GMM1d}
        \item  $g_0\left(\omega \mvert \gamma_0\right)$ is continuously differentiable in $\omega $ on $\omega$ with
            partial derivatives with respect to $\omega$ and $\xi$ denoted by $g_{\omega}\left(\theta \mvert
            \gamma_0\right) \in R^{k\times d_{\omega }}$ and $g_{\xi }\left(\omega \mvert \gamma_0\right)\in R^{k\times
            d_{\xi }}$, respectively.
            \label{ass:GMM1e}
        \item $\W\left(\omega \mvert \gamma_0\right)$ is continuous in $\omega$ on $\omega$ for all $\gamma_0\in
            \Gamma$.  \label{ass:GMM1f}
        \item $0 < \lambda_{\min}(\W\left(\xi_0, \pi \mvert \gamma_0\right))\leq \lambda_{\max }(\W\left(\xi_0,\pi
            \mvert \gamma_0\right)) < \infty$, $\forall \pi \in \Pi$, for all $\gamma_0 \in \Gamma$.
            \label{ass:GMM1g}
        \item $\lambda_{\min} (g_{\xi}\left(\xi_0,\pi \mvert \gamma_0\right)^{\prime} \W\left(\xi_0,\pi \mvert
            \gamma_0\right)g_{\xi }\left(\xi_0,\pi \mvert \gamma_0\right))>0$, for all $\pi \in \Pi$,  and for all 
            $\gamma_0 \in \Gamma$ with $\phi_0=0.$
            \label{ass:GMM1h}
        \item$\Xi(\pi)$ is compact for all $\pi \in \Pi$, and both $\Pi$ and $\omega$ are compact.
            \label{ass:GMM1i}
        \item For all $\epsilon > 0$, there exits a $\delta > 0$ such that $d_{H}\left(\Xi \left(\pi_{1}\right),
            \Xi \left( \pi_{2}\right) \right) < \epsilon$ for $\pi_{1}, \pi_{2} \in \Pi$ with
            $\norm*{\pi_{1}-\pi_{2}} < \delta$, where $d_{H}\left( \cdot \right)$ is the Hausdorff metric.
            \label{ass:GMM1j}
    \end{assumplist}
    \end{assump}
    
    
    
    \begin{assump}[GMM 2*]\label{ass:GMM2}
    \begin{assumplist}
        \item $\sampmom(\omega)$ is continuously differentiable in $\omega$ for all $T \geq 1$. 
            \label{ass:GMM2a}
        \item If $\{\gamma_T\} \in \Gamma\left(\gamma_0, 0, b\right)$, $\sup_{\left\lbrace \omega \in \Omega \mvert
            \norm*{(\phi, \zeta')' - (\phi_T, \zeta_0')} \leq \delta_T \right\rbrace}
            \norm*{\frac{\partial}{\partial (\phi, \zeta')'} \sampmom(\omega) - \E\left[\popmom_{(\phi,
            \zeta')'}(\omega) \mvert \gamma_0\right]} = o_p(1)$ for all deterministic sequences  $\delta_T \to 0$.
            \label{ass:GMM2b}
        \item  Let $\omega_T \coloneqq \left\lbrace \omega \in \Omega \mvert \norm*{(\phi, \zeta_) - (\phi_T, \zeta_T)}
            \leq \delta_T \norm*{\beta_T}\, \text{and}\, \norm*{\pi - \pi_T} \leq \delta_T \right\rbrace$.  Let
            $\delta_T$ be a deterministic sequence that converges to zero.  If $\{\gamma_T \} \in
            \Gamma\left(\gamma_0, \infty, b_0\right)$, then we have the following asymptotic behavior.
            $\sup_{\omega \in \Omega_T} \norm*{\left(\frac{\partial}{\partial \omega'} \overline{g}_T -
            \E\left[g_{\omega}(\omega) \mvert \gamma_0\right]\right) \diag\left(1_{1+d_\zeta}',
            (1/\phi_T)_{d_{\pi}}'\right)}  = o_p(1)$. 
            \label{ass:GMM2c}
    \end{assumplist}
    \end{assump}
    
    Once we have \nameref{ass:GMM1} and \nameref{ass:GMM2}, we use \nameref{ass:GMM3} to derive the asymptotic
    distribution under weak and semi-strong identification.
    These conditions will be characterized using the expected derivative of the population moment conditions. 
    
    \begin{defn}
        \label{defn:moment_derivative_func}
        \begin{equation}
            K_{T,g}\left(\omega \mvert \gamma^{*}\right) \coloneqq  \frac{1}{T} \sum_{i=1}^T \frac{\partial}{\partial
            \phi^{*}} \E \left[ \popmom(W_T, \omega) \mvert \gamma^{*} \right]
        \end{equation}
    \end{defn}
    
    
    \begin{assump}[GMM 3]\label{ass:GMM3}
    \begin{assumplist}
        \item $\sampmom(\omega) = \frac{1}{T} \sum_{i=1}^T \popmom(W_T, \omega)$  for some function $\popmom(W_T,
            \omega) : \R^{k \times k} \times \omega \to \R^k$.
            \label{ass:GMM3a}
        \item $\E\left[\popmom(W_T, \beta_0, \zeta^{*}, \pi) \mvert \gamma^{*} \right] = 0$ for all $\pi \in \Pi$ and
            for all $i \geq 1$ if $\gamma^{*} = \left(0,\zeta^{*}, \pi^{*}, \xi^{*} \right) \in \Gamma$.
            \label{ass:GMM3b}
        \item If $\{ \gamma_T \} \in \Gamma(\gamma_0, 0, b)$, $\frac{1}{\sqrt{T}} \sum_{i=1}^T \left(g(W_T,
            \zeta_{0,T}, \pi_T) - \E \left[g(W_T, \zeta_{0,T}, \pi_T)\mvert \gamma_T \right]\right)  \dto \N\left(0,
            \aleph(\gamma_0)\right)$, where $\aleph(\gamma_0)$ is a $k \times k$ matrix.
            \label{ass:GMM3c}
        \item 
            \label{ass:GMM3d}
            \begin{enumerate}
                \item  $K_{T,g}\left(\omega \mvert \gamma^{*}\right)$ exists for all $\{\omega, \gamma^{*} \} \in
                    \left(\omega_{\delta} \times \Gamma_{0}\right)$ and for all $T \geq 1$.
                \item $K_{T,g}\left(\phi_T, \zeta_T, \pi \mvert \widetilde{\gamma}_T\right)$ uniformly converges
                    to some non-stochastic matrix-valued function  $K_{g}\left(0, \zeta_0, \pi \mvert
                    \gamma_0\right)$ over $\pi \in \Pi$ for all deterministic sequences $\{\phi_T, \zeta_T,
                    \widetilde{\gamma}_T \}$ satisfying $\widetilde{\gamma}_T \in \Gamma$, $\widetilde{\gamma}_T
                    \to \gamma_0 \coloneqq (0, \zeta_0, \pi_0, \xi)$, $\{\phi_T, \zeta_T, \pi \} \in \omega$ and
                    $\{\phi_T, \zeta_T \} \to (0, \zeta_0)$.
                \item $K_g\left(\phi_0, \zeta_0, \pi \mvert \gamma_0\right)$ is continuous on $\Pi$ for all
                    $\gamma_0 \in \Gamma$ with $\phi_0 = 0$.
            \end{enumerate}
            \item $K\left(\phi_0, \zeta_0, \pi \mvert \gamma_0\right) = \popmom_{\phi, \zeta}\left(\phi_0,
                \pi\mvert \gamma_0\right) x$ for some $x \in \R^{1+d_{\zeta}}$ if and only $\pi =
                \pi_0$.\footnote{Since $\dim(\phi) = 1$, we can assume without loss of generality that the
                $\omega_0$ from \textcite{andrews2014Gmm} equals $1$.}
                \label{ass:GMM3e}
            \item If $\{ \gamma_T \} \in \Gamma(\gamma_0, 0, b)$, $\frac{1}{T} \sum_{i=1}^T
                \frac{\partial}{\partial \omega}  \E\left[ \popmom\left(W_T, \omega_T \right ) \mvert \gamma_T \right]
                \to \popmom_{\omega}\left(\omega_0 \mvert \gamma_0\right)$.
            \label{ass:GMM3f}
    \end{assumplist}
    \end{assump}

    \begin{defn}{\popmom*}
        \begin{equation}
            g_{\phi, \zeta}^{*}\left(\phi_0, \zeta_0, \pi_1, \pi_2 \mvert \gamma_0\right)  =
            \left[g_{\phi}\left(\phi_0, \zeta_0, \pi_1 \mvert \gamma_0\right)  , g_{\phi}\left(\phi_0, \zeta_0,
            \pi_2 \mvert \gamma_0\right) , g_{\zeta} \left(\phi_0, \zeta_0 \mvert \gamma_0\right)  \right]  \in
            \R^{k \times (d_{\zeta} + 2)}
        \end{equation}
    \end{defn}


    \begin{assump}[GMM 4]\label{ass:GMM4}
    \begin{assumplist}
        \item $\phi$ is a scalar.
            \label{ass:GMM4a}
        \item $g_{\phi, \zeta}^{*}\left(\phi_0, \zeta_0, \pi_1, \pi_2 \mvert \gamma_0\right)$ has full column
            rank. 
            \label{ass:GMM4b}
        \item $\aleph(\gamma_0)$ is positive definite for all $\gamma_0 \in \gamma $ with $\phi_0 = 0$. 
            \label{ass:GMM4c}
    \end{assumplist}
    \end{assump}

\section{Inference Proofs}


\begin{restatable}[Uniform Convergence under Strong Identification]{lemma}{UllnStrongID}
    \label{lemma:UniformConvergenceStrongID}
    Let $\omega$ be the identified set.
    Further assume that $\phi_0 \neq 0$. 
    Let $\sampmom$ be the sample moment condition defined above, and $\W_T$ be the associated optimal weight matrix
    estimator.
    Then we have the following convergence.

    \begin{align}
        &\sup_{\omega \in \Omega} \norm*{\sampmom(\omega) - \E\left[g\left(\omega \mvert
          \gamma_0\right)\right]}_{Fro} \pto 0 \\ 
        &\sup_{\omega \in \Omega} \norm{\W_{T}(\omega)-\E\left[\W\left(\omega \mvert \gamma_0\right)\right]} \pto 
    \end{align}

\end{restatable}

\begin{proof}

    In this proof we rely heavily on the continuity of the moment conditions over their domain. 
    This can be seen from simple inspection since we assumed that $\phi_0 \geq \underline{\phi} -1$.
    Furthermore since $\omega$ is compact, this continuity implies uniform continuity.
    
    For any positive definite weight-matrix by \textcite[Lemma 2.3]{newey1994large} our criterion function has
    a unique optimum.
    The data, $\sigma^2_{t+1}, r_{t+1}$, are ergodic and stationary.
    Since the moment conditions are not redundant the optimal (GMM) weight matrix $\W$ is positive definite. 
    In addition, $\popmom$ is continuous at each $\omega$, given the restrictions above and properties of
    characteristic functions imply that $\popmom$ is uniformly bounded. 
    For convenience, we assume that the space of $\omega$ is compact.
    This should not be an issue here because the parameters  are either a priori bounded, such as $\phi$ or we
    have substantial a priori knowledge on their plausible magnitudes.
    Hence, \textcite[Theroem 2.6]{newey1994large} implies our estimator is consistent.
    
    However, when we allow for weak identification late on, we need this convergence to be uniform. 
    One straightforward way to show this is to show that our criterion function is globally Lipschitz in a set of
    high probability. 
    
    The other issue is that we need the weight matrix to converge uniformly to its expectation.
    Since the moments are continuous functions over their domain as is the square function.
    This convergence is uniform if and only if the matrix inverse is continuous.
    
    Since we have a finite number of non-redundant moments, the minimum eigenvalue, \\
    $\lambda_{min}\left(\W\left(\phi_0, \zeta_0, \pi \mvert \gamma_0\right)\right) > 0$, and so the matrix inverse
    is uniformly continuous in $\gamma_0$ with respect to the Frobenius norm, which is the sum of the eigenvalues.
    (Recall, that the eigenvalues of the inverse are the inverse of the eigenvalues.)


\end{proof}


\begin{assump}[Weak Dependence]
    \label{assumption:weak_dependence}
    $z_t \coloneqq \begin{pmatrix} r_{t+1} \\ \sigma^2_{t+1} \end{pmatrix}$ are $\alpha$-mixing with $\alpha_t =
       O\left(T^{-5}\right)$
\end{assump}


\begin{theorem}[Inference for $\omega$ under Strong Identification]
    Assume that $\phi_0  \in (-1,1) \setminus 0$, $\rho_0 \in [0,1)$, and $c_0 > 0$. 
    Further assume that the data are ergodic, stationary, and satisfy \cref{assumption:weak_dependence}.
    Then the following convergence in distribution holds.

    \begin{equation}
    \sqrt{T} (\widehat{\omega}_T - \omega_0) \dto \N\left(0, \left(G' \E[\W] G\right)^{-1}\right)
    \end{equation}
\end{theorem}

\begin{proof}

    By the above arguments, we have a consistent estimator for $\omega$ and the optimal weight matrix $\W \coloneqq
    (\E\left[g g'\right])^{-1}$, and we will assume that the true value $\omega_{0}$ is in the interior of its
    sample space $\omega$.\footnote{Throughout we will use subscript \num{0}  to denote true values for parameters.}
    Let $G \coloneqq \E\left[\frac{\partial}{\partial \omega} \popmom \right]$ Clearly, $g$ is continuously
    differentiable, and its derivative $G$ is continuous.
    In addition, by the identification discussion $G' W \nabla G$ is nonsingular.
    
    %TODO Replace this with strong (alpha) mixing of order (1+\epsilon). 
    %Don't you need a lot more than this...?

    Since, $\norm*{g_t}$ is almost surely bounded by $1$ it has all of its moments and $z_t$ being $\alpha$-mixing
    implies $g_t$ is as well by the central limit theorem for strongly mixing process $\sqrt{T} \sampmom(\omega^{*})
    \dto \N\left(0, \E\left[\W\right]^{-1}\right)$ as required. 
    Consequently, by \textcite[theorem 3.2]{newey1994large} we have convergence in distribution as well as
    convergence in probability.
    

\end{proof}


\InferenceWeakID*

\begin{proof}
We prove this result by showing that Assumptions GMM 1-4 are satisfied.

\begin{proofpart}
    \label{part:main_theorem_proof_part1}
    In this part, we show that \nameref{ass:GMM1} is satisfied. 
    To do this, we break \nameref{ass:GMM1}  down into three subsections.
    Assumptions \namedref{ass:GMM1a}, \namedref{ass:GMM1b}, \namedref{ass:GMM1c}, and \namedref{ass:GMM1d} state
    that when $\phi = 0$, the moment conditions contain no information regarding $\pi$, but when $\phi \neq 0$,
    the model is identified.
    
    \purple{We need to show the above statement}

    We further showed the relevant uniform convergence to verity \namedref{ass:GMM1b} in
    \cref{lemma:UniformConvergenceStrongID}.
    
    The next two assumptions (\namedref{ass:GMM1e} and \namedref{ass:GMM1f}) are  technical conditions regarding
    the behavior of the moment conditions and weight matrix. 
    Since our moment conditions are derived from an infinitely-differentiable  characteristic function and the
    weight matrix is the optimal one, they both hold trivially.
    
    The third subsection of Assumption \nameref{ass:GMM1} concerns the weight matrix.
    Since we are using the inverse covariance matrix of valid non-redundant model, assumptions
    \namedref{ass:GMM1g} and \namedref{ass:GMM1h} automatically hold.
    
    The last two assumptions, \namedref{ass:GMM1i} and \namedref{ass:GMM1j} require that the parameter spaces do
    not vary too much with the parameters and are compact.
    Since $\omega$ is compact, \namedref{ass:GMM1i} holds trivially, and since it has  has a product form,
    \namedref{ass:GMM1j}  holds trivially as well.
    
\end{proofpart}


\begin{proofpart}
    \label{part:mainTheoremProofPart2}

    In this section, we show that the derivatives of the moment conditions have the correct behavior locally to
    the true parameters.
    We have to do this for the different classes of drifting sequences.
    We will do this by verifitying \cref{ass:GMM2}.
    This is valid since \textcite{andrews2014Gmm} show that this is a sufficient condition for their Assumption
    GMM2, which is what we actually need. 

    Our moment conditions are sample averages of the characteristic function, they satisfy \namedref{ass:GMM2a}
    automatically. 
    Since characteristic functions are uniformly bounded, by the dominated convergence theorem we can interchange the 
    expectation and derivative operators. 
    Hence \namedref{ass:GMM2b} and \namedref{ass:GMM2c} are equivalent to the statements in terms of the moment
    conditions themselves mutatis mutandis.  
    In addition, sice the derivate is a linear operator, we can pull it outside of the norm.
    The reason that the uniform law of large numbers in \cref{part:main_theorem_proof_part1} does not trivially
    imply this result is because we are not considering sequences $\phi_T \to \phi_0$. 


    We create a mean value expansions around around $(\phi_0, \zeta_T, \pi_T)$ of the sample moment condition and
    around $(\phi_0, \zeta_0, \pi_0)$ for the population moment condition.
    (This is not the same in both cases, not is it the true parameter for the drifting sequence in the case of the
    sample moment condition.)
    In addition, also since we are considering continuous functions of compact spaces --- the $\delta_T$ ball in
    $\R^{\dim(\omega)}$ --- pointwise convergence implies uniform convergence, and so we only need to show pointwise
    convergence below.

    \begin{alignat}{2}
        & &&\norm*{\sampmom(\phi_T, \zeta_T, \pi_T) -  \E\left[\popmom(\phi_T, \zeta_T, \pi_T) \mvert
          \gamma_0\right] } \\ 
        \intertext{We take a mean value expansion of both functions around $\omega_0$. The point at which the
        derivative in the two locations is taken may not be the same.}
        &= &&\left\lVert \sampmom(\phi_0, \zeta_0, \pi_0) + \frac{\partial}{\partial (\phi, \zeta,
           \pi)}\sampmom(\widetilde{\phi}^s, \widetilde{\zeta}^s, \widetilde{\pi}^s)\left((\phi_0, \zeta_0, \pi_0)
            - (\phi, \zeta, \pi)\right)\right. \\
        &  &&\quad \left. - \E\left[\popmom(\phi_0, \zeta_0, \pi_0) \mvert \gamma_0\right] + 
           \frac{\partial}{\partial (\phi, \zeta, \pi)} \E\left[\popmom(\widetilde{\phi}^p, \widetilde{\zeta}^p,
           \widetilde{\pi}^p)\mvert \gamma_0\right] \left((\phi_0, \zeta_0, \pi_0) - (\phi, \zeta,
           \pi)\right) \right\rVert \\ 
        \intertext{By the triangle inequality.}
        &\leq && \norm*{\sampmom(\phi_0, \zeta_0, \pi_0) - \E\left[\popmom(\phi_0, \zeta_0, \pi_0) \mvert
           \gamma_0\right]}  \\
        &+  && \norm*{\frac{\partial}{\partial (\phi, \zeta)}\sampmom(\widetilde{\phi}^s, \widetilde{\zeta}^s,
          \widetilde{\pi}^s)\left((\phi_0, \zeta_0)  - (\phi, \zeta)\right) -  \frac{\partial}{\partial (\phi,
          \zeta)} \E\left[\popmom(\widetilde{\phi}^s, \widetilde{\zeta}^p, \widetilde{\pi}^p)\mvert \gamma_0\right]
          \left((\phi_0, \zeta_0) - (\phi, \zeta)\right)} \\
        &+  && \norm*{\frac{\partial}{\partial \pi}\sampmom(\widetilde{\phi}^s, \widetilde{\zeta}^s,
          \widetilde{\pi}^s)\left(\pi_0 - \pi\right) -  \frac{\partial}{\partial \pi}
          \E\left[\popmom(\widetilde{\phi}^p, \widetilde{\zeta}^p, \widetilde{\pi}^p)\mvert \gamma_0\right]
          \left(\pi_0 - \pi\right)} 
          \label{eqn:pi_derivative_term}
    \end{alignat}

    By the uniform law of law numbers in \cref{part:main_theorem_proof_part1}, the first equation is $o_p(1)$.
    For $(\phi_0, \zeta_0) - (\phi, \zeta)$ small, the middle term is bounded by the quantity below. 


    \begin{equation}
        \norm*{\frac{\partial}{\partial (\phi, \zeta)}\sampmom(\widetilde{\phi}^s, \widetilde{\zeta}^s,
        \widetilde{\pi}^s) -  \frac{\partial}{\partial (\phi, \zeta)} \E\left[\popmom(\widetilde{\phi}^s,
        \widetilde{\zeta}^p, \widetilde{\pi}^p)\mvert \gamma_0\right]} \norm*{(\phi_0, \zeta_0) - (\phi, \zeta)}
    \end{equation}

    The first term is almost surely bounded, and the second term is less than $\delta_T$ by assumption, and so the
    product is $o_p(1)$.

    The hard part is the third expression.
    Like before we can bound the pull the $\pi_0 - \pi$ term out of the equation.
    However, this term is no longer converges to zero.
    We will consider the two cases, separately.
    Throughout, we will refer to the behavior of the following equation, which bounds
    \cref{eqn:pi_derivative_term}.


    \begin{equation}
        \norm*{\frac{\partial}{\partial \pi}\sampmom(\widetilde{\phi}^s, \widetilde{\zeta}^s, \widetilde{\pi}^s) -
        \frac{\partial}{\partial \pi} \E\left[\popmom(\widetilde{\phi}^p, \widetilde{\zeta}^p,
        \widetilde{\pi}^p)\mvert \gamma_0\right] } \norm*{\pi_0 - \pi} 
        \label{eqn:pi_derivative_norm_bound}
    \end{equation}

    In general, $\tilde{\pi}^s$ and $\tilde{\pi}^p$ can be arbitrarily far apart.
    However, since $\sampmom \pto \E\left[\popmom \mvert \gamma_0\right]$, and the limiting value is independent of
    $\pi$, the derivative does not depend upon $\pi$ asymptotically by the dominated convergence theorem.
    This applies that the difference between the two derivatives evaluated at different $\pi$ converges to zero.
    Consequently, \cref{eqn:pi_derivative_norm_bound} is $o_p(1)$ and we have shown \namedref{ass:GMM2b}.

    \begin{equation}
        \norm*{\abs*{\frac{\partial}{\partial \pi}\sampmom(\widetilde{\phi}^s, \widetilde{\zeta}^s,
        \widetilde{\pi}^s) - \frac{\partial}{\partial \pi} \E\left[\popmom(\widetilde{\phi}^p,
        \widetilde{\zeta}^p, \widetilde{\pi}^p)\mvert \gamma_0\right]} (1, 1, T)} \norm*{\pi_0 - \pi} 
        \label{eqn:pi_derivative_rescaled_bound}
    \end{equation}

    If we consider the setup in \namedref{ass:GMM2c}, $\tilde{\pi}^s$ and $\tilde{\pi}^p$ are now close together.
    However, we need to show that \cref{eqn:pi_derivative_rescaled_bound} is $o_p(1)$.
    Since we are considering the limiting behavior of a function with a continuous derivative, we can assume that
    the derivative is uniformly bounded without loss of generality. 
    (The constant might depend upon the true value, but not $T$.)
    By a Taylor series expansion of $\sampmom$ around the true value, $\frac{\widetilde{\pi}^p}{\sqrt{T}} \to 0$,
    and $\norm{\pi_0 - \pi} \propto \delta_T$ the result follows.

\end{proofpart}

\begin{proofpart}
    \label{part:mainTheoremProofPart3}

    Assumption \namedref{ass:GMM3a} is trivially satisfied,  and we showed that \namedref{ass:GMM3b} is satisfied
    in \cref{sec:GMM}.  
        
    The conceptual idea driving the reulsts this section is that moment conditions for each $T$ minus their
    conditional exceptions converge to a normal random variable.
    In other words, we are almost in a standard triangular C.L.T.\@ setup with weak time-dependence.

    In particular, both $\sigma^2_t$ and $r_t$ are infinitely differentiable functions of the innovations to
    the volatility and return processes and $\popmom$ are infinitely differentiable functions of $r_{t+1}$ and
    $r_t$ and the innovations are i.i.d.\@ across time by assumption.
    (Note, i.i.d.\@ implies strong mixing of any size.)
    Consequently, $\popmom$ is near epoch dependent (NED) of any size as defined in
    \textcite{andrews1991empirical}.  
    (Take $s>2.$) 
    By \textcite[Theorem 3]{andrews1991empirical}, we have the necessary finite-dimensional convergence in
    distribution to a Gaussian random variable. 

    We now show that \namedref{ass:GMM3d} holds.
    Clearly, $K_{t,g}\left(\omega \mvert \gamma^{*}\right)$ always exists.
    It uniformly converges because the derivatives of the moments are continuous functions of the data and the
    parameters, the process is ergodic, and the characteristic function lives on a compact set.
    It is also clearly continuous.
    By the dominated convergence theorem, we can exchange the derivative and expectations.
    In addition, since \popmom\ does not depend upon $\phi_T$, the limiting behavior is independent of the value
    of $\phi_T$.
    Hence \namedref{ass:GMM1d} holds.
    
    \namedref{ass:GMM3e} says the derivative of the moment function does not depend the true parameter $\phi$ if
    and only if  $\pi =  \pi_0$. 
    We showed this in the proof of \cref{part:mainTheoremProofPart2}.
    \namedref{ass:GMM3f} follows directly from the compactness of the parameter space and the continuity of the
    cross derivatives of \popmom\ by the dominated convergence theorem.

\end{proofpart}

\begin{proofpart}
    \label{mainTheoremProofPart4}
   
    \namedref{ass:GMM4a} holds trivially.

    \purple{This is no longer true.}
    We verified \namedref{ass:GMM4b} when we showed that $\pi$ and $\zeta$ are strongly identified

    \namedref{ass:GMM4c} holds because the identification conditions do not create any singularity in the
    asymptotic covariance matrix. 

\end{proofpart}

\end{proof}





\end{appendices}


\end{document}



\documentclass[11pt, letterpaper, twoside, final]{article}
\usepackage{risk_price_inference}
\addbibresource{riskpriceinference.bib}

\author{Xu Cheng\thanks{University of Pennsylvania, The Ronald O. Perlman Center for Political Science and
    Economics, 133 South 36th Street, Philadelphia, PA 19104, \href{mailto:xucheng@upenn.edu}{xucheng@upenn.edu}}
    \and 
    Eric Renault\thanks{Brown University, Department of Economics -- Box B, 64 Waterman Street, Providence, RI
    02912, \href{mailto:eric_renault@brown.edu}{eric\_renault@brown.edu}}
    \and 
    Paul Sangrey\thanks{University of Pennsylvania, The Ronald O. Perlman Center for Political Science and
    Economics, 133 South 36th Street, Philadelphia, PA 19104, \href{mailto:paul@sangrey.io}{paul@sangrey.io}}}
    
\title{Inference for the Price of Volatility Risk Under Weak Identification}

\date{\today}

\begin{document}

\begin{titlepage}


    \maketitle
    \thispagestyle{empty}
    \addtocounter{page}{-1}

    \begin{abstract} \singlespacing \noindent 
        The risk-return trade off is a central question in modern asset pricing. 
        Even though the theoretical literature cohesively argues there should be a strong positive correlation
        between expected returns and volatility, empirically measuring it has proven difficult. 
        In addition, the option pricing literature shows that volatility enters as its own risk factor, not just
        as a predictor of returns.
        Even obtaining point estimates of these risk prices in the presence of contemporaneous correlation between
        returns and volatility, the  volatility feedback effect, has proven delicate.
        We develop methods to provide  valid confidence intervals for the price of volatility and equity risk from
        equity data directly that account for this weak identification of the various parameters. 
        We do this by adapting the results in weak identification literature that uses drifting sequences in order
        to be robust to the data's identification strength. 
    \end{abstract}

    \jelcodes{C12, C14, C38,  C58, G12}

    \keywords{identification, robust inference, stochastic volatility, leverage, equity risk premium, volatility
    risk premium, risk price, confidence set, asymptotic size}

\end{titlepage}

\phantomsection
\addcontentsline{toc}{section}{Introduction}

Modern finance is all about the risk return trade offs that investors face and how to optimally respond to them. 
In particular, the central question of asset pricing is what drives expected returns.
Standard economic theory predicts you must compensate investors with higher expected returns when they face more
risk.
In other words, we would expect a positive relationship between the mean and volatility of returns.
In fact, in \gentextcites{sharpe1964capital,lintner1965security} capital asset pricing model (CAPM) the expected
return varies proportionally with the volatility. 
In other words, we have a constant price of volatility risk and expected returns are perfectly correlated with
volatility.
Even in more complicated models such as the long-run risk model and rare disasters models find a very close
relationship between variance and mean of returns \parencite{bansal2014volatility, wachter2013can}.

Consequently, an important empirical question is what is the precise magnitude of this relationship.
However, unlike the consensus in the theoretical literature, the empirical literature has found pinning down this
relationship quite difficult.
Not only has its magnitude proven difficult to determine, but various estimates even differ in sign,
\parencite{lettau2010measuring}, .

The empirical literature, which we examine in more detail in the literature review, has focused on point estimates
of this magnitude. 
However, if individual investors are ambiguity averse as in \textcite{hansen2001robust, jiu2012ambiguity}, they
will care not just about how the representative investor prices volatility but also their uncertainty regarding
this estimate. 
Furthermore, when economists calibrate models, they need to know how precisely the data determine the parameters
they are calibrating.
If the need to alter the parameter value slightly in order to make their model perform well, are they bringing in
more restrictions or data to more precisely determine the parameter of interest or are they using a value that the
data tell us is incorrect?

Clearly, as obtaining precise believable point estimates of the price of volatility risk has proven quite
difficult, we should expect doing valid inference to be even more delicate.
To the best of our knowledge, this is the first paper to directly tackle this question.
Various authors report confidence intervals as well as their point estimates.
However, they do not take into account the weak identification that makes getting the point estimate difficult in
computing these estimates.

Why is it that measuring this price is so difficult when the theoretical literature is so cohesive?
Econometrically, it is because the volatility price is weakly identified, as in \textcite{andrews2012estimation},
in that the strength of the identification of the price depends upon the value of other parameters. 
This obviously begs the question --- what are these parameters? 

To estimate the risk prices, there are three different phenomena that must be distinguished.
First, the econometrician must disentangle the volatility feedback effect (leverage) which is a contemporaneous
relationship between the volatility and returns from the risk premium, which is a relationship between volatility
and expected returns. 
It is not a contemporaneous relationship, but rather a predictable one. 

Second, and just as important.
In the papers estimating the volatility expected-return relationship such as
\textcite{brandt2004relationship,lettau2010measuring} are implicitly using a one-factor model where volatility
is that factor.
However, the option pricing literature, such as  \textcite{christoffersen2013capturing}, has argued that in order
to match the option pricing data we need to include another factor.
In other there are two sources of risk that the investors face: what returns will do and what the volatility
will do.
One can view the loading on the equity risk to itself be a function of the volatility.
The econometrician can rewrite this as a two factor model with equity and volatility risk up to higher order
terms.
Since, the econometrician must disentangle these two effects, their ability to do this will depend upon various
other parameters.
As mentioned in the introduction,in CAPM we only have factor, and so clearly we cannot disentangle them.
However, most models of interest to economists nest CAPM as a special case. 
Consequently, for certain values of the parameters, the model has only one factor. 
This directly implies that we cannot separately identify the two risks for some values of the other parameters.
More generally, the identification strength of the risk prices depends upon the other parameters.
The weak identification literature shows that in these scenarios, the finite-sample distributions of the estimates
are highly nonstandard, and the usual asymptotic approximations do not perform well.


Hopefully, the reader has a high-level understanding of the issue in at hand, and why simple econometric
procedures does not provide a useful guide to the true uncertainty regarding the parameters.
The obvious next step is considering how we need to do this in practice.
Since the contribution of this paper is in terms of methodology and empirical results, we will take a model from
the literature that has the various components, instead of developing our own asset or option pricing model.
In particular, we take the model from \textcite{khrapov2016affine} and use it to estimate the relevant parameters. 

This model has a few nice features. 
First, it has both equity and volatility prices and a leverage effect. 
As such, it is the natural discrete time analogue of the \textcite{heston1993closedform} option pricing model. 
It has an exponentially affine stochastic discount factor  and shares with \textcite{heston1993closedform} the
advantage of having a structure preserving change of measure between the physical and risk-neutral models.
By doing our analysis in discrete-time we are able to more directly compare our results to risk-premia estimates
outside of the option pricing literature and the jumps in high-frequency innovations will not dramatically affect
our results.  
If we were to use a diffusion process in continuous time, we would be severely counterfactually constraining the
higher-order  moments of the process in way that would likely bias our inference. 

As far as estimation is concerned, we follow \textcite{khrapov2016affine} in using a finite version of spectral
General Method of Moments (GMM).
Effectively, we take some moments implied by the characteristic function of returns and use them in a standard GMM
setup.
The data we use are the bivariate series $\begin{pmatrix} r_{t+1}, \sigma^2_{t+1} \end{pmatrix}$.
$r_{t+1}$ is the daily return on some asset, and we use its associated realized volatility for $\sigma^2_{t+1}$.
We go into further detail in \cref{sec:data} regarding how we obtained it, the time-span covered, and so on.

\section{Literature Review}\label{sec:lit_review}


\section{The Model}\label{sec:model}

\addtocounter{subsection}{1}

We estimate the prices of some factors using moment conditions derived from an option pricing model. 
As is standard in that literature, we will do  this by specifying the physical and risk-neutral measures and their
relationship, i.e.\@ the stochastic discount factor or pricing kernel.
Let $\F_t$ be the representative investor's information set, and $P_t$ be the price on the asset in question, with
associated return $r_{t+1}$ and volatility  $\sigma^2_{t+1}$.

Given $\F_{t}$, the vector $\left( r_{t+1},  \sigma^2_{t+1}\right)$ is drawn from some process --- $\PP$ --- the
physical measure. 
We can further define the risk neutral measure --- $\QQ$ ---  as the process that makes $P_t$ a martingale, i.e.\@
the following holds. 
The advantage of defining $\QQ$ is that for some payoff function --- $f$  -- that is a function of future return
--- $r_{t+1}$ and volatility $\sigma^{t+1}$ and potentially depends upon the current information available --
$\F_t$, we can price this payoff as its expectation with respect to $\QQ$.
In other  words, the price of $f(r_{t+1}, \sigma^2_{t+1}, \F_t)$ satisfies for all $t$ and for all $f$.
This is useful because we can choose $f$ as a convenient function for our estimation. 

\begin{equation}
    P_t(f) = \E_{\QQ}\left[ f\left(r_{t+1}, \sigma^2_{t+1}, \F_{t}\right)  \mvert \F_{t}\right]
\end{equation}

Since $P_t(f), r_{t+1}$ and $\sigma_t^2$ are observable, if we specify a model for $\F_t$ in terms of observable
(to the econometrician) variables, this provides a moment condition that we can use. 
However, this condition does not identify everything we wish to estimate, in particular it does not identify the
risk prices. 


To see this, we complete the model by defining the stochastic discount factor --- $M_{t, t+1}$ --- as the
Radon-Nikodym derivative between the $\PP$ and $\QQ$ measures. 
No arbitrage guarantees that this will exist, \parencite{harrison1978martingales}.
Since risk prices arise from investors' demand for compensation to hold risk, i.e.\@ their risk-aversion, here is
where they show up in the $\QQ$ measure. 
(We collect the parameter of interest into a vector --- $\eta$.)


\begin{defn}{Asset Pricing Moments}
    \begin{equation}
        P_t(f)  = \E_{\QQ} \left[f\left(r_{t+1}, \sigma^2_{t+1} , \F_t\right) \mvert \F_t \right] =
        \E_{\PP}\left[M_{t,t+1}(\eta)f\left(r_{t+1}, \sigma^2_{t+1}, \F_t\right) \mvert \F_t \right] 
    \end{equation}
\end{defn}

\subsection{Dynamics}

To specify the model we must specify what the $\PP$ and $\QQ$ measures are and what $M_{t, t+1}(\eta)$ is.
We start by specifying the $\PP$ measure.

Following \textcite{khrapov2016affine}, we specify the joint dynamics of $\left(r_{t+1}, \sigma^2_{t+1}\right)$
under $\PP$ using Compound Autoregressive models (Car) of \textcite{darolles2006structural}.
These models are autoregressive models for the Laplace transform of the process.
The leading parameterization is the autoregressive gamma process (ARG) of \textcite{gourieroux2006autoregressive}.

In particular, we assume that the variables are first-order Markov and there is no Granger causality from return
to the volatility and that returns are serially independent given the volatility path.
In other words, the volatility drives all of the dynamics of the process.
Note, we do allow $\sigma^2_{t+1}$ and $r_{t+1}$ to be contemporaneously correlated.
They are in the data.

Since the Laplace transform is an expectation of a specific function, by assuming the data are jointly CAR(1), we
can represent out model under the physical  as follows for some functions $a_{\PP}, b_{\PP}, \alpha_{\PP},
    \beta_{\PP}$, and $\gamma_{\PP}$ for all $u$ in its domain.

\begin{restatable}[The Model under the Physical Measure]{defn}{physicalMeasureModel}
    \label{defn:physical_model}
    \begin{align}
        \E_{\PP} \left[\exp(-u \sigma^2_{t+1}) \mvert \sigma^2_t\right] &= \exp\left( - a_{\PP}(u)
            \sigma^2_{t} - b_{\PP}(u) \right) \\
        \E\left[\exp(-u r_{t+1}) \mvert \sigma^2_{t},  \sigma^2_{t+1}\right] &= \exp\left(- \alpha_{\PP}(u)
        \sigma^2_{t+1} - \beta_{\PP}(u)\sigma^2_{t} - \gamma_{\PP}(u) \right) 
    \end{align}
\end{restatable}

The model in \cref{defn:physical_model} is parameterized using the functions mentioned and at this level of
generality fully nonparametric. 
To gain tractability, we will specify these function below obtaining a parametric model.
In other words, we replace the infinite degrees of freedom in \cref{defn:physical_model} with a parameter vector
with finite degrees of freedom.

First, we assume that the volatility follows an autoregressive gamma process---ARG(1), and so its physical measure
dynamics are governed by following equations.

\begin{gather}
    a_{\PP}(x) = \frac{\rho x}{1 + c x}  \\
    b_{\PP}(x) = \delta \log \left(1 + c x\right) \\
    \rho \in [0, 1), c > 0, \delta > 0 
\end{gather}

In order to interpret these parameters, it is useful to note the formula for the conditional mean
\parencite[8]{khrapov2016affine}.
As we can see, $\rho$ is a persistence parameter, and $\delta$ is a drift.
The parameter $c$ scales the volatility.

\begin{equation}
    \E\left[\sigma^2_{t+1} \mvert \sigma^2_t\right] = c \delta + \rho \sigma^2_t
\end{equation}

Before, we discuss how we parameterize the Laplace function for the return process, we will consider the $\QQ$
measure.
We will assume that the risk-neutral measure is structure preserving.
In other words, the Laplace functions maintain the same structure for some functions some functions $a_{\QQ},
b_{\QQ}, \alpha_{\QQ}, \beta_{\QQ}$, and $\gamma_{\QQ}$.
This is a relatively weak assumption because the absolute continuity of the two measures places strong
restrictions on deviations from this case.
For example, in the continuous-time limit this structure-preserving relationship would always hold.


\begin{defn}{The Model under the Risk-Neutral Measure}
    \label{defn:risk_neutral_model}
    \begin{align}
        \E_{\QQ} \left[\exp(-u \sigma^2_{t+1}) \mvert \sigma^2_t\right] &= \exp\left( - a_{\QQ}(u)
            \sigma^2_{t} - b_{\QQ}(u) \right) \\
        \E\left[\exp(-u r_{t+1}) \mvert \sigma^2_{t},  \sigma^2_{t+1}\right] &= \exp\left(- \alpha_{\QQ}(u)
        \sigma^2_{t+1} - \beta_{\QQ}(u)\sigma^2_{t} - \gamma_{\QQ}(u) \right) 
    \end{align}
\end{defn}

Since the Laplace transform for $r_{t+1}$  is exponentially affine under both $\PP$ and $\QQ$, the measure change
between them is also exponentially affine.
If we let $\pi$ be the price of volatility risk and $\theta$ be the price of equity risk, we can write down the
change of measure as follows.
We let $r_{f,t}$ be the risk-free rate.
It deterministically discounts the prices but plays no further role in the analysis.


\begin{defn}{The Stochastic Discount Factor}
    \begin{equation}
        M_{t,t+1}(\pi, \theta) = \exp\left(-r_{f,t}\right) \exp\left(m_{0}(\pi, \theta) + m_1(\pi, \theta)
        \sigma_t^2 - \pi \sigma^2_{t+1} - \theta r_{t+1}\right) 
    \end{equation}
\end{defn}


\subsection{Derivation of the Moment Conditions}\label{sec:deriving_mom_conds}

In this section, we derive the moment conditions that we use to identify the model.
We follow \textcite{khrapov2016affine} closely.
Recall the model under the physical measure. 


\physicalMeasureModel*

By definition, we have the following. 

\begin{equation}
    a_{\PP}(0) = b_{\PP}(0) = \alpha_{\PP}(0) = \beta_{\PP}(0) = \gamma_{\PP}(0) 
\end{equation}

We can differentiate the Laplace transform at zero to compute the moments.
The negative signs on the mean, but the not the 2nd moment come from the negative signs inside the exponential on
the right-hand side canceling for the negative signs in the Taylor series expansion of the left-hand side for the
mean but not the 2nd moment. 

We start by deriving the first two conditional moments of $r_{t+1}$ by viewing the log of the Laplace transform as
a cumulant generating function.
$K\left(-t \right) \coloneqq \log \E\left[\exp(-t X) \right]$. 
This is useful because the first two derivatives of $K(-t)$ evaluated at zero are the centered moments.

\begin{align}
    \E\left[r_{t+1} \mvert \F_t, \sigma^2_{t+1}\right]  &= \alpha_{\PP}'(0) \sigma^2_{t+1}  + \beta_{\PP}'(0)
    \sigma^2_{t} - \gamma_{\PP}'(0) \\
    \Var\left[r_{t+1}^2 \mvert \F_t, \sigma^2_{t+1}\right]  &= -\alpha_{\PP}''(0) \sigma^2_{t+1}  -
    \beta_{\PP}''(0) \sigma^2_{t} - \gamma_{\PP}''(0)
\end{align}

We can then use the law of total variance to compute $\Var\left[r_{t+1} \mvert \F_t\right]$.

\begin{equation}
    \Var\left[r_{t+1} \mvert \F_t\right]  = -\left( \alpha_{\PP}''(0) \E \left[\sigma^2_{t+1} \mvert \F_t \right]
    + \beta_{\PP}''(0) \sigma^2_{t} + \gamma_{\PP}''(0)\right) + {\alpha_{\PP}'(0)}^2 \Var\left[\sigma^2_{t+1}
    \mvert \F_{t} \right]
\end{equation}

We are computing $\E\left[\sigma^2_{t+1} \mvert \F_t\right]$ and $\Var\left[\sigma^2_{t+1} \mvert
\F_t\right]$


\begin{align}
    \E\left[\sigma^2_{t+1} \mvert \F_t \right]  &= a_{\PP}'(0) \sigma^2_{t}  + b_{\PP}'(0) \\
    \Var\left[\sigma^2_{t+1} \mvert \F_t \right]  &= -a_{\PP}''(0) \sigma^2_{t}  - b_{\PP}''(0) 
\end{align}

Before we show how the risk prices $\pi, \theta$ show up in the equations above, we will introduce some parameters
that govern the dynamics of the volatility process.


\begin{defn}{Volatility Dynamics Parameters}
    \begin{align}
        \rho &\coloneqq a_{\PP}'(0) \in (0,1) \\
        c &\coloneqq - \frac{a_{\PP}''(0)}{2 a_{\PP}'(0)} > 0 \\
        \delta &\coloneqq -2 \frac{a_{\PP}'(0) b_{\PP}'(0)}{a_{\PP}''(0)} > 0 \\
        \omega &\coloneqq -4 \frac{b_{\PP}''(0) [a_{\PP}'(0)]^2}{[a_{\PP}''(0)]^2}
    \end{align}
\end{defn}

To interpret these factors, can derive the conditional conditional moments. 
$\rho$ is a persistence parameter.
$c$ is a scaling parameter.


\begin{align}
    \E\left[\sigma^2_{t+1} \mvert \F_t \right]  &= \rho \sigma^2_{t}  + c \delta\\
    \E\left[\sigma^2_{t+1}\right]  &= \frac{c \delta}{1 - \rho} \\
    \Var\left[\sigma^2_{t+1} \mvert \F_t \right]  &=  2 c \rho \sigma^2_{t}  + c^2 \omega \\
    \Var\left[\sigma^2_{t+1} \right]  &=  \frac{c^2}{1 - \rho^2} \left(\frac{2 \rho \delta}{1 - \rho}  +
        \omega \right) 
\end{align}

Clearly, we can identify all four of these parameters from the volatility data as long as the parameters are in
the interior of their domains.

\subsubsection{\texorpdfstring{Parameterizing the mean and variance of $r_{t+1}$}{Parameterizing the return's
mean and variance}}\label{sec:deriving_sdf_functions}


In this section, we derive the formulas for the mean and variance of the expected return in terms of the risk
prices.

\begin{equation}
    M_{t,t+1}(\pi, \theta) = \exp(-r_{f,t}) \exp \left( m_0(\theta, \pi) + m_1(\theta, \pi) \sigma_t^2 - \pi
    \sigma^2_{t+1} - \theta r_{t+1}\right)
\end{equation}

Since the SDF must integrate to $1$ once we discount using the risk-free rate $r_{f,t}$\, we have the following
restrictions.


\begin{gather}
    \E_{\PP}\left[\exp\left(m_0(\theta, \pi) + m_1(\theta, \pi) \sigma^2_t - \pi \sigma^2_{t+1} - \theta r_{t+1}
    \right) \mvert \F_t \right] = 1 \\
    \intertext{By the law of iterated expectations.}
    \E_{\PP}\left[ \E\left[\exp\left(m_0(\theta, \pi) + m_1(\theta, \pi) \sigma^2_t - \pi \sigma^2_{t+1}\right)
        \exp\left( - \theta r_{t+1}\right) \mvert \F_t, \sigma^2_{t+1} \right]\right] = 1 \\
    \intertext{The second term is the Laplace transform of $r_{t+1}$.}
    \E_{\PP}\left[\exp\left(m_0(\theta, \pi) + m_1(\theta, \pi) \sigma^2_t - \pi \sigma^2_{t+1} \right)
        \exp(-\alpha_{\PP}(\theta) \sigma^2_{t+1} - \beta_{\PP}(\theta) \sigma^2_{t} - \gamma_{\PP}(\theta_2)
        \mvert \F_t \right] = 1 \\
    \intertext{Reorganizing terms.}
    \E_{\PP}\left[\exp\left(m_0(\theta, \pi) + m_1(\theta, \pi) \sigma^2_t - \beta_{\PP}(\theta) \sigma^2_{t} -
        \gamma_{\PP}(\theta_2) \right) \exp(-\left(\pi + \alpha_{\PP}(\theta)\right) \sigma^2_{t+1}) \mvert \F_t
        \right] = 1 \\ 
    \intertext{Substituting in the Laplace transform for $\sigma^2_{t+1}$.} 
    \E_{\PP}\left[\exp(m_0(\theta, \pi) + m_1(\theta, \pi) \sigma^2_t - \beta_{\PP}(\theta) \sigma^2_{t} -
        \gamma_{\PP}(\theta_2)  - a_{\PP}(\pi + \alpha_{\PP}(\theta)) - b_{\PP}(\pi + \alpha_{\PP}(\theta))
        \mvert \F_t \right] = 1 
\end{gather}

Since this equality must hold for all $\sigma^2_{t}$, we can solve for both $m_0(\theta, \pi)$ and $m_1(\theta,
\pi)$ in terms of the functions determining the physical measures.

\begin{align}
    m_0(\theta, \pi)  &= \gamma_{\PP}(\theta) + b_{\PP}(\alpha_{\PP}(\theta) + \pi) \\
    m_1(\theta, \pi)  &= \beta_{\PP}(\theta) + a_{\PP}(\alpha_{\PP}(\theta) + \pi) 
\end{align}

The reason that this is interesting is all of the right-hand side variables  are in terms of the physical
measure.
In other words, we do not need, at least in principle, option data to estimate the SDF.
In particular, we can determine the risk prices from equity prices if the identified parts of the $\alpha_{\PP},
\beta_{\PP}$ etc.\@ functions are \textquote{sufficiently} invertible.
(I.e.\@ the matrix of their derivatives satisfies the appropriate non-singularity conditions.)


\subsubsection{Identification of the Risk Prices}


The goal moving forward is to determine what we need to assume about the physical measure functions in order to
identify the risk prices.
To do this, we need to do two things.
First, we need to characterize the part of the physical measure functions that is identifiable from equity data.
Second, we must consider under what conditions that allows us to identify the risk prices. 

The information that return data contain about equity pricing data is entirely encapsulated by the asset pricing
equation for excess returns.  
(In what follows, we will use $r_{t+1}$ as the excess log-return.)

\begin{equation}
    \E\left[ M_{t,t+1}(\theta, \pi) \exp(r_{t+1}) \mvert \F_{t-1} \right]
\end{equation}

We now characterize, the information in this set of moment conditions regarding the risk prices.
We start by substituting in the SDF formula from above.

\begin{gather}
    \E \left[ \exps*{ - \pi \sigma^2_{t+1} - (\theta - 1) r_{t+1} } \mvert \F_{t} \right]
        = \exps*{- m_0(\theta, \pi) - m_1(\theta, \pi) \sigma^2_t}
    \intertext{Similar to above, we used the law of iterated expectations to substitute in the conditional Laplace
        transforms of $r_{t+1}$ and $\sigma^2_{t+1}$.}
    \E\left[\exps*{- a_{\PP}\left(\pi + \alpha_{\PP}(\theta -1)\right) \sigma^2_t - b_{\PP}(\pi) -
        \beta_{\PP}(\theta-1) \sigma^2_t - \gamma_{\PP}(\theta-1)} \mvert \F_t \right] = \exps*{- m_0(\theta, \pi)
        - m_1(\theta, \pi) \sigma^2_t} 
\end{gather}


Similar, to above, we can now match the coefficients of the $\F_t$ measurable variables in the above moment
condition. 
We also substitute in the formulas for $m_0(\theta, \pi)$ and $m_1(\theta, \pi)$ that we derived in
\cref{sec:deriving_sdf_functions}.

\begin{align}
    \label{eqn:identification_eqn_1}
   \gamma_{\PP}(\theta-1) + b_{\PP}(\pi + \alpha_{\PP}(\theta - 1)  &= \gamma_{\PP}(\theta) +
       b_{\PP}(\alpha_{\PP}(\theta) + \pi)  \\
    \label{eqn:identification_eqn_2}
    \beta_{\PP}(\theta-1) + a_{\PP}(\pi + \alpha_{\PP}(\theta -1)) &= \beta_{\PP}(\theta) +
        a_{\PP}(\alpha_{\PP}(\theta) + \pi) 
\end{align}

We can characterize the identification restrictions in \cref{eqn:identification_eqn_1} and
\cref{eqn:identification_eqn_2} in two different cases.   

\begin{enumerate}
    \item[Case 1:] The price of equity risk $\theta$ satisfies the following equations. 
        \begin{equation}
            \alpha_{\PP}(\theta - 1) = \alpha_{\PP}(\theta)\ \text{and}\ \beta_{\PP}(\theta) = \beta_{\PP}(\theta
            -1)\ \text{and}\ \gamma_{\PP}(\theta) = \gamma_{\PP}(\theta - 1)
            \label{eqn:lack_of_id_condition}
        \end{equation}
        In this situation, clearly, regardless of the value of $\pi$, we can satisfy these equations.
        In other words, the asset pricing equation does not identify $\pi$. 
        As noted by \textcite{khrapov2016affine}, this is in line with the common belief that the econometrician
        needs options data to be able to identify the price of volatility risk. 
    \item[Case 2:] 
        In general, there is no reason to expect the \cref{eqn:lack_of_id_condition} to hold.
        Since we have two equations and two unknowns, it might seem reasonable to expect that we should be able to
        identify both $\theta$ and $\pi$.
\end{enumerate}


As discussed in \textcite[13]{khrapov2016affine}, in this 2nd case the existence of leverage allows us to
separate the two risk prices.
Consider a 2nd-order Taylor series expansion of $\alpha_{\PP}$.

\begin{align}
    \alpha(\theta) - \alpha(\theta - 1) &= \alpha_{\PP}'(0) + \alpha_{\PP}''(0) (\theta - \frac{1}{2}) +
    \text{higher-order terms}  \\
    \intertext{Implicitly defining $\psi \coloneqq \alpha_{\PP}'(0)$ and $\phi \coloneqq \sqrt{1 -
        \alpha_{PP}''(0)}$, we can rewrite this as follows.} 
        &=  \psi + (1 - \phi^2) (\theta - \frac{1}{2}) + \text{higher-order terms}  
\end{align}

If we are wiling to assume that the conditional distribution of $r_{t+1} \vert \sigma^2_{t+1}, \F_t$ is Gaussian,
the higher-order terms are identically zero.
The question at hand is there a way of estimating $\psi, \phi, \theta, \pi$ that is not asymptotically biased by
the existence of these higher-order terms.
We will explain precisely how we do this in \cref{sec:pricing_the_variance}.


\purple{Add a discussion about how the first two conditional moments identify the model in the Gaussian case $\iff$
    $\psi \neq (1 - \phi^2) (\theta - 1/2).$}



\subsection{Pricing the Variance}\label{sec:pricing_the_variance}


The main issue in case 2 is that we also do not know the physical measure equations.
In principle, we could estimate them from equity price data.
However, estimating conditional densities is quite difficult and asset pricing data is rather badly behaved.
Consequently, we do not want to either make parametric assumptions on these physical measure functions nor try to
non-parametrically estimate them.
However, we can estimate some of their low-order moments, such as the conditional mean and variance.


The specification for risks that we are using relies exclusively on the mean and volatility (i.e.\@ the first two
moments.)
If higher moments, such as skewness and kurtosis are also priced factors, as in \textcites{harvey2000conditional,
conrad2012exante, chang2013market},  and we used higher sample moments as well to determine the price of our
risk-factors our resulting estimates would be biased, likely substantially so. 

The question facing us is how to we prevent our model's potential misspecification regarding the risk prices of
higher-order moments from affecting our estimates of the volatility prices.
To answer this question, we turn to what our model specifies the continuous-time process, that our model is a
discretization of.

Imposing consistency with a continuous-time model is reasonable in our context because even though our model is
specified in discrete-time it is for an asset that is is continuously tradable.
A good discrete-time model for a continuously-tradable asset would is consistent with a continuous-time
generalization that rules our no-arbitrage.

When we get to the empirical exercise, we will use the integrated volatility as our measure of volatility.
 
\begin{defn}[Integrated Volatilty]
    \begin{equation}
        \sigma^2_{t+1} = \int_{t}^{t+1} \sigma^2(s) \dif s
    \end{equation}
\end{defn}

Standard asset pricing theory in continuous-time implies that continuous-time volatility --- $\sigma^2(s)$ ---
is the only priced factor \parencites{merton1973intertemporal, tsai2018pricing, sangrey2018jumps}. 
(This is a direct consequence of the form of It\^{o}'s Lemma for (potentially discontinuous) continuous-time
process.)
In general its price could vary over the course of the day.
For example, its price could  itself be a function of the volatility: $\pi(s-) = \pi(\sigma^2(s-))$.
If $\pi(t)$ is not a constant function intra-day, we would see that higher-order moments are priced.
The object of interest here is the price of volatility, and so we want the 0th order Taylor series expansion of
the $\pi(t)$.

To make the discussion in the previous paragraph more concrete, we want the following equalities to hold. 

\begin{equation}
    \pi \sigma^2_{t+1} = \pi \int_{t}^{t+1} \sigma^2(s) \dif s =  \int_{t}^{t+1} \pi(s) \sigma^2(s) \dif s
\end{equation}



\subsection{Characterizing the Structure of the Identification Problem in Continuous-Time}

In this section, we have the following two continuous-time processes.
The goal is to show that by using the first-two conditional moments in our identification strategy, the parameters
we are estimating converge to the true equity risk and volatility risk even in the presence of higher-order
time-variation in the price processes.
Our moment conditions we use in the estimation project the risk prices onto the information set generated by
$\E\left[r_{t+1}\mvert \F_{t-1}\right]$  and $\E\left[\sigma^2_{t-1} \mvert \F_{t-1}\right]$.
What we do here is the equivalent projection in population.
This procedure will give us a series of estimates $\hat{\pi}, \hat{\theta}$, etcetera.
These estimates will have some limiting values, as long as the moment conditions are sufficiently non-collinear.
This section characterizes precisely the role these limiting values play in population.
For example, we show that the risk-prices are the actual risk prices under some conditions, which we make
explicit.

The way by which we do this is by representing the processes in continuous-time.
Since the assets we are characterizing are continuously tradable they must satisfy some continuous-time
no-arbitrage conditions. 
In other words, there exists a continuous-time DGP that the DGP we have been using so far is a discretization of. 
This is useful because we can work with the first two moments of the process effectively without loss of
generality.
The linear-affine structure we have been using before will show up as certain terms being constant functions of
time in what follows.
The cost of doing this is that it requires more involved notation and more sophisticated mathematical tools.

\begin{defn}{Continuous-Time DGP}
    \label{defn:cont_time_dgp}

    \begin{align}
        \dif p(t) &= \mu_{p}(t) + \sigma^2(s) \dif W_{\sigma}(s)  \\
        \dif \sigma^2(s)  &= \mu_{\sigma}(t) + \sigma_{\sigma}^2(s) \dif W_{\sigma}(s) 
    \end{align}

\end{defn}

We also use  $\F_t$ to denote the filtration generated by $p(t)$.
We let $\F_{\sigma^2, t}$ be the filtration generated by both $p(t)$ and $\sigma^2(t)$.
Clearly, everything measurable with respect to $\F_t$ is measurable with respect to $\F_t$ as well.
This implies that the processes with to $\F_{\sigma^2, t}$ are absolutely continuous with respect to the same
process with respect to $\F_t$.
We also follow standard conventions and use $s-$ notation to refer to the predictable version of the object in
question. 

The log-Laplace transform for the physical measure that we have been using in the previous sections is the
discretization of a process with the following form.\footnote{The existence of the process below is implied
    by the existence of a discrete-time objects and mild smoothness conditions on the $\alpha_{\PP}$ etc.\@
    functions because local integrability conditions are implied by global integrability conditions plus
smoothness conditions.}

In continuous-time, with stochastic diffusion or stochastic variance-gamma processes, the first two moments of the
process determine the entire distribution under some general conditions which this DGP satisfies,
\parencite{sangrey2018jumps}.

Consequently, as long as we match the first two derivatives of continuous-time log-Laplace transform we are
matching the entire distribution.
The intuition here is that by using It\^{o}'s formula for jump  processes, the 2nd-order Taylor series is exact in
continuous-time.
The higher-order  terms disappear as we shrink the intervals. 


Consider the following derivation.
We write down the conditional log-Laplace transform as follows.

The only information we have concerning the volatility risk price arises from the difference between the
conditional distribution with respect to the $\F_{t, \sigma}$ filtration and the $\F_{t}$ filtration.
We can represent this information as the difference between the moments, or equivalent the log-Laplace
transforms, under the two measures. 

This raises the question what is the appropriate way of characterizing the difference between these two measure.
One way of doing this and the way that we will do it is by considering the difference between conditional
distributions at time $\tau + \Delta$ given the information sets $\F_{\tau}$, and $\F_{\sigma^2, \tau}$.
Clearly, if we do this for all $\tau$ and all $\Delta$, we will have fully characterized the differences between
the processes themselves.
In addition, as is standard in stochastic process theory we can consider $\Delta \to 0$ without loss of
generality.



\begin{equation}
    K(u, \tau) \coloneqq \log \E_{\PP}\left[\exp(-u r_{\tau + \Delta}) \mvert \F_{\sigma^2, \tau}\right] = -
    \alpha_{PP}(u) \sigma^2_{\tau + \Delta} - \beta_{\PP}(u) \sigma^2_{\tau} - \gamma_{PP}(u) 
\end{equation}

The next step is to replace the physical measure functions with their Taylor series expansions.
We have assumed that they display sufficient smoothness for this to be valid.
The future volatility --- $\sigma^2_{t+\Delta}$ --- is a random variable with respect to $\F_{\tau}$.
Consequently, we have a series of random variables with respect to $\F_\tau$, one for each $u$.

The question is how many effective degrees of freedom do we actually have.
Since the moments must be valid processes with respect to both filtrations as must the underlying price and
volatility, the moments of the price and volatility cannot vary too much over time.
To put it in terms of Taylor series, only the first few terms can matter, i.e.\@ the physical measure functions
must be representable by a low-order polynomial.


\begin{equation}
    \label{eqn:cumulant_process_taylor_expansion}
    \begin{alignedat}{2}
        K(u, \tau) &= &&- \alpha_{\PP, \tau}'(0) u \sigma^2_{\tau + \Delta} - \alpha_{\PP, \tau}''(0) u^2 \sigma^2_{\tau +
            \Delta} \alpha_{\PP, \tau}''(0) \sigma^2_{\tau + \Delta}  \\ 
        & &&- \beta_{\PP, \tau}'(0) u\sigma^2_{\tau} + O_p(\beta_{\PP, \tau}''(0)) u^2 \sigma^2_{\tau} -
          \gamma_{\PP, \tau}'(0)u  + O_P(\gamma_{\PP, \tau}''(0)) u^2 \\
    \end{alignedat}
\end{equation}

The next step in the derivation is to figure out the magnitude of the Taylor series coefficients in
\cref{eqn:cumulant_process_taylor_expansion}.
Recall with stochastic diffusion or stochastic variance-gamma processes, the first two moments of the
process determine the entire distribution under some general conditions which this DGP satisfies,
\parencite{sangrey2018jumps}
Since the first two derivatives of the cumulant generating function evaluated at zero are the moments of the first
two moments all of the higher-order coefficients  equal zero.

To make this concrete, \cref{eqn:cumulant_process_taylor_expansion} must equal the following. 

\purple{Is $K(\tau, u)$ currently the negative cumulant generating function?}

\begin{equation}
    \label{eqn:cond_log_cumulant_function}
    K(u, \tau) = \text{constant} + \E\left[r_{\tau + \Delta} \mvert \F_{\sigma^2, \tau}\right]  u  +
    \Var\left[r_{\tau + \Delta} \mvert \F_{\sigma^2, \tau}\right]  u^2 
\end{equation}

Note, Since $-\alpha''(0)$ is a variance, $\alpha''(0)$ must be negative.

\begin{defn}{Conditional Variances and Correlations} 
    \begin{align}
        \psi(\tau) &\coloneqq \alpha_{\PP, \tau}'(0) \\
        \phi(\tau) &\coloneqq \sqrt{1 - \phi^2}) 
    \end{align}
\end{defn}

$\phi(\tau)$ and $\phi(\tau)$ define stochastic processes.

By matching coefficient between \cref{eqn:cumulant_process_taylor_expansion} and
\cref{eqn:cond_log_cumulant_function}, we can rewrite the log-cumulant function as follows.

Since the prices are semimartingales with respect to $\F_{\sigma^2, \tau}$, and the $(\cdot)_{\PP,\tau}$ functions
characterize their conditional means and variances with respect to this filtration, they are must have finite
variation. 

\purple{Why are the $(\cdot)_{\PP, \tau}$ functions continuous functions of $\tau$?}


\begin{equation}
    K(u, \tau) = [\alpha_{\PP, \tau}'(0)\sigma^2(\tau)  + \beta_{\PP, \tau}'(0)\sigma^2(\tau-)] u - [\alpha_{\PP,
    \tau}''(0) \sigma^2(\tau)] u^2  - \gamma_{\PP, \tau}'(0) 
\end{equation}


The physical measure functions are continuous functions of $\tau$ and hence predictable and of finite-variation.
Consequently, for each $u$, we have implicitly defined $K$ as a differentiable function of $\sigma^2(\tau)$.
So by It\^{o}'s formula, we can rewrite $K(u, \sigma^2, \tau)$ as follows.

\begin{equation}
    \dif K(u, \tau) = \psi(\tau) \dif \tau + \phi(\tau) \dif W(\tau) + 
\end{equation}


\purple{The goal is to write down the price formula as follows.}

First, I write down the formula for the price process with respect to the $\F_{\sigma^2, \tau}$ filtration.

\begin{align}
    \dif p(\tau) = \left[\mu + coeff1 \E\left[\sigma^2(\tau)  \mvert \F_{\tau}\right]  + \psi \left[\sigma^2(\tau)
    - \E\left[\sigma^2(\tau)  \mvert \F_{\tau}\right]\right]\right]  \dif \tau + \sigma_{p} \dif W^{p}(\tau) 
\end{align}

Conversely, the formula for the price process with respect to the $\F_{\tau}$ filtration is as follows. 

\begin{align}
    \dif p(\tau) &= \left[\mu + (coeff1 + \psi) \E\left[\sigma^2(\tau)  \mvert \F_{\tau}\right]\right] \dif \tau +
        \sigma^2(\tau) \dif W(\tau)  \\
    &= \left[\mu + (coeff1 + \psi) \E\left[\sigma^2(\tau)  \mvert \F_{\tau}\right]\right] \dif \tau +
        \sigma^2(\tau) [\dif W^{p}(\tau) + \phi \dif W^{\sigma}(\tau)] \\
    &= \left[\gamma_{\PP, \tau}'(0) + (\beta_{\PP, \tau}'  + \psi(t) ) \E\left[\sigma^2(\tau)  \mvert
       \F_{\tau}\right]\right] \dif \tau + \sigma^2(\tau) [\dif W^{p}(\tau) + \phi(\tau) \dif W^{\sigma}(\tau)] \\
       \label{eqn:price_process_decomp}
    &= \gamma_{\PP, \tau}'(0) \dif \tau + \left[\beta_{\PP, \tau}'  + \psi(t)\right] \sigma^2(\tau-) \dif \tau +
       \sigma^2(\tau) [\dif W^{p}(\tau) + \phi(\tau) \dif W^{\sigma}(\tau)]
\end{align}


We can time-aggregate \cref{eqn:price_process_decomp}, and get the following formula for the discrete-time
returns.  
Let $\gamma(\tau) \coloneqq \gamma_{\PP, \tau}'(0)$.
Let $\beta(\tau) \coloneqq \beta_{\PP, \tau}'(0)$.


\begin{equation}
    r_{t+1} = \int_t^{t+1} \gamma(s) \dif s + \int_{t}^{t+1} [\beta(s) + \psi(s)] \sigma^2(s) \dif s +
    \int_t^{t+1} \sigma^2(s) \left[\sqrt{1 - \phi(\tau)^2} \dif W^{p}(s) + \phi(\tau) \dif W^{\sigma}(s)\right]
\end{equation}

We start by defining the means of $\beta(\tau), \psi(\tau)$, and $\phi(\tau)^2$: $\beta_0, \psi_0, \phi^2$.
By the intermediate value theorem, there exists an $s_i^{\dagger}$ for each of these functions that function evaluated
at $s^{\dagger}$ equals the mean over the interval.
For example $\beta\left(s_i^{\dagger}\right) = \int \beta(\tau) \dif \tau$.  
We can take mean value expansions for each of these functions.

\begin{equation}
    \label{eqn:price_decomp}
    \begin{alignedat}{1}
        r_{t+1} &= \gamma_0 + \beta_0 \E\left[\int_t^{t+1} \sigma^2(s) \dif s \mvert \F_t\right] + \psi_0
            \int_{t}^{t+1} \sigma^2(s) \dif s  \\
        &\qquad + \int_t^{t+1} \sigma^2(s) \left[\sqrt{1 - \phi_0^2} \dif W^{p}(s) + \phi_0 \dif
        W^{\sigma}(s)\right] + \eta_{t+1} \\
    \end{alignedat}
\end{equation}

To show identification of our results, we need to characterize the error term in the expression above.
In particular, we need to show that it is mean-zero and conditionally mean-independent.
The only non-trivial part in characterizing $\eta_t$ is characterizing the error generated by replacing $\sqrt{1 -
\phi^2(\tau)}$ with $\sqrt{1 - \phi_0^2}$.
Note, we know by the mean value theorem, there exists a $\phi^{\star} \in [\inf_{\tau} \phi(\tau), \sup_{\tau}
\phi(\tau)]$, so that the following holds.

\begin{equation}
    \label{eqn:price_error_term}
    \begin{alignedat}{1}
        \eta_{t+1} &= [\gamma(\tau)  - \gamma(0)] + [\beta(\tau)  - \beta(0)] \E\left[\int_t^{t+1} \sigma^2(\tau)
            \dif \tau \mvert \F_t\right] + \int_{t}^{t+1} [\psi(\tau) - \psi_0] \sigma^2(\tau) \dif \tau  \\
        &\qquad + \int_t^{t+1} \left[\frac{(\phi^{\star})^2 - \phi_0^2}{2}\right] \sigma(\tau) \dif W^{p}(\tau)
          + \int_t^{t+1} [\phi(\tau) - \phi_0] \sigma(\tau) \dif W^{\sigma}(\tau) \\
    \end{alignedat}
\end{equation}

All of they terms in  \cref{eqn:price_error_term} are clearly unconditionally mean zero over time except for the
second to the last one.
We are just considering deviations from the means.
Since $\phi(\tau)$ is a correlation and hence contained within $[-1,1]$ and contained within the $\F_{\tau}$, it
is independent of $\dif W^{p}(\tau)$.
Consequently, the penultimate term is also unconditionally mean zero. 





\section{GMM}\label{sec:GMM}

We derive a set of moment conditions from the characteristic function above by evaluating it at a grid of points
in $[0,1] \times i [0,1]$. 
That is we can define a function $g_t(x, \eta)$

\begin{defn}{Moment Conditions}
    \begin{equation}
        g_t(x, \eta) \coloneqq Z_t \otimes \begin{bmatrix} \exp(- x \sigma^2_{t+1}) - \exp\left( - a(x) \sigma_t^2
        - b(x) \right) \\ \exp\left(- x r_{t+1}\right) - \exp\left(- \alpha(x) \sigma^2_{t+1} - \beta(x)
        \sigma^2_t - \gamma(x)\right) \end{bmatrix}
    \end{equation}
\end{defn}

Where the instruments are given by \cref{defn:instruments} for complex unit $i$. 

\begin{defn}{Instruments}
    \label{defn:instruments}
    \begin{equation}
        Z_t = \left[1, \exp\left(- i \sigma_{t-1}^2\right), \exp\left(-i \sigma^2_{t-2}\right)\right] 
    \end{equation}
\end{defn}

The implied unconditional moment restrictions are the following.  

\begin{equation}
    \E \begin{bmatrix}  \mathrm{Re} (g_t(x, \eta)) \\ \mathrm{Im} (g_t(x, \eta)) \end{bmatrix} = 0
\end{equation}


The optimal weighting matrix has its standard form as the precision matrix of the moments as long as we choose a
finite gird for $x$. 
If we use the entire continuum, handling the weights becomes more delicate. 
So we use only finitely many moments for now.


\begin{restatable}[Identified Set]{lemma}{identifiedSet}
    \label{lemma:IdentifiedSet}
    Let $\eta_0 \coloneqq \left(\rho_0, c_0, \phi_0, \pi_0, \theta_0\right)$.
    Let the domain $\Eta$ for $\eta$ be defined as follows. 
    \begin{equation}
    \begin{split}
        \Eta &\coloneqq \left\{ \eta \in  [0, \overline{\rho}] \otimes [0, \overline{c}] \otimes \left[-1,
            1\right] \otimes \left[-\underline{\theta}_1, \overline{\theta}_1\right] \otimes
            \left[-\underline{\theta}_2, \overline{\theta}_2\right]\right\} \\
        &0 < \overline{c}, -\underline{\theta}_1, \overline{\theta}_1, -\underline{\theta}_2 < \infty, 0 <
            \overline{\rho} < 1
    \end{split}
    \end{equation}


    \begin{equation}
        \text{Given}\ \eta_0 \in \Eta,\ \text{the moment conditions}\ g_t(x, \eta)\ \text{identify}\
    \begin{cases}
        \eta_0                  &\text{if}\ \phi \neq 0 \\
        \eta_0 \setminus \pi_0  &\text{if}\ \phi = 0
    \end{cases}
    \label{eqn:EtaDefn}
    \end{equation}

    In addition, if $\phi = 0$, the GMM criterion function is independent of $\theta$.
\end{restatable}


If we plug in the estimated values of the parameters from \textcite{khrapov2016affine} into $\frac{\partial
\phi}{\partial \pi}$ and plot it as a function of $\phi$,  we get the following.
The scale is omitted because it is not meaningful. 
As can clearly be seen in \cref{fig:fig:gamma_diff_theta2}, there is a zero when $\phi = 0$.

\begin{figure}[htb]
    \centering
    \caption{Derivative of $\gamma(x)$ with respect to $\theta$}
    \label{fig:fig:gamma_diff_theta2}
    \includegraphics[width=.5\textwidth]{gamma_diff_theta2.pdf}
\end{figure}

\begin{restatable}[Uniform Convergence under Strong Identification]{lemma}{UllnStrongID}
    \label{lemma:UniformConvergenceStrongID}
    Let $\Eta$ be the identified set defined by \cref{eqn:EtaDefn}.
    Further assume that $\phi_0 \neq 0$. 
    Let $\sampmom$ be the sample moment condition defined above, and $\W_T$ be the associated optimal weight matrix
    estimator.
    Then we have the following convergence.

    \begin{align}
        &\sup_{\eta \in \Eta} \norm*{\sampmom(\eta) - \E\left[g\left(\eta \mvert \gamma_0\right)\right]}_{Fro}
          \pto 0 \\ 
        &\sup_{\eta \in \Eta} \norm{\W_{T}(\eta)-\E\left[\W\left(\eta \mvert \gamma_0\right)\right]} \pto 
    \end{align}

\end{restatable}



%TODO where does this go?
By the above arguments, we have a consistent estimator for $\eta$ and the optimal weight matrix $\W \coloneqq
(\E\left[g g'\right])^{-1}$, and we will assume that the true value $\eta_{0}$ is in the interior of its sample
space $\Eta$.\footnote{Throughout we will use subscript \num{0}  to denote true values for parameters.}
Let $G \coloneqq \E\left[\frac{\partial}{\partial \eta} \popmom \right]$
Clearly, $g$ is continuously differentiable, and its derivative $G$ is continuous.
In addition, by the identification discussion $G' W \nabla G$ is nonsingular.

%TODO Replace this with strong (alpha) mixing of order (1+\epsilon). 
%Don't you need a lot more than this...?

\begin{assump}[Weak Dependence]
    \label{assumption:weak_dependence}
    $z_t \coloneqq \begin{pmatrix} r_{t+1} \\ \sigma^2_{t+1} \end{pmatrix}$ are $\alpha$-mixing with $\alpha_t =
       O\left(T^{-5}\right)$
\end{assump}

Since, $\norm*{g_t}$ is almost surely bounded by $1$ it has all of its moments and $z_t$ being $\alpha$-mixing
implies $g_t$ is as well by the central limit theorem for strongly mixing process 
$\sqrt{T} \sampmom(\eta^{*}) \dto \N\left(0, \E\left[\W\right]^{-1}\right)$ as required. 
Consequently, by \textcite[theorem 3.2]{newey1994large} we have convergence in distribution as well as convergence
in probability.

\begin{theorem}[Inference for $\eta$ under Strong Identification]
    Assume that $\phi_0  \in (-1,1) \setminus 0$, $\rho_0 \in [0,1)$, and $c_0 > 0$. 
    Further assume that the data are ergodic, stationary, and satisfy \cref{assumption:weak_dependence}.
    Then the following convergence in distribution holds.

    \begin{equation}
    \sqrt{T} (\widehat{\eta}_T - \eta_0) \dto \N\left(0, \left(G' \E[\W] G\right)^{-1}\right)
    \end{equation}
\end{theorem}





\section{Weak Identification Setup}

In this section, take the model described in the previous sections and place it in the setup of
\textcite{andrews2014Gmm} so that we ca analyze the effects of possible lack of identification in the model in a
nice clean way.
The goal here is to perform valid inference for $\pi, \theta$ even when $\phi$ might be zero. 


From the discussion above, we can collect the parameters discussed above into a parameter vector of the following
form,i.e.\@ recall the following: $\eta = \lbrace \rho, c, \delta, \phi, \pi, \theta \rbrace$
To write it in the notation of \textcite{andrews2014Gmm}, we partition $\eta$ into three subsets.

\begin{align}
    \phi &\coloneqq \phi  \in (-1, 1) \\ 
    \zeta &\coloneqq \lbrace \rho, c, \delta, \theta \rbrace \in [0,1) \times \R_{++} \times \R_{++} \times
    \R  \\
    \pi &\coloneqq \pi \in \R 
\end{align}

Let $\Eta$ be the set of possible $\eta$, that as defined above.
It is worth noting that the parameter space has a product form, i.e.\@ the values do not affect the valid values
of the other parameters.

In this environment, $\pi$ is not identified when $\phi = 0$.
Both $\phi$ and $\zeta$ are always identified, and $\zeta$ does not affect the identification of $\pi$.

Let $Q_T(\eta)$ be the GMM criterion function, then the GMM estimator $\hat{\eta}_T$ satisfies the following.


\begin{equation}
    \widehat{\eta}_T \in \Eta\ \text{and}\ Q_T(\hat{\eta}_T) = \inf_{\eta \in \Eta} Q_T(\eta) +
    o\left(T^{-1}\right) 
\end{equation}


Now that we have defined the parameters, we can characterize the set of assumptions necessary for valid inference.
We will work through the assumptions described in \textcite{andrews2014Gmm}.
The set of necessary assumptions is relatively complicated because we have to characterize the asymptotic
distribution under several different estimation strengths simultaneously, and the assumptions required to do that
  differ in the various cases. 
In what follows, we will use 

The first assumption specifies the basic identification
problem. It also provides conditions that are used to determine the
probability limit of the GMM estimator, when it exists, under all categories
of drifting sequences of distributions.
Let $\xi$ index the part of the distribution of the data $r_{t+1}, \sigma^2_{t+1}$ that is not determined by the
moment equations.
In general, it is a (likely infinite-dimensional) nuisance parameter that affects the distribution of the data. 


We collect the parameters that we are estimating $\eta$ and the nuisance parameter $\xi$ into one parameter,
$\gamma$ and associated parameter space $\Gamma$.
In the previous discussion we characterized the parameter spaces in a non-compact fashion, let $\Eta^{*}$ be a
compact subset of $\Eta$, where the true parameter values live.
%TODO Does the set of possible nuisance parameters depend upon the estimated parameters in our model?

\begin{defn}{Complete Parameter Space}
    \begin{equation}
        \Gamma \coloneqq \left\lbrace \gamma = (\eta, \xi) \mvert \eta \in \Eta, \xi \in \Xi \right\rbrace 
    \end{equation}
\end{defn}

We characterize these drifting sequences of distributions by sequences of true parameters $\gamma_T \coloneqq
(\eta_T, \phi_T)$.

\purple{TODO Add discussion of the limiting process.}
\purple{Verify that the assumptions on the parameter space hold.}
\purple{Discuss what happens if we lack identification and hence cannot consistently estimate the parameter.}

\begin{restatable}[Inference for $\eta$ under Weak Identification]{theorem}{InferenceWeakID}
    Let that $\phi_0  \in (\underline{\phi}_0,1)$, for some $\underline{\phi}_0 > -1$. 
    $\rho_0 \in [0,1)$, and $c_0 > 0$. 

    \purple{TODO  Add Conclusion}
\end{restatable}


\section{Simulations}

\section{Data}\label{sec:data}

We also assume that $\left[ \frac{\psi}{\phi} \right]^2 \approx \frac{\E \left[\sigma^2_{t+1} \mvert
\F_t\right]}{\Var\left[r_{t+1} \mvert \F_t\right]}$, which enables our approximation of $\sigma^2_{t+1}$ by the
realized volatility.

\section{Empirical Results}

\section{Conclusion}

\clearpage

\phantomsection
\addcontentsline{toc}{section}{References}
\printbibliography
\clearpage

\begin{appendices}

\section{Assumptions}

    In what follows, three sets of drifting sequences $\lbrace \gamma_T \rbrace$ are key. 
    
    \begin{defn}{Drifting Sequence Parameter Spaces}
        \begin{align}
            \Gamma\left(\gamma_0\right) &\coloneqq \left\lbrace \left\lbrace \gamma_T \in \Gamma \right\rbrace
            \mvert \gamma_T \to \gamma_0 \in \Gamma \right\rbrace\\ 
            \Gamma(\gamma_0, 0, b) &\coloneqq \left\lbrace \lbrace \gamma_T \rbrace \in \Gamma(\gamma_0) \mvert
            \phi_0 = 0\ \text{and}\ \sqrt{T} \phi_T \to b \in (\R \cup \lbrace \pm \infty) \right\rbrace \\
            \Gamma(\gamma_0, \infty, b_0) &\coloneqq \left\lbrace \lbrace \gamma_T \rbrace \in \Gamma (\gamma_0)
            \mvert \sqrt{T} \norm{\phi_T} \to \infty\ \text{and}\ \frac{\phi_T}{\norm{\phi_T}} \to b_0
            \right\rbrace 
        \end{align}
    \end{defn}
    
    These are the standard GMM regularity conditions appropriately adjusted for the lack of identification when
    $\phi =0$.
    
    \begin{assump}[GMM 1]\label{ass:GMM1}
    \begin{assumplist}
        \item If $\phi_0=0$, $\sampmom(\eta)$ and $\W_{T}(\eta)$ do not depend on $\pi$ for all $\eta \in \Eta$,
            for all $T \geq 1$, and for all $\gamma^{*}\in \Gamma.$ 
            \label{ass:GMM1a}
        \item If $\lbrace \gamma_{T} \rbrace \in \Gamma\left(\gamma_0\right)$, $\sup_{\eta \in \Eta}
            \norm*{\sampmom(\eta) - \E\left[g\left(\eta \mvert \gamma_0\right)\right]} \pto 0$ and $\sup_{\eta
            \in \Eta} \norm{\W_{T}(\eta)-\E\left[\W\left(\eta \mvert \gamma_0\right)\right]} \pto 0$.
            \label{ass:GMM1b}
        \item When $\phi_0 = 0$,  $g_0\left(\phi, \zeta ,\pi \mvert \gamma_0\right) = 0$ if and only if $\phi
            =\phi_0$ and $\zeta = \zeta_0$ for all $\pi \in \Pi$ and for all $\gamma_0 \in \Gamma.$
            \label{ass:GMM1c}
        \item When $\phi_0 \neq 0$, $g_0\left(\eta \mvert \gamma_0\right)=0$ if and only if $\eta =\eta_0$ for all
            $\gamma_0 \in \Gamma.$
            \label{ass:GMM1d}
        \item  $g_0\left(\eta \mvert \gamma_0\right)$ is continuously differentiable in $\eta $ on $\Eta$ with
            partial derivatives with respect to $\eta$ and $\xi$ denoted by $g_{\eta}\left(\theta \mvert
            \gamma_0\right) \in R^{k\times d_{\eta }}$ and $g_{\xi }\left(\eta \mvert \gamma_0\right)\in R^{k\times
            d_{\xi }}$, respectively.
            \label{ass:GMM1e}
        \item $\W\left(\eta \mvert \gamma_0\right)$ is continuous in $\eta$ on $\Eta$ for all $\gamma_0\in
            \Gamma$.  \label{ass:GMM1f}
        \item $0 < \lambda_{\min}(\W\left(\xi_0, \pi \mvert \gamma_0\right))\leq \lambda_{\max }(\W\left(\xi_0,\pi
            \mvert \gamma_0\right)) < \infty$, $\forall \pi \in \Pi$, for all $\gamma_0 \in \Gamma$.
            \label{ass:GMM1g}
        \item $\lambda_{\min} (g_{\xi}\left(\xi_0,\pi \mvert \gamma_0\right)^{\prime} \W\left(\xi_0,\pi \mvert
            \gamma_0\right)g_{\xi }\left(\xi_0,\pi \mvert \gamma_0\right))>0$, for all $\pi \in \Pi$,  and for all 
            $\gamma_0 \in \Gamma$ with $\phi_0=0.$
            \label{ass:GMM1h}
        \item$\Xi(\pi)$ is compact for all $\pi \in \Pi$, and both $\Pi$ and $\Eta$ are compact.
            \label{ass:GMM1i}
        \item For all $\epsilon > 0$, there exits a $\delta > 0$ such that $d_{H}\left(\Xi \left(\pi_{1}\right),
            \Xi \left( \pi_{2}\right) \right) < \epsilon$ for $\pi_{1}, \pi_{2} \in \Pi$ with
            $\norm*{\pi_{1}-\pi_{2}} < \delta$, where $d_{H}\left( \cdot \right)$ is the Hausdorff metric.
            \label{ass:GMM1j}
    \end{assumplist}
    \end{assump}
    
    
    
    \begin{assump}[GMM 2*]\label{ass:GMM2}
    \begin{assumplist}
        \item $\sampmom(\eta)$ is continuously differentiable in $\eta$ for all $T \geq 1$. 
            \label{ass:GMM2a}
        \item If $\{\gamma_T\} \in \Gamma\left(\gamma_0, 0, b\right)$, $\sup_{\left\lbrace \eta \in \Eta \mvert
            \norm*{(\phi, \zeta')' - (\phi_T, \zeta_0')} \leq \delta_T \right\rbrace}
            \norm*{\frac{\partial}{\partial (\phi, \zeta')'} \sampmom(\eta) - \E\left[\popmom_{(\phi,
            \zeta')'}(\eta) \mvert \gamma_0\right]} = o_p(1)$ for all deterministic sequences  $\delta_T \to 0$.
            \label{ass:GMM2b}
        \item  Let $\Eta_T \coloneqq \left\lbrace \eta \in \Eta \mvert \norm*{(\phi, \zeta_) - (\phi_T, \zeta_T)}
            \leq \delta_T \norm*{\beta_T}\, \text{and}\, \norm*{\pi - \pi_T} \leq \delta_T \right\rbrace$.  Let
            $\delta_T$ be a deterministic sequence that converges to zero.  If $\{\gamma_T \} \in
            \Gamma\left(\gamma_0, \infty, b_0\right)$, then we have the following asymptotic behavior.
            $\sup_{\eta \in \Eta_T} \norm*{\left(\frac{\partial}{\partial \eta'} \overline{g}_T -
            \E\left[g_{\eta}(\eta) \mvert \gamma_0\right]\right) \diag\left(1_{1+d_\zeta}',
            (1/\phi_T)_{d_{\pi}}'\right)}  = o_p(1)$. 
            \label{ass:GMM2c}
    \end{assumplist}
    \end{assump}
    
    Once we have \nameref{ass:GMM1} and \nameref{ass:GMM2}, we use \nameref{ass:GMM3} to derive the asymptotic
    distribution under weak and semi-strong identification.
    These conditions will be characterized using the expected derivative of the population moment conditions. 
    
    \begin{defn}
        \label{defn:moment_derivative_func}
        \begin{equation}
            K_{T,g}\left(\eta \mvert \gamma^{*}\right) \coloneqq  \frac{1}{T} \sum_{i=1}^T \frac{\partial}{\partial
            \phi^{*}} \E \left[ \popmom(W_T, \eta) \mvert \gamma^{*} \right]
        \end{equation}
    \end{defn}
    
    
    \begin{assump}[GMM 3]\label{ass:GMM3}
    \begin{assumplist}
        \item $\sampmom(\eta) = \frac{1}{T} \sum_{i=1}^T \popmom(W_T, \eta)$  for some function $\popmom(W_T,
            \eta) : \R^{k \times k} \times \Eta \to \R^k$.
            \label{ass:GMM3a}
        \item $\E\left[\popmom(W_T, \beta_0, \zeta^{*}, \pi) \mvert \gamma^{*} \right] = 0$ for all $\pi \in \Pi$ and
            for all $i \geq 1$ if $\gamma^{*} = \left(0,\zeta^{*}, \pi^{*}, \xi^{*} \right) \in \Gamma$.
            \label{ass:GMM3b}
        \item If $\{ \gamma_T \} \in \Gamma(\gamma_0, 0, b)$, $\frac{1}{\sqrt{T}} \sum_{i=1}^T \left(g(W_T,
            \zeta_{0,T}, \pi_T) - \E \left[g(W_T, \zeta_{0,T}, \pi_T)\mvert \gamma_T \right]\right)  \dto \N\left(0,
            \aleph(\gamma_0)\right)$, where $\aleph(\gamma_0)$ is a $k \times k$ matrix.
            \label{ass:GMM3c}
        \item 
            \label{ass:GMM3d}
            \begin{enumerate}
                \item  $K_{T,g}\left(\eta \mvert \gamma^{*}\right)$ exists for all $\{\eta, \gamma^{*} \} \in
                    \left(\Eta_{\delta} \times \Gamma_{0}\right)$ and for all $T \geq 1$.
                \item $K_{T,g}\left(\phi_T, \zeta_T, \pi \mvert \widetilde{\gamma}_T\right)$ uniformly converges
                    to some non-stochastic matrix-valued function  $K_{g}\left(0, \zeta_0, \pi \mvert
                    \gamma_0\right)$ over $\pi \in \Pi$ for all deterministic sequences $\{\phi_T, \zeta_T,
                    \widetilde{\gamma}_T \}$ satisfying $\widetilde{\gamma}_T \in \Gamma$, $\widetilde{\gamma}_T
                    \to \gamma_0 \coloneqq (0, \zeta_0, \pi_0, \xi)$, $\{\phi_T, \zeta_T, \pi \} \in \Eta$ and
                    $\{\phi_T, \zeta_T \} \to (0, \zeta_0)$.
                \item $K_g\left(\phi_0, \zeta_0, \pi \mvert \gamma_0\right)$ is continuous on $\Pi$ for all
                    $\gamma_0 \in \Gamma$ with $\phi_0 = 0$.
            \end{enumerate}
            \item $K\left(\phi_0, \zeta_0, \pi \mvert \gamma_0\right) = \popmom_{\phi, \zeta}\left(\phi_0,
                \pi\mvert \gamma_0\right) x$ for some $x \in \R^{1+d_{\zeta}}$ if and only $\pi =
                \pi_0$.\footnote{Since $\dim(\phi) = 1$, we can assume without loss of generality that the
                $\omega_0$ from \textcite{andrews2014Gmm} equals $1$.}
                \label{ass:GMM3e}
            \item If $\{ \gamma_T \} \in \Gamma(\gamma_0, 0, b)$, $\frac{1}{T} \sum_{i=1}^T
                \frac{\partial}{\partial \eta}  \E\left[ \popmom\left(W_T, \eta_T \right ) \mvert \gamma_T \right]
                \to \popmom_{\eta}\left(\eta_0 \mvert \gamma_0\right)$.
            \label{ass:GMM3f}
    \end{assumplist}
    \end{assump}

    \begin{defn}{\popmom*}
        \begin{equation}
            g_{\phi, \zeta}^{*}\left(\phi_0, \zeta_0, \pi_1, \pi_2 \mvert \gamma_0\right)  =
            \left[g_{\phi}\left(\phi_0, \zeta_0, \pi_1 \mvert \gamma_0\right)  , g_{\phi}\left(\phi_0, \zeta_0,
            \pi_2 \mvert \gamma_0\right) , g_{\zeta} \left(\phi_0, \zeta_0 \mvert \gamma_0\right)  \right]  \in
            \R^{k \times (d_{\zeta} + 2)}
        \end{equation}
    \end{defn}


    \begin{assump}[GMM 4]\label{ass:GMM4}
    \begin{assumplist}
        \item $\phi$ is a scalar.
            \label{ass:GMM4a}
        \item $g_{\phi, \zeta}^{*}\left(\phi_0, \zeta_0, \pi_1, \pi_2 \mvert \gamma_0\right)$ has full column
            rank. 
            \label{ass:GMM4b}
        \item $\aleph(\gamma_0)$ is positive definite for all $\gamma_0 \in \gamma $ with $\phi_0 = 0$. 
            \label{ass:GMM4c}
    \end{assumplist}
    \end{assump}

    \section{Proofs}

    \identifiedSet*

    \begin{proof}
    
    Since the exponential function is a strictly positive function, and we are considering a grid of $x$ values, a
    sufficient condition for $\rho, \delta$, \& $c$ to be identified is for the relevant rows of $\nabla a(x)$ and
    $\nabla b(x)$ to equal zero only at $\eta_{0}$ which are satisfied if $\rho, c, \delta > 0$.
    Testing if if $\phi$ and $\theta$ are identified is somewhat trickier. 
    Consider $\nabla \alpha(x)$. 
    Since we are using a grid of $x$'s, and the gradient of $\alpha$ is a nonlinear function of $x$, the first two
    rows of the \cref{eqn:alpha_gradient} imply $\phi$ is identified.
    
    \begin{equation}
        \label{eqn:alpha_gradient}
        \frac{\partial \alpha(x)}{\partial (\phi, \theta, c)'}  = \begin{bmatrix} \phi x^{2} + x \left(- 2 \beta
        \left(\theta_{2} - \frac{1}{2}\right) - \frac{\phi}{2 \sqrt{c} \left(\phi + 1\right)^{\frac{3}{2}}} +
        \frac{1}{\sqrt{c} \sqrt{\phi + 1}}\right) \\ x \left(- \phi^{2} + 1\right) \\ \frac{\phi x}{2
        c^{\frac{3}{2}} \sqrt{\phi + 1}} \end{bmatrix} 
    \end{equation}
    
    The top line of \cref{eqn:alpha_gradient} can be solved for $\theta$, which would create a local lack of
    identification for $\theta$.
    However, this creates a linear relationship between $\theta$ and the other parameters and $x$.
    However, since we are using multiple $x$'s we can avoid this issue.
    Consequently,  $\phi \in (-1,1], c > 0$, are sufficient to identify all of the parameters except for $\pi$,
    the price of volatility risk.
    We can use $a_{\PP}(x)$ to identify $\rho$.
    
    If $\phi = 0$, $\beta(x) = x \left(a (\pi + \alpha(\theta)) - a(\pi + \alpha(\theta))\right)$, and
    $\gamma(x) =  x \left(b (\pi + \alpha(\theta)) - b(\pi + \alpha(\theta))\right)$.
    These both  identically zero, and $\pi$ does not show up in anywhere  else in the criterion function.
    
    \end{proof}


\UllnStrongID*

\begin{proof}

    In this proof we rely heavily on the continuity of the moment conditions over their domain. 
    This can be seen from simple inspection since we assumed that $\phi_0 \geq \underline{\phi} -1$.
    Furthermore since $\Eta$ is compact, this continuity implies uniform continuity.
    
    For any positive definite weight-matrix by \textcite[Lemma 2.3]{newey1994large} our criterion function has
    a unique optimum.
    The data, $\sigma^2_{t+1}, r_{t+1}$, are ergodic and stationary.
    Since the moment conditions are not redundant the optimal (GMM) weight matrix $\W$ is positive definite. 
    In addition, $\popmom$ is continuous at each $\eta$, given the restrictions above and properties of
    characteristic functions imply that $\popmom$ is uniformly bounded. 
    For convenience, we assume that the space of $\eta$ is compact.
    This should not be an issue here because the parameters  are either a priori bounded, such as $\phi$ or we
    have substantial a priori knowledge on their plausible magnitudes.
    Hence, \textcite[Theroem 2.6]{newey1994large} implies our estimator is consistent.
    
    However, when we allow for weak identification late on, we need this convergence to be uniform. 
    One straightforward way to show this is to show that our criterion function is globally Lipschitz in a set of
    high probability. 
    
    The other issue is that we need the weight matrix to converge uniformly to its expectation.
    Since the moments are continuous functions over their domain as is the square function.
    This convergence is uniform if and only if the matrix inverse is continuous.
    
    Since we have a finite number of non-redundant moments, the minimum eigenvalue, \\
    $\lambda_{min}\left(\W\left(\phi_0, \zeta_0, \pi \mvert \gamma_0\right)\right) > 0$, and so the matrix inverse
    is uniformly continuous in $\gamma_0$ with respect to the Frobenius norm, which is the sum of the eigenvalues.
    (Recall, that the eigenvalues of the inverse are the inverse of the eigenvalues.)


\end{proof}

\InferenceWeakID*

\begin{proof}
We prove this result by showing that Assumptions GMM 1-4 are satisfied.

\begin{proofpart}
    \label{part:main_theorem_proof_part1}
    In this part, we show that \nameref{ass:GMM1} is satisfied. 
    To do this, we break \nameref{ass:GMM1}  down into three subsections.
    Assumptions \namedref{ass:GMM1a}, \namedref{ass:GMM1b}, \namedref{ass:GMM1c}, and \namedref{ass:GMM1d} state
    that when $\phi = 0$, the moment conditions contain no information regarding $\pi$, but when $\phi \neq 0$,
    the model is identified.
    This is what we show in \cref{lemma:IdentifiedSet}.
    We further showed the relevant uniform convergence to verity \namedref{ass:GMM1b} in
    \cref{lemma:UniformConvergenceStrongID}.
    
    The next two assumptions (\namedref{ass:GMM1e} and \namedref{ass:GMM1f}) are  technical conditions regarding
    the behavior of the moment conditions and weight matrix. 
    Since our moment conditions are derived from an infinitely-differentiable  characteristic function and the
    weight matrix is the optimal one, they both hold trivially.
    
    The third subsection of Assumption \nameref{ass:GMM1} concerns the weight matrix.
    Since we are using the inverse covariance matrix of valid non-redundant model, assumptions
    \namedref{ass:GMM1g} and \namedref{ass:GMM1h} automatically hold.
    
    The last two assumptions, \namedref{ass:GMM1i} and \namedref{ass:GMM1j} require that the parameter spaces do
    not vary too much with the parameters and are compact.
    Since $\Eta$ is compact, \namedref{ass:GMM1i} holds trivially, and since it has  has a product form,
    \namedref{ass:GMM1j}  holds trivially as well.
    
\end{proofpart}


\begin{proofpart}
    \label{part:mainTheoremProofPart2}

    In this section, we show that the derivatives of the moment conditions have the correct behavior locally to
    the true parameters.
    We have to do this for the different classes of drifting sequences.
    We will do this by verifitying \cref{ass:GMM2}.
    This is valid since \textcite{andrews2014Gmm} show that this is a sufficient condition for their Assumption
    GMM2, which is what we actually need. 

    Our moment conditions are sample averages of the characteristic function, they satisfy \namedref{ass:GMM2a}
    automatically. 
    Since characteristic functions are uniformly bounded, by the dominated convergence theorem we can interchange the 
    expectation and derivative operators. 
    Hence \namedref{ass:GMM2b} and \namedref{ass:GMM2c} are equivalent to the statements in terms of the moment
    conditions themselves mutatis mutandis.  
    In addition, sice the derivate is a linear operator, we can pull it outside of the norm.
    The reason that the uniform law of large numbers in \cref{part:main_theorem_proof_part1} does not trivially
    imply this result is because we are not considering sequences $\phi_T \to \phi_0$. 


    We create a mean value expansions around around $(\phi_0, \zeta_T, \pi_T)$ of the sample moment condition and
    around $(\phi_0, \zeta_0, \pi_0)$ for the population moment condition.
    (This is not the same in both cases, not is it the true parameter for the drifting sequence in the case of the
    sample moment condition.)
    In addition, also since we are considering continuous functions of compact spaces --- the $\delta_T$ ball in
    $\R^{\dim(\eta)}$ --- pointwise convergence implies uniform convergence, and so we only need to show pointwise
    convergence below.

    \begin{alignat}{2}
        & &&\norm*{\sampmom(\phi_T, \zeta_T, \pi_T) -  \E\left[\popmom(\phi_T, \zeta_T, \pi_T) \mvert
          \gamma_0\right] } \\ 
        \intertext{We take a mean value expansion of both functions around $\eta_0$. The point at which the
        derivative in the two locations is taken may not be the same.}
        &= &&\left\lVert \sampmom(\phi_0, \zeta_0, \pi_0) + \frac{\partial}{\partial (\phi, \zeta,
           \pi)}\sampmom(\widetilde{\phi}^s, \widetilde{\zeta}^s, \widetilde{\pi}^s)\left((\phi_0, \zeta_0, \pi_0)
            - (\phi, \zeta, \pi)\right)\right. \\
        &  &&\quad \left. - \E\left[\popmom(\phi_0, \zeta_0, \pi_0) \mvert \gamma_0\right] + 
           \frac{\partial}{\partial (\phi, \zeta, \pi)} \E\left[\popmom(\widetilde{\phi}^p, \widetilde{\zeta}^p,
           \widetilde{\pi}^p)\mvert \gamma_0\right] \left((\phi_0, \zeta_0, \pi_0) - (\phi, \zeta,
           \pi)\right) \right\rVert \\ 
        \intertext{By the triangle inequality.}
        &\leq && \norm*{\sampmom(\phi_0, \zeta_0, \pi_0) - \E\left[\popmom(\phi_0, \zeta_0, \pi_0) \mvert
           \gamma_0\right]}  \\
        &+  && \norm*{\frac{\partial}{\partial (\phi, \zeta)}\sampmom(\widetilde{\phi}^s, \widetilde{\zeta}^s,
          \widetilde{\pi}^s)\left((\phi_0, \zeta_0)  - (\phi, \zeta)\right) -  \frac{\partial}{\partial (\phi,
          \zeta)} \E\left[\popmom(\widetilde{\phi}^s, \widetilde{\zeta}^p, \widetilde{\pi}^p)\mvert \gamma_0\right]
          \left((\phi_0, \zeta_0) - (\phi, \zeta)\right)} \\
        &+  && \norm*{\frac{\partial}{\partial \pi}\sampmom(\widetilde{\phi}^s, \widetilde{\zeta}^s,
          \widetilde{\pi}^s)\left(\pi_0 - \pi\right) -  \frac{\partial}{\partial \pi}
          \E\left[\popmom(\widetilde{\phi}^p, \widetilde{\zeta}^p, \widetilde{\pi}^p)\mvert \gamma_0\right]
          \left(\pi_0 - \pi\right)} 
          \label{eqn:pi_derivative_term}
    \end{alignat}

    By the uniform law of law numbers in \cref{part:main_theorem_proof_part1}, the first equation is $o_p(1)$.
    For $(\phi_0, \zeta_0) - (\phi, \zeta)$ small, the middle term is bounded by the quantity below. 


    \begin{equation}
        \norm*{\frac{\partial}{\partial (\phi, \zeta)}\sampmom(\widetilde{\phi}^s, \widetilde{\zeta}^s,
        \widetilde{\pi}^s) -  \frac{\partial}{\partial (\phi, \zeta)} \E\left[\popmom(\widetilde{\phi}^s,
        \widetilde{\zeta}^p, \widetilde{\pi}^p)\mvert \gamma_0\right]} \norm*{(\phi_0, \zeta_0) - (\phi, \zeta)}
    \end{equation}

    The first term is almost surely bounded, and the second term is less than $\delta_T$ by assumption, and so the
    product is $o_p(1)$.

    The hard part is the third expression.
    Like before we can bound the pull the $\pi_0 - \pi$ term out of the equation.
    However, this term is no longer converges to zero.
    We will consider the two cases, separately.
    Throughout, we will refer to the behavior of the following equation, which bounds
    \cref{eqn:pi_derivative_term}.


    \begin{equation}
        \norm*{\frac{\partial}{\partial \pi}\sampmom(\widetilde{\phi}^s, \widetilde{\zeta}^s, \widetilde{\pi}^s) -
        \frac{\partial}{\partial \pi} \E\left[\popmom(\widetilde{\phi}^p, \widetilde{\zeta}^p,
        \widetilde{\pi}^p)\mvert \gamma_0\right] } \norm*{\pi_0 - \pi} 
        \label{eqn:pi_derivative_norm_bound}
    \end{equation}

    In general, $\tilde{\pi}^s$ and $\tilde{\pi}^p$ can be arbitrarily far apart.
    However, since $\sampmom \pto \E\left[\popmom \mvert \gamma_0\right]$, and the limiting value is independent of
    $\pi$, the derivative does not depend upon $\pi$ asymptotically by the dominated convergence theorem.
    This applies that the difference between the two derivatives evaluated at different $\pi$ converges to zero.
    Consequently, \cref{eqn:pi_derivative_norm_bound} is $o_p(1)$ and we have shown \namedref{ass:GMM2b}.

    \begin{equation}
        \norm*{\abs*{\frac{\partial}{\partial \pi}\sampmom(\widetilde{\phi}^s, \widetilde{\zeta}^s,
        \widetilde{\pi}^s) - \frac{\partial}{\partial \pi} \E\left[\popmom(\widetilde{\phi}^p,
        \widetilde{\zeta}^p, \widetilde{\pi}^p)\mvert \gamma_0\right]} (1, 1, T)} \norm*{\pi_0 - \pi} 
        \label{eqn:pi_derivative_rescaled_bound}
    \end{equation}

    If we consider the setup in \namedref{ass:GMM2c}, $\tilde{\pi}^s$ and $\tilde{\pi}^p$ are now close together.
    However, we need to show that \cref{eqn:pi_derivative_rescaled_bound} is $o_p(1)$.
    Since we are considering the limiting behavior of a function with a continuous derivative, we can assume that
    the derivative is uniformly bounded without loss of generality. 
    (The constant might depend upon the true value, but not $T$.)
    By a Taylor series expansion of $\sampmom$ around the true value, $\frac{\widetilde{\pi}^p}{\sqrt{T}} \to 0$,
    and $\norm{\pi_0 - \pi} \propto \delta_T$ the result follows.

\end{proofpart}

\begin{proofpart}
    \label{part:mainTheoremProofPart3}

    Assumption \namedref{ass:GMM3a} is trivially satisfied,  and we showed that \namedref{ass:GMM3b} is satisfied
    in \cref{sec:GMM}.  
        
    The conceptual idea driving the reulsts this section is that moment conditions for each $T$ minus their
    conditional exceptions converge to a normal random variable.
    In other words, we are almost in a standard triangular C.L.T.\@ setup with weak time-dependence.

    In particular, both $\sigma^2_t$ and $r_t$ are infinitely differentiable functions of the innovations to
    the volatility and return processes and $\popmom$ are infinitely differentiable functions of $r_{t+1}$ and
    $r_t$ and the innovations are i.i.d.\@ across time by assumption.
    (Note, i.i.d.\@ implies strong mixing of any size.)
    Consequently, $\popmom$ is near epoch dependent (NED) of any size as defined in
    \textcite{andrews1991empirical}.  
    (Take $s>2.$) 
    By \textcite[Theorem 3]{andrews1991empirical}, we have the necessary finite-dimensional convergence in
    distribution to a Gaussian random variable. 

    We now show that \namedref{ass:GMM3d} holds.
    Clearly, $K_{t,g}\left(\eta \mvert \gamma^{*}\right)$ always exists.
    It uniformly converges because the derivatives of the moments are continuous functions of the data and the
    parameters, the process is ergodic, and the characteristic function lives on a compact set.
    It is also clearly continuous.
    By the dominated convergence theorem, we can exchange the derivative and expectations.
    In addition, since \popmom\ does not depend upon $\phi_T$, the limiting behavior is independent of the value
    of $\phi_T$.
    Hence \namedref{ass:GMM1d} holds.
    
    \namedref{ass:GMM3e} says the derivative of the moment function does not depend the true parameter $\phi$ if
    and only if  $\pi =  \pi_0$. 
    We showed this in the proof of \cref{part:mainTheoremProofPart2}.
    \namedref{ass:GMM3f} follows directly from the compactness of the parameter space and the continuity of the
    cross derivatives of \popmom\ by the dominated convergence theorem.

\end{proofpart}

\begin{proofpart}
    \label{mainTheoremProofPart4}
   
    \namedref{ass:GMM4a} holds trivially.
    We verified \namedref{ass:GMM4b} when we showed that $\pi$ and $\zeta$ are strongly identified
    (\cref{lemma:IdentifiedSet}).
    \namedref{ass:GMM4c} holds because the identification conditions do not create any singularity in the
    asymptotic covariance matrix. 

\end{proofpart}

\end{proof}





\end{appendices}


\end{document}



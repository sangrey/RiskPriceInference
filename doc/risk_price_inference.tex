\documentclass[11pt, letterpaper, twoside, final]{article}
\usepackage{risk_price_inference}
\addbibresource{riskpriceinference.bib}

\author{Xu Cheng\thanks{University of Pennsylvania, The Ronald O. Perlman Center for Political Science and
    Economics, 133 South 36th Street, Philadelphia, PA 19104, \href{mailto:xucheng@upenn.edu}{xucheng@upenn.edu}}
    \and 
    Eric Renault\thanks{Brown University, Department of Economics -- Box B, 64 Waterman Street, Providence, RI
    02912, \href{mailto:eric_renault@brown.edu}{eric\_renault@brown.edu}}
    \and 
    Paul Sangrey\thanks{University of Pennsylvania, The Ronald O. Perlman Center for Political Science and
    Economics, 133 South 36th Street, Philadelphia, PA 19104, \href{mailto:paul@sangrey.io}{paul@sangrey.io}}}
    
\title{Identification and Inference for Risk Prices using Equity Data}

\date{\today}

\begin{document}

\begin{titlepage}


    \maketitle
    \thispagestyle{empty}
    \addtocounter{page}{-1}

    \begin{abstract} \singlespacing \noindent 
        The risk-return trade off is a central question in modern asset pricing. 
        Even though the theoretical literature cohesively argues there should be a strong positive correlation
        between expected returns and volatility, empirically measuring it has proven difficult. 
        In addition, the option pricing literature shows that volatility enters as its own risk factor, not just
        as a predictor of returns.
        Even obtaining point estimates of these risk prices in the presence of contemporaneous correlation between
        returns and volatility, the  volatility feedback effect, has proven delicate.
        We develop methods to provide  valid confidence intervals for the price of volatility and equity risk from
        equity data directly that account for this weak identification of the various parameters. 
        We do this by adapting the results in weak identification literature that uses drifting sequences in order
        to be robust to the data's identification strength. 
    \end{abstract}

    \jelcodes{C12, C14, C38,  C58, G12}

    \keywords{identification, robust inference, stochastic volatility, leverage, equity risk premium, volatility
    risk premium, risk price, confidence set, asymptotic size}

\end{titlepage}

\phantomsection
\addcontentsline{toc}{section}{Introduction}

Modern finance is all about the risk return trade offs that investors face and how to optimally respond to them. 
In particular, the central question of asset pricing is what drives expected returns.
Standard economic theory predicts you must compensate investors with higher expected returns when they face more
risk.
In other words, we would expect a positive relationship between the mean and volatility of returns.
In fact, in \gentextcites{sharpe1964capital,lintner1965security} capital asset pricing model (CAPM) the expected
return varies proportionally with the volatility. 
In other words, we have a constant price of volatility risk and expected returns are perfectly correlated with
volatility.
Even in more complicated models such as the long-run risk model and rare disasters models find a very close
relationship between variance and mean of returns \parencite{bansal2014volatility, wachter2013can}.

Consequently, an important empirical question is what is the precise magnitude of this relationship.
However, unlike the consensus in the theoretical literature, the empirical literature has found pinning down this
relationship quite difficult.
Not only has its magnitude proven difficult to determine, but various estimates even differ in sign,
\parencite{lettau2010measuring}, .

The empirical literature, which we examine in more detail in the literature review, has focused on point estimates
of this magnitude. 
However, if individual investors are ambiguity averse as in \textcite{hansen2001robust, jiu2012ambiguity}, they
will care not just about how the representative investor prices volatility but also their uncertainty regarding
this estimate. 
Furthermore, when economists calibrate models, they need to know how precisely the data determine the parameters
they are calibrating.
If the need to alter the parameter value slightly in order to make their model perform well, are they bringing in
more restrictions or data to more precisely determine the parameter of interest or are they using a value that the
data tell us is incorrect?

Clearly, as obtaining precise believable point estimates of the price of volatility risk has proven quite
difficult, we should expect doing valid inference to be even more delicate.
To the best of our knowledge, this is the first paper to directly tackle this question.
Various authors report confidence intervals as well as their point estimates.
However, they do not take into account the weak identification that makes getting the point estimate difficult in
computing these estimates.

Why is it that measuring this price is so difficult when the theoretical literature is so cohesive?
Econometrically, it is because the volatility price is weakly identified, as in \textcite{andrews2012estimation},
in that the strength of the identification of the price depends upon the value of other parameters. 
This obviously begs the question --- what are these parameters? 

To estimate the risk prices, there are three different phenomena that must be distinguished.
First, the econometrician must disentangle the volatility feedback effect (leverage) which is a contemporaneous
relationship between the volatility and returns from the risk premium, which is a relationship between volatility
and expected returns. 
It is not a contemporaneous relationship, but rather a predictable one. 

Second, and just as important.
In the papers estimating the volatility expected-return relationship such as
\textcite{brandt2004relationship,lettau2010measuring} are implicitly using a one-factor model where volatility
is that factor.
However, the option pricing literature, such as  \textcite{christoffersen2013capturing}, has argued that in order
to match the option pricing data we need to include another factor.
In other there are two sources of risk that the investors face: what returns will do and what the volatility
will do.
One can view the loading on the equity risk to itself be a function of the volatility.
The econometrician can rewrite this as a two factor model with equity and volatility risk up to higher order
terms.
Since, the econometrician must disentangle these two effects, their ability to do this will depend upon various
other parameters.
As mentioned in the introduction,in CAPM we only have factor, and so clearly we cannot disentangle them.
However, most models of interest to economists nest CAPM as a special case. 
Consequently, for certain values of the parameters, the model has only one factor. 
This directly implies that we cannot separately identify the two risks for some values of the other parameters.
More generally, the identification strength of the risk prices depends upon the other parameters.
The weak identification literature shows that in these scenarios, the finite-sample distributions of the estimates
are highly nonstandard, and the usual asymptotic approximations do not perform well.


Hopefully, the reader has a high-level understanding of the issue in at hand, and why simple econometric
procedures does not provide a useful guide to the true uncertainty regarding the parameters.
The obvious next step is considering how we need to do this in practice.
Since the contribution of this paper is in terms of methodology and empirical results, we will take a model from
the literature that has the various components, instead of developing our own asset or option pricing model.
In particular, we take the model from \textcite{khrapov2016affine} and use it to estimate the relevant parameters. 

This model has a few nice features. 
First, it has both equity and volatility prices and a leverage effect. 
As such, it is the natural discrete time analogue of the \textcite{heston1993closedform} option pricing model. 
It has an exponentially affine stochastic discount factor  and shares with \textcite{heston1993closedform} the
advantage of having a structure preserving change of measure between the physical and risk-neutral models.
By doing our analysis in discrete-time we are able to more directly compare our results to risk-premia estimates
outside of the option pricing literature and the jumps in high-frequency innovations will not dramatically affect
our results.  
If we were to use a diffusion process in continuous time, we would be severely counterfactually constraining the
higher-order  moments of the process in way that would likely bias our inference. 

As far as estimation is concerned, we follow \textcite{khrapov2016affine} in using a finite version of spectral
General Method of Moments (GMM).
Effectively, we take some moments implied by the characteristic function of returns and use them in a standard GMM
setup.
The data we use are the bivariate series $\begin{pmatrix} r_{t+1}, \sigma^2_{t+1} \end{pmatrix}$.
$r_{t+1}$ is the daily return on some asset, and we use its associated realized volatility for $\sigma^2_{t+1}$.
We go into further detail in \cref{sec:data} regarding how we obtained it, the time-span covered, and so on.

\section{Literature Review}\label{sec:lit_review}


\section{The Model}\label{sec:model}

\addtocounter{subsection}{1}

We estimate the prices of some factors using moment conditions derived from an option pricing model. 
As is standard in that literature, we will do  this by specifying the physical and risk-neutral measures and their
relationship, i.e.\@ the stochastic discount factor or pricing kernel.
Let $\F_t$ be the representative investor's information set at time $t$, and $P_t$ be the price on the asset in
question, with associated return $r_{t+1}$ and volatility  $\sigma^2_{t+1}$.

Given $\F_{t}$, the vector $\left( r_{t+1},  \sigma^2_{t+1}\right)$ is drawn from some process --- $\PP$ --- the
physical measure. 
We can further define the risk neutral measure --- $\QQ$ ---  as the process that makes $P_t$ a martingale, i.e.\@
the following holds. 
The advantage of defining $\QQ$ is that for some payoff function --- $f$  -- that is a function of future return
--- $r_{t+1}$ and volatility $\sigma^2_{t+1}$ and potentially depends upon the current information available --
$\F_t$, we can price this payoff as its expectation with respect to $\QQ$.
In other  words, the price of $f(r_{t+1}, \sigma^2_{t+1}, \F_t)$ satisfies for all $t$ and for all $f$.
This is useful because we can choose $f$ to make our estimation convenient.

\begin{equation}
    P_t(f) = \E_{\QQ}\left[ f\left(r_{t+1}, \sigma^2_{t+1}, \F_{t}\right)  \mvert \F_{t}\right]
\end{equation}

Since $P_t(f), r_{t+1}$ and $\sigma_t^2$ are observable, if we specify a model for $\F_t$ in terms of observable
(to the econometrician) variables, this provides a moment condition that we can use. 
However, this condition does not identify everything we wish to estimate, in particular it does not identify the
risk prices. 


To see this, we complete the model by defining the stochastic discount factor --- $M_{t, t+1}$ --- as the
Radon-Nikodym derivative between the $\PP$ and $\QQ$ measures. 
No arbitrage guarantees that this will exist, \parencite{harrison1978martingales}.
Since risk prices arise from investors' demand for compensation to hold risk, i.e.\@ their risk-aversion, here is
where they show up in the $\QQ$ measure. 
(We collect the parameter of interest into a vector --- $\eta$.)


\begin{defn}{Asset Pricing Moments}
    \begin{equation}
        P_t(f)  = \E_{\QQ} \left[f\left(r_{t+1}, \sigma^2_{t+1} , \F_t\right) \mvert \F_t \right] =
        \E_{\PP}\left[M_{t,t+1}(\eta)f\left(r_{t+1}, \sigma^2_{t+1}, \F_t\right) \mvert \F_t \right] 
    \end{equation}
\end{defn}

\subsection{Dynamics}\label{sec:dynamics}

To specify the model we must specify what the $\PP$ and $\QQ$ measures are and what $M_{t, t+1}(\eta)$ is.
We start by specifying the $\PP$ measure.

Following \textcite{khrapov2016affine}, we specify the joint dynamics of $\left(r_{t+1}, \sigma^2_{t+1}\right)$
under $\PP$ using Compound Autoregressive models (Car) of \textcite{darolles2006structural}.
These models are autoregressive models for the Laplace transform of the process.
The leading parameterization is the autoregressive gamma process (ARG) of \textcite{gourieroux2006autoregressive}.

In particular, we assume that the variables are first-order Markov and there is no Granger causality from return
to the volatility and that returns are serially independent given the volatility path.
In other words, the volatility drives all of the dynamics of the process.
Note, we do allow $\sigma^2_{t+1}$ and $r_{t+1}$ to be contemporaneously correlated, which they are in the data. 

We construct the following conditional Laplace transforms as follows.
This is well-defined because we can define the true Laplace transform as the expectation of a specific function,
and then integrate out all of the unknown variables using their marginal distributions.
Hence, we can represent out model under the physical  as follows for some functions $a_{\PP}, b_{\PP},
\alpha_{\PP}, \beta_{\PP}$, and $\gamma_{\PP}$ for all $u$ in its domain.

\begin{restatable}[The Limited Information Model under the Physical Measure]{defn}{physicalMeasureModel}
    \label{defn:physical_model}
    \begin{align}
        \E_{\PP} \left[\exp(-u \sigma^2_{t+1}) \mvert \sigma^2_t \right] &= \exp\left( - a_{\PP}(u) \sigma^2_t -
        b_{\PP}(u) \right) \\
        \E\left[\exp(-u r_{t+1}) \mvert \sigma^2_t,  \sigma^2_{t+1}\right] &= \exp\left(- \alpha_{\PP}(u)
        \sigma^2_{t+1} - \beta_{\PP}(u)\sigma^2_t - \gamma_{\PP}(u) \right) 
    \end{align}
\end{restatable}

The model in \cref{defn:physical_model} is parameterized using the functions mentioned, and at this level of
generality is fully nonparametric. 
We now derive the moment conditions that govern the physical measure. 
We follow \textcite{khrapov2016affine} closely.
Recall the model under the physical measure. 

% \subsection{Derivation of the Moment Conditions}\label{sec:deriving_mom_conds}

Note, by the physical-measure functions satisfy the following constraint by definition.

\begin{equation}
    a_{\PP}(0) = b_{\PP}(0) = \alpha_{\PP}(0) = \beta_{\PP}(0) = \gamma_{\PP}(0) 
\end{equation}

We can differentiate the Laplace transform at zero to compute the moments.
The logarithm of the Laplace transform above is a conditional cumulant generating function.
Since, we are only using the mean and variance, we can differentiate this function  and evaluate the
resultant function at zero to construct formulas for these moments.

We start by computing $\E\left[\sigma^2_{t+1} \mvert \F_t\right]$ and $\Var\left[\sigma^2_{t+1} \mvert
\F_t\right]$


\begin{align}
    \E\left[\sigma^2_{t+1} \mvert \sigma^2_t \right]  &= a_{\PP}'(0) \sigma^2_t  + b_{\PP}'(0) \\
    \Var\left[\sigma^2_{t+1} \mvert \sigma^2_t \right]  &= -a_{\PP}''(0) \sigma^2_t  - b_{\PP}''(0) 
\end{align}

To simplify the exposition, we will reparameterize these equations. 
Essentially, we will give these derivatives names.

\begin{defn}{Volatility Dynamics Parameters}
    \begin{align}
        \rho &\coloneqq a_{\PP}'(0) \in (0,1) \\
        c &\coloneqq - \frac{a_{\PP}''(0)}{2 a_{\PP}'(0)} > 0 \\
        \delta &\coloneqq -2 \frac{a_{\PP}'(0) b_{\PP}'(0)}{a_{\PP}''(0)} > 0 \\
        \omega &\coloneqq -4 \frac{b_{\PP}''(0) [a_{\PP}'(0)]^2}{[a_{\PP}''(0)]^2}
    \end{align}
\end{defn}

To interpret these factors, can derive a series of moments. 
$\rho$ is a persistence parameter.
$c$ is a scaling parameter.
We will also assume that the unconditional mean of $\sigma^2_t$ agrees with that of $\sigma^2_{t+1}$, and so we can use
the law of iterated expectation and the law of total variance to determine the moments.


\begin{align}
    \E\left[\sigma^2_{t+1} \mvert \sigma^2_t \right]  &= \rho \sigma^2_t  + c \delta\\
%
    \E\left[\sigma^2_{t+1}\right]  &= \frac{c \delta}{1 - \rho} \\
%
    \Var\left[\sigma^2_{t+1} \mvert \sigma^2_t \right]  &=  2 c \sigma^2_t  + c^2 \omega \\
%
    \Var\left[\sigma^2_{t+1} \right]  &=  \frac{c^2}{1- \rho^2} \left(\frac{2 \rho \delta}{1 - \rho}  + \omega
        \right) 
\end{align}

Clearly, we can identify all four of these parameters from the volatility data as long as the parameters are in
the interior of their domains.

\subsubsection{The moments of the return}

We then turn to computing the moments of the return distribution. 

\begin{align}
    \E\left[r_{t+1} \mvert \sigma^2_t, \sigma^2_{t+1}\right]  &= \alpha_{\PP}'(0) \sigma^2_{t+1}  + \beta_{\PP}'(0)
    \sigma^2_t - \gamma_{\PP}'(0) \\
    \Var\left[r_{t+1}^2 \mvert \sigma^2_t, \sigma^2_{t+1}\right]  &= -\alpha_{\PP}''(0) \sigma^2_{t+1}  -
    \beta_{\PP}''(0) \sigma^2_t - \gamma_{\PP}''(0)
\end{align}

We can then use the law of total variance to compute $\Var\left[r_{t+1} \mvert \F_t\right]$.

\begin{equation}
    \Var\left[r_{t+1} \mvert \sigma^2_t\right]  = -\left( \alpha_{\PP}''(0) \E \left[\sigma^2_{t+1} \mvert \sigma^2_t \right]
    + \beta_{\PP}''(0) \sigma^2_t + \gamma_{\PP}''(0)\right) + {\alpha_{\PP}'(0)}^2 \Var\left[\sigma^2_{t+1}
    \mvert z_{t} \right]
\end{equation}

Before, we discuss how we parameterize the Laplace function for the return process, we will consider the
risk-neutral measure.
We assume that the risk-neutral measure is structure preserving.
In other words, the Laplace functions maintain the same structure for some functions $a_{\QQ}, b_{\QQ},
\alpha_{\QQ}, \beta_{\QQ}$, and $\gamma_{\QQ}$.
This is a relatively weak assumption because the absolute continuity of the two measures places strong
restrictions on deviations from this case.
For example, in the continuous-time limit this structure-preserving relationship always holds.

\begin{defn}{The Model under the Risk-Neutral Measure}
    \label{defn:risk_neutral_model}
    \begin{align}
        \E_{\QQ} \left[\exp(-u \sigma^2_{t+1}) \mvert \sigma^2_t\right] &= \exp\left( - a_{\QQ}(u)
            \sigma^2_{t} - b_{\QQ}(u) \right) \\
        \E\left[\exp(-u r_{t+1}) \mvert \sigma^2_{t},  \sigma^2_{t+1}\right] &= \exp\left(- \alpha_{\QQ}(u)
        \sigma^2_{t+1} - \beta_{\QQ}(u)\sigma^2_{t} - \gamma_{\QQ}(u) \right) 
    \end{align}
\end{defn}

Since the Laplace transform for $r_{t+1}$  is exponentially affine under both $\PP$ and $\QQ$, the measure change
between them is also exponentially affine.
If we let $\pi$ be the price of volatility risk and $\theta$ be the price of equity risk, we can write down the
change of measure as follows.
We let $r_{f,t}$ be the risk-free rate.
It deterministically discounts the prices but plays no further role in the analysis.


\begin{defn}{The Stochastic Discount Factor}
    \begin{equation}
        M_{t,t+1}(\pi, \theta) = \exp\left(-r_{f,t}\right) \exp\left(m_{0}(\pi, \theta) + m_1(\pi, \theta)
        \sigma_t^2 - \pi \sigma^2_{t+1} - \theta r_{t+1}\right) 
    \end{equation}
\end{defn}


Before we show how the risk prices $\pi, \theta$ show up in the equations above, we will introduce some parameters
that govern the dynamics of the volatility process.



\subsubsection{\texorpdfstring{Parameterizing the mean and variance of $r_{t+1}$}{Parameterizing the return's
mean and variance}}\label{sec:deriving_sdf_functions}


In this section, we derive the formulas for the mean and variance of the expected return in terms of the risk
prices.

\begin{equation}
    M_{t,t+1}(\pi, \theta) = \exp(-r_{f,t}) \exp \left( m_0(\theta, \pi) + m_1(\theta, \pi) \sigma_t^2 - \pi
    \sigma^2_{t+1} - \theta r_{t+1}\right)
\end{equation}

Since the SDF must integrate to $1$ once we discount using the risk-free rate $r_{f,t}$\, we have the following
restrictions.


\begin{gather}
    \E_{\PP}\left[\exp\left(m_0(\theta, \pi) + m_1(\theta, \pi) \sigma^2_t - \pi \sigma^2_{t+1} - \theta r_{t+1}
    \right) \mvert \F_t \right] = 1 \\
    \intertext{By the law of iterated expectations.}
    \E_{\PP}\left[ \E\left[\exp\left(m_0(\theta, \pi) + m_1(\theta, \pi) \sigma^2_t - \pi \sigma^2_{t+1}\right)
        \exp\left( - \theta r_{t+1}\right) \mvert \F_t, \sigma^2_{t+1} \right]\right] = 1 \\
    \intertext{The second term is the Laplace transform of $r_{t+1}$.}
    \E_{\PP}\left[\exp\left(m_0(\theta, \pi) + m_1(\theta, \pi) \sigma^2_t - \pi \sigma^2_{t+1} \right)
        \exp(-\alpha_{\PP}(\theta) \sigma^2_{t+1} - \beta_{\PP}(\theta) \sigma^2_{t} - \gamma_{\PP}(\theta_2)
        \mvert \F_t \right] = 1 \\
    \intertext{Reorganizing terms.}
    \E_{\PP}\left[\exp\left(m_0(\theta, \pi) + m_1(\theta, \pi) \sigma^2_t - \beta_{\PP}(\theta) \sigma^2_{t} -
        \gamma_{\PP}(\theta_2) \right) \exp(-\left(\pi + \alpha_{\PP}(\theta)\right) \sigma^2_{t+1}) \mvert \F_t
        \right] = 1 \\ 
    \intertext{Substituting in the Laplace transform for $\sigma^2_{t+1}$.} 
    \E_{\PP}\left[\exp(m_0(\theta, \pi) + m_1(\theta, \pi) \sigma^2_t - \beta_{\PP}(\theta) \sigma^2_{t} -
        \gamma_{\PP}(\theta_2)  - a_{\PP}(\pi + \alpha_{\PP}(\theta)) - b_{\PP}(\pi + \alpha_{\PP}(\theta))
        \mvert \F_t \right] = 1 
\end{gather}

Since this equality must hold for all $\sigma^2_{t}$, we can solve for both $m_0(\theta, \pi)$ and $m_1(\theta,
\pi)$ in terms of the functions determining the physical measures.

\begin{align}
    m_0(\theta, \pi)  &= \gamma_{\PP}(\theta) + b_{\PP}(\alpha_{\PP}(\theta) + \pi) \\
    m_1(\theta, \pi)  &= \beta_{\PP}(\theta) + a_{\PP}(\alpha_{\PP}(\theta) + \pi) 
\end{align}

The reason that this is interesting is all of the right-hand side variables  are in terms of the physical
measure.
In other words, we do not need, at least in principle, option data to estimate the SDF.
In particular, we can determine the risk prices from equity prices if the identified parts of the $\alpha_{\PP},
\beta_{\PP}$ etc.\@ functions are \textquote{sufficiently} invertible.
(I.e.\@ the matrix of their derivatives satisfies the appropriate non-singularity conditions.)

However, before we do that we are going to consider the difference in the physical measures when we condition on
the volatility and when we do not.
In discrete-time this is straightforward.
The distribution of $ r_{t+1} \vert \F_t$ is not the same as $r_{t+1} \vert \F_t, \sigma^2_{t+1}$.
In continuous-time, the issue becomes more difficult.
In particular, what does it mean to condition on future volatility? 
In a sense, you want to condition on $\lim_{\Delta \to 0^{+}} \sigma^2(t+\Delta)$, but when you time-aggregate
it is not obvious how the difference between a right and left limits affects the resulting integral.
We discuss this in the next section.

\subsection{Characterizing the Structure of the Identification Problem in Continuous-Time}

In this section, we have the following two continuous-time processes.
The goal is to show that by using the first-two conditional moments in our identification strategy, the parameters
we are estimating converge to the true equity risk and volatility risk even in the presence of higher-order
time-variation in the price processes.
Our moment conditions we use in the estimation project the risk prices onto the information set generated by
$\E\left[r_{t+1}\mvert \F_{t-1}\right]$  and $\E\left[\sigma^2_{t-1} \mvert \F_{t-1}\right]$.
What we do here is the equivalent projection in population.
This procedure will give us a series of estimates $\hat{\pi}, \hat{\theta}$, etcetera.
These estimates will have some limiting values, as long as the moment conditions are sufficiently non-collinear.
This section characterizes precisely the role these limiting values play in population.
For example, we show that the risk-prices are the actual risk prices under some conditions, which we make
explicit.

The way by which we do this is by representing the processes in continuous-time.
Since the assets we are characterizing are continuously tradable they must satisfy some continuous-time
no-arbitrage conditions. 
In other words, there exists a continuous-time DGP where the DGP we have been using so far is a discretization of. 
This is useful because we can work with the first two moments of the process effectively without loss of
generality.
The linear-affine structure we have been using before will show up as certain terms being constant functions of
time in what follows.
The cost of doing this is that it requires more involved notation and more sophisticated mathematical tools.


It is not entirely obvious that volatility risk premia is even a well-defined concept if neither the volatility
nor the price process itself jumps.\footnote{In general, the predictable projection of the instantaneous
    volatility $\sigma^2(t-)$ is measurable with respect the filtration generated by the data $\F_t$. In addition,
    if both $\sigma^2(t)$ and $p(t)$ are continuous processes, their filtrations are predictable as well: $\F_t =
    \F_{t-}$. Consequently, all of four o the relevant filtrations coincide. Hence, changing filtrations is
    possible. It is not obvious how volatility risk and market risk can be distinguished if conditioning on
    volatility at time $t$ does not change the information set from $\F^p_{t-}$. The discrete-time
characterization of volatility risk is doing precisely this by using the log-Laplace transform.}
Consequently, we will adapt the following continuous-time data-generating process (DGP)
This is arguably the simplest DGP that allows for us to discuss these issues. 
We follow \textcite{sangrey2018jumps} in modeling the jumps as an integral with respect to a variance-gamma
process.
The constraint that $\sigma^2(t) > 0$, will imply some dependence between $\sigma^2(t), \sigma_{\sigma}(t)$, and
$\gamma_{\sigma}(t)$, which we do not rule out.



\begin{defn}{Continuous-Time DGP}
    \label{defn:cont_time_dgp}

    \begin{align}
        \dif p(t) &= \gamma(t) \dif t + \beta(t) \sigma^2(t-) \dif t + \sigma(t) \dif W(t)  \\
        \dif \sigma^2(s)  &= \mu_{\sigma}(t) + \sigma_{\sigma}(t) \dif W_{\sigma}(t) +
        \frac{\gamma_{\sigma}(t)}{\sqrt{2}} \dif \Lap(t) 
    \end{align}

\end{defn}

The other complication with \cref{defn:cont_time_dgp}, is that we split the drift into two parts $\gamma(t)\dif t$
and $\beta(t) \sigma(t-) \dif t$. 
All of the terms here are finite-variation predictable processes.
It is not immediately obvious how to separate out $\gamma(t)$ and $\beta(t)$.
To get the correct discrete-time interpretation of the integrals of these processes, you can take the predictable
finite-variation (drift) of the ordinal process and project it onto $\sigma^2(t)$ without an intercept.
You can then define $\beta(t)$ to be the coefficient from this regression and $\gamma(t)$ to be the residual.
Intuitively, we are separating out the part of price drift driven by expectations of vitality from other sources
of variation.


We now define the relevant filtrations.
We will continue to use $\F_t$ to be the filtration generated by $p(t)$.
We will use $\F_{t-}$ to be the predictable filtration associated with $\F_t$, i.e.\@ the filtration generated by
the predictable processes adapted to $\F_t$.
In addition, we will define $\F_{\sigma^2, t}$ to be $\F_t$ augmented by the right-limits of the volatility. 
You can view it as $\lim_{\Delta to 0} \F_t \cup \F^{\sigma^2}_{t+\Delta}$, where $\F^{\sigma^2}_t$ is the
filtrated generated by $\sigma^2(t)$.
In other words, we will look infinitesimally far into the future path of $\sigma^2(t)$ and add that information to
our filtration.

This is the continuous-time analogue of adding $\int_t^{t+1} \sigma^2(s) \dif s$ to your $\F_t$ information set.
In both cases you are adding the value of the volatility in the \textquote{next period} to the current information
set.
Another way to view this expansion of filtration is that in $\F_t$, $\int^t_{-\infty} \sigma^2(s) \dif s$ is an
optional process, and we are constructing the minimal filtration that makes $\int^t_{-\infty} \sigma^2(s) \dif s$
predictable.\footnote{The progressive enlargement of the filtrations we are considering is an augmentation of
    $\F_t$ with a series of honest times, since $\F^{\sigma^2}_{t}$ is generated by a left-continuous processes.
    Consequently, all of the adapted processes to $\F_t$ are semimartingales with respect to $\F_{\sigma^2,t}$,
    \parencite[Theorem C]{barlow1978study}.  We will further assume that all of the processes maintain the same
stochastic jump-diffusion structure under both filtrations. The relevant technical conditions under which this
structure is preserved is outside this scope of this paper.}


\subsubsection{Relating Change of Filtration to the Risk-Neutral Measure.}

To fully understand the price of volatility risk we need to understand how the physical change of measure
caused  by switching between $\F_t$ and $\F_{\sigma^2,t}$ interacts with the measure change caused from switching
between $\PP$ and $\QQ$.
The volatility risk price governs how pricing under the $\F_{t-}$ and $\F_{\sigma^2,t-}$ filtrations is related.
Note, we can price under both filtrations because they are predictable filtrations.

Moving forward we will rely on a  specification for risks that depends exclusively on the mean and volatility
(i.e.\@ the first two moments.)
If higher moments, such as skewness and kurtosis are also priced factors, as in \textcites{harvey2000conditional,
conrad2012exante, chang2013market},  and we used higher sample moments as well to determine the price of our
risk-factors our resulting estimates would be biased, likely substantially so. 

The question facing us is how to we prevent our model's potential misspecification regarding the risk prices of
higher-order moments from affecting our estimates of the volatility prices.
To answer this question, we turn to what our model specifies the continuous-time process, that our model is a
discretization of.

Imposing consistency with a continuous-time model is reasonable in our context because even though our model is
specified in discrete-time it is for an asset that is is continuously tradable.
A good discrete-time model for a continuously-tradable asset would is consistent with a continuous-time
generalization that rules our no-arbitrage.

Instead of using the log-Laplace transform as we did in discrete-time, we will tackle the change of measure
induced by the change of filtration for the log-price directly.
We will do by inducing some functions $\psi(t)$ and $\phi(t)$ which control the drift and volatility of $p(t)$
under $\F_{\sigma^2,t}$.
To be concrete, we will show that you can represent the log-price with respect to $\F_{\sigma^2,t}$ as follows,
where $\widetilde{M}_{\sigma}(t)$ has mean zero, and predictable quadratic variation  $t$.

\begin{restatable}[Log-Price Given Future Volatility]{theorem}{priceDGPGivenVol}
    \label{defn:logPriceGivenVol}

    Assume that volatility is an It\^{o} semimartingale without any predictable jumps, and the prices can be
    represented as follows.
    Further assume that all of the prices are locally-square integrable, and have the following representation.

    \begin{equation}
        \dif p(t) = \gamma(t) + \beta(t) \sigma^2(t) + \sigma(t) \sqrt{1 - \phi(t)^2} \dif W^p(t) + \phi(t)
        \sigma(t) \dif W^{\sigma}(t) 
    \end{equation}

    Then we can represent the prices progressively enlarged by $\lim_{\Delta \to 0^{+}} \sigma(t + \Delta)$ for
    each $t$ as follows.

    \begin{equation}
        \label{eqn:eqn:price_process_decomp}
        \dif p(t) = \gamma(t) + \beta(t) \sigma^2(t) + \psi(t) \sigma^2(t) + \sigma(t) \sqrt{1 - \phi(t)^2} \dif
        W^p(t) 
    \end{equation}

\end{restatable}

The structure here closely mimics the discrete-time structure in \textcite{khrapov2016affine}.
The two key differences are that it is on continuous-time, obviously, and that it is an exact nonparametric
representation.
Their representation only holds under some parametric assumptions, and so their discussion of identification is
parametric.


The $\psi(t)$ parameter governs how the drift changes once we condition on the volatility in the immediate
future.
To see this more fully consider the following, where we assume $\psi(t)$ and $\beta(t)$ are constant.
Note, $F_{t+1, \sigma^2}$ conditions on the entire path of volatility from $t$ to $t+1$.


\begin{align}
    &\phantom{=} \E\left[r_{t+1} \mvert \F_{t}\right]  - \E\left[\E\left[r_{t+1} \mvert \F_{t+1, \sigma^2} \right]
      \mvert \F_t \right]  \\
      %
      \intertext{Then by the law of iterated expectations.}
      %
    &=  \E\left[\beta(t) \sigma^2_{t+1} - \beta(t) \sigma^2_{t+1} + \psi \sigma^2_{t+1}  \mvert \F_t \right] \\
      \label{eqn:change_in_expected_rtn}
      &= \psi \E\left[\sigma^2_{t+1} \mvert \F_t \right]
\end{align}

The parameter $\phi(t)$, on the other hand, governs the correlation between $p(t)$ and $\sigma^2(t)$.
Consider their predictable quadratic variation of the part of the precises that is orthogonal to $\sigma^2(t)$.

By properties of Gaussian conditioning, we can split the Wiener process driving $p(t)$ into two parts.
One that drives $\sigma^2(t)$ --- $W^{\sigma}t)$ and one that is independent of $\sigma^2(t)$ --- $W^p(t)$.
Consequently, we can write the price process, with respect to $\F_t$ as follows.

\begin{equation}
    \predQV*{p(t), \sigma^2(t)}  
    = \predQV*{\int^t_0 \phi(t) \sigma(t) \dif W^{\sigma}, \int^t_0 \sigma_{\sigma} \dif W^{\sigma}} 
    = \int^t_0 \phi(s) \sigma(s) \sigma_{\sigma}(s) \dif s
\end{equation}

If all of the functions are  constant then the correlation between the price's and the volatility's diffusion part
is $\phi$, i.e.\@  $\mathrm{corr}\,(p(t), {\sigma^2(t)}^D) = \phi$.

\subsubsection{Going from Continuous-Time to Discrete-Time}\label{sec:discrete_time_to_cont_time}

The data that we have are daily, not in continuous-time. 
Consequently, we need to consider what the price process in  \cref{eqn:eqn:price_process_decomp} implies about the
daily data.
Thankfully, all of the infinite-activity components are independent by construction.
Consequently, we can time-aggregate \cref{eqn:eqn:price_process_decomp}, and get the following formula for the
discrete-time returns.  
We can then use those returns to characterize the identification problem.
We can define the returns with respect to the following two filtrations.
First, we define $r_{t+1}$ with respect to the $\F_{t}$ filtration.

\begin{equation}
    \label{eqn:discrete_time_rtn}
    r_{t+1} = \int_t^{t+1} \gamma(s) \dif s + \int_{t}^{t+1} \beta(s) \sigma^2(s) \dif s +
    \int_t^{t+1} \sigma(s) \left[\sqrt{1 - \phi(s)^2} \dif W^{p}(s) + \phi(s) \dif W^{\sigma}(s)\right]
\end{equation}

If we knew the entire path of future volatility, we could use the following equation, where we characterize
$r_{t+1}$ with respect to the $\sigma\left(\F_{t}, \F_{\sigma^2_{t+1}}\right)$ filtration.

\begin{equation}
    \label{eqn:discrete_time_cond_rtn}
    r_{t+1} = \int_t^{t+1} \gamma(s) \dif s + \int_{t}^{t+1} [\beta(s) + \psi(s)] \sigma^2(s) \dif s +
    \int_t^{t+1} \sigma(s) \sqrt{1-\phi(s)^2} \dif W^{p}(s)
\end{equation}

In both \cref{eqn:discrete_time_rtn} and \cref{eqn:discrete_time_cond_rtn}, we have the random functions
$\gamma(t), \beta(t), \psi(t), \phi(t)$.
In addition, in we also condition on the entire path of future volatility in \cref{eqn:discrete_time_cond_rtn}
instead of just $\int_t^{t+1} \sigma^2(s) \dif s$.

However, since we are using long time-span asymptotics, and not infill asymptotics, the model we end up estimating
will have fixed parameters instead of continuous-time processes for each of these parameters.

%TODO Added assumption here.
We start by defining the means of $\gamma_0, \beta(t), \psi(t)$, and $\phi(t)$: $\gamma_0, \beta_0, \psi_0, \phi_0$.
We assume that all of the processes are ergodic and so time and cross-sectional means are the same.

%TODO Added assumption here.
By the intermediate value theorem, assuming the predictable functions are continuous, there exists an
$s_i^{\dagger}$ for each of these functions that function evaluated at $s^{\dagger}$ equals the mean over the
interval.
For example $\beta\left(s_i^{\dagger}\right) = \int_0^{\infty} \beta(s) \dif s$.  
We can take mean value expansions for each of these functions.

\begin{alignat}{1}
    \label{eqn:price_decomp}
    \left. r_{t+1} \mvert \F_t \right. 
    %
    &= \gamma_0 + \beta_0 \E\left[\int_t^{t+1} \sigma^2(s) \dif s \mvert \F_t\right] + \int_t^{t+1} \sigma(s)
       \left[\sqrt{1 - \phi_0^2} \dif W^{p}(s) + \phi_0 \dif W^{\sigma}(s)\right] + \eta_{1,t+1} \\
    \left. r_{t+1} \mvert \F_t, \sigma^2_{t+1} \right. 
    %
    &= \gamma_0 + (\beta_0 + \psi_0) \int_t^{t+1} \sigma^2(s) \dif s + \int_t^{t+1} \sigma(s) \sqrt{1-\phi_0^2}
       \dif W^{p}(s) + \eta_{2, t+1} 
\end{alignat}

To show identification of our results, we need to characterize the error terms in the expression above.
In particular, we need to show that they are mean-zero and conditionally mean-independent.
We can compute what the error terms are by adding and subtracting the relevant terms.

\begin{alignat}{1}
    \eta_{1,t+1} 
    &= \int_t^{t+1}(\gamma(s) - \gamma_0) \dif s + \E\left[\int_t^{t+1} (\beta(s) - \beta_0) \sigma^2(s) \dif s
        \mvert \F_t\right] \\
    &+ \int_t^{t+1} \sigma(s) \left[\left(\sqrt{1 - \phi(s)^2} - \sqrt{1 - \phi_0^2}\right) \dif W^{p}(s) +
        \left(\phi(s) - \phi_0\right) \dif W^{\sigma}(s)\right] \nonumber \\
%
    \eta_{2,t+1} 
    &= \int_t^{t+1}(\gamma(s) - \gamma_0) \dif s + \int_t^{t+1} (\beta(s) + \psi(s) - (\beta_0 + \psi_0))
       \sigma^2(s) \dif s \\
    &+ \int_t^{t+1} \sigma(s) \left(\sqrt{1 - \phi(s)^2} - \sqrt{1 - \phi_0^2}\right) \dif W^{p}(s) \nonumber 
\end{alignat}

Clearly, all of the terms are  unconditionally mean zero. 
The non-martingale terms are deviations from their global means, and the martingale terms are martingales.
To get them to be conditionally zero, we need to work a little harder.

Assume that the prices of volatility and market risk and the dependence of volatility on its past are fixed over
time.
That implies that approximate log-Laplace transform studied in \cref{sec:dynamics} is fixed.
Consequently, the $\gamma_0, \beta_0$, etc.\ functions are valid means for each day, e.g.\@ $\E\left[\int_t^{t+1}
\gamma(t) \mvert \F_t \right] = \gamma_0$. 
Note, the length of the day does not entire in here because it equals $1$.

\begin{equation}
    \label{eqn:cond_zero_err_terms}
    \E\left[\eta_{i,t+1} \mvert \F_{t}\right] = 0
\end{equation}

The only question then is if the $\eta_{i,t+1}$ are correlated with the regressors. 
If we were to use $\sigma^2_{t+1}$ directly as a regressors, they would be.
Consequently, we will solve this using the standard method --- instruments.
We have a collection of $Z_{i,t}$ that are $\F_t$ measurable, but correlated with $\sigma^2_{t+1}$.
Since $Z_{i,t+1} \in \F_t$, \cref{eqn:cond_zero_err_terms} implies they satisfy the necessary orthogonality
condition. 
Any good predictor of $\sigma^2_{t+1}$ will satisfy this criterion.
In the empirical example, we will use lags of $\sigma^2_{t+1}$.


\subsubsection{Identification of the Risk Prices}


The goal moving forward is to determine what we need to assume about the physical measure functions in order to
identify the risk prices.
To do this, we need to do two things.
First, we need to characterize the part of the physical measure functions that is identifiable from equity data.
Second, we must consider under what conditions that allows us to identify the risk prices. 

The information that return data contain about equity pricing data is entirely encapsulated by the asset pricing
equation for excess returns.  
(In what follows, we will use $r_{t+1}$ as the excess log-return.)

\begin{equation}
    \E\left[ M_{t,t+1}(\theta, \pi) \exp(r_{t+1}) \mvert \F_{t} \right]
\end{equation}

We now characterize, the information in this set of moment conditions regarding the risk prices.
We start by substituting in the SDF formula from above, and we also replace all of the dependence on $\F_t$ with
$\sigma^2_t$.

\begin{gather}
    \E \left[ \exps*{ - \pi \sigma^2_{t+1} - (\theta - 1) r_{t+1} } \mvert \sigma^2_t \right]
        = \exps*{- m_0(\theta, \pi) - m_1(\theta, \pi) \sigma^2_t}
    \intertext{Similar to above, we used the law of iterated expectations to substitute in the conditional Laplace
        transforms of $r_{t+1}$ and $\sigma^2_{t+1}$.}
    \E\left[\exps*{- a_{\PP}\left(\pi + \alpha_{\PP}(\theta -1)\right) \sigma^2_t - b_{\PP}(\pi) -
        \beta_{\PP}(\theta-1) \sigma^2_t - \gamma_{\PP}(\theta-1)} \mvert \sigma^2_t \right] = \exps*{- m_0(\theta, \pi)
        - m_1(\theta, \pi) \sigma^2_t} 
\end{gather}






Similar, to above, we can now match the coefficients of the $\F_t$ measurable variables in the above moment
condition. 
We also substitute in the formulas for $m_0(\theta, \pi)$ and $m_1(\theta, \pi)$ that we derived in
\cref{sec:deriving_sdf_functions}.

\begin{align}
    \label{eqn:identification_eqn_1}
   \gamma_{\PP}(\theta-1) + b_{\PP}(\pi + \alpha_{\PP}(\theta - 1)  &= \gamma_{\PP}(\theta) +
       b_{\PP}(\pi + \alpha_{\PP}(\theta))  \\
    \label{eqn:identification_eqn_2}
    \beta_{\PP}(\theta-1) + a_{\PP}(\pi + \alpha_{\PP}(\theta -1)) &= \beta_{\PP}(\theta) +
        a_{\PP}(\pi + \alpha_{\PP}(\theta)) 
\end{align}

We can characterize the identification restrictions in \cref{eqn:identification_eqn_1} and
\cref{eqn:identification_eqn_2} in two different cases.   

\begin{enumerate}
    \item[Case 1:] The price of equity risk $\theta$ satisfies the following equations. 
        \begin{equation}
            \alpha_{\PP}(\theta - 1) = \alpha_{\PP}(\theta)\ \text{and}\ \beta_{\PP}(\theta) = \beta_{\PP}(\theta
            -1)\ \text{and}\ \gamma_{\PP}(\theta) = \gamma_{\PP}(\theta - 1)
            \label{eqn:lack_of_id_condition}
        \end{equation}
        In this situation, clearly, regardless of the value of $\pi$, we can satisfy these equations.
        In other words, the asset pricing equation does not identify $\pi$. 
        As noted by \textcite{khrapov2016affine}, this is in line with the common belief that the econometrician
        needs options data to be able to identify the price of volatility risk. 
    \item[Case 2:] 
        In general, there is no reason to expect the \cref{eqn:lack_of_id_condition} to hold.
        Since we have two equations and two unknowns, it might seem reasonable to expect that we should be able to
        identify both $\theta$ and $\pi$.
\end{enumerate}

Requiring \cref{eqn:identification_eqn_1} and \cref{eqn:identification_eqn_2} hold is equivalent to
requiring that their Taylor series expansions equal.
We need the entire physical measure functions to characterize the full information available in the physical
measures about the risk-prices.
However, if we want to allow for higher-order priced risks, we cannot use all of that information.
\purple{TODO (Paul) Write this out.}
In addition, in the Taylor series formulas the change in the first two moments is captured in the change of the
first two moments of the change of measure.
Consequently, we need to require that their quadratic expansions equal.\footnote{The equality of the quadratic
    expansion of the logarithm of the density is precisely what the change of measure in the discrete-time
Gaussian and continuous-time cases requires.}


In the first case, as discussed in \textcite[13]{khrapov2016affine}, in this 2nd case the existence of leverage
allows us to separate the two risk prices.
Clearly, both identification equations are satisfied if $\alpha_{\PP}(\theta) = \alpha_{\PP}(\theta -1)$.
To see when this will happen consider a 2nd-order Taylor series expansion of $\alpha_{\PP}$.

\begin{align}
    \alpha(\theta) - \alpha(\theta - 1) &= \alpha_{\PP}'(0)  + \alpha_{\PP}''(0) \left(\theta - \frac{1}{2}\right)
    + \text{higher-order terms} \\
%
    \intertext{Implicitly defining $\psi \coloneqq \alpha_{\PP}'(0)$ and $\phi \coloneqq \sqrt{1 -
        \alpha_{\PP}''(0)}$, we can rewrite this as follows.} 
 %
    &=  \psi + (1 - \phi^2) \left(\theta - \frac{1}{2}\right) + \text{higher-order terms}  
%
    \intertext{Since we only want to use mean and variances for identification, we can drop the higher-order
    terms.}
%
    \label{eqn:lack_of_id}
    &=  \psi + (1 - \phi^2) \left(\theta - \frac{1}{2}\right) 
\end{align}

If the \cref{eqn:lack_of_id} equals zero, then we cannot identify $\pi$.
As discussed in \textcite[13]{khrapov2016affine}, since $\psi$ is a adjustment to a drift, if $\phi=0$, (there is
no leverage effect), the drift would change by $\theta - \frac{1}{2}$ through a Jensen type effect if the
intra-day volatility is constant. 
This decomposition would still go through in the general case if $\phi=0$ because then instantaneous changes in
volatility would not be priced, and so the time-aggregation formulas and Jensen inequality terms would not be
affected.
(Predictable variation in the volatility enters into the price of market risk. The condition argument in
\cref{sec:discrete_time_to_cont_time} was entirely concerned with unpredictable variation, which is irrelevant if
$\phi = 0$.)


\purple{TODO: (Paul) justify why we are taking a linear expansion with respect to $\gamma$, $\beta$, and $b$.}

In the second case, we take Taylor series expansions of \cref{eqn:identification_eqn_1} and
\cref{eqn:identification_eqn_2}.  
We require that the approximations we are using hold exactly. 
Effectively, this is an identification assumption for $\theta, \pi$.
They are the parameters that make the means and variances have the relationship in the model that they do in the
data between the physical and risk-neutral measures.
If we wanted to make the equations hold with respect to a higher-order approximation we would want to introduce
prices of other types of risk such as skewness risk and kurtosis risk.

Before, we continue to simplify the equations, there is another asset pricing restriction that we should impose.
If the risk prices --- $\theta$ --- equal zero, then the expected return should be zero.
Then by noting that the physical measure functions equal zero when evaluated at zero, we have the following.

\begin{align}
    \gamma_{\PP}(-1) + b_{\PP}(\alpha_{\PP}(-1))  = 0  \\
    \beta_{\PP}(-1) + a_{\PP}(\alpha_{\PP}(-1))  =  0
\end{align}

To simplify this further, take the 2nd order Taylor expansion of consider $\alpha_{\PP}(-1) = \psi + \frac{1}{2}(1 - \phi^2)
(-1)$. 
Plugging that in, we have the following equations, and imposing linearity on $\gamma_{\PP}$ and $\beta_{\PP}$.

\begin{align}
    \gamma_{\PP}(v) &= - \gamma_{\PP}(-1) v = v b_{\PP}\left(\psi - \frac{1}{2}\left(1 - \phi^2\right)\right)  \\
    \beta_{\PP}(v) &= - \gamma_{\PP}(-1) v = v a_{\PP}\left(\psi - \frac{1}{2}\left(1 - \phi^2\right)\right)  
\end{align}


\begin{align}
    \gamma (\theta - 1) + c \delta \left(\pi + \psi(\theta - 1) + \phi \frac{(\theta - 1)^2}{2}\right) &= \gamma
    \theta + c \delta \left(\pi + \psi \theta + \phi \frac{\theta^2}{2} \right) \\ 
%
    \beta (\theta - 1) + \rho \left(\pi + \psi (\theta-1)  + \phi \frac{(\theta -1)^2}{2}\right)
    &= \beta \theta + \rho \left(\pi + \psi \theta  + \phi \frac{\theta^2}{2}\right) 
\end{align}

The two equations above implicitly define $\theta$, $\pi$, as functions of $\psi$ and $\phi$.
Note, as is standard in this environment, we have an equilibrium relationship in terms of various parameters.
We are not making any causal claims here, just econometric ones.

Equivalently, we can solve for $\psi$ and $\gamma$ as functions of the other parameters.
This is a natural reparameterization because $\psi$ and $\beta$ are the two physical measure parameters that are
drive the conditional risk premia and hence are determined by the risk prices.

\begin{defn}
    \begin{align}
        \gamma(c, \delta, \pi, \theta)  &\coloneqq c \delta \left[\left(\pi + \psi \theta + \phi
        \frac{\theta^2}{2} \right)  - \left(\pi + \psi(\theta - 1) + \phi \frac{(\theta - 1)^2}{2}\right)
        \right]\\ 
%
    \beta (\theta - 1) + \rho \left(\pi + \psi (\theta-1)  + \phi \frac{(\theta -1)^2}{2}\right)
    &= \beta \theta + \rho \left(\pi + \psi \theta  + \phi \frac{\theta^2}{2}\right) 
    \end{align}
\end{defn}



\section{GMM}\label{sec:GMM}

To estimate the model we use the various moment conditions that we have derived.
The one that we have yet to derive is the ones implied by the continuous-time model that allow us to estimate
$\phi$ and $\psi$.
Deriving them is straightforward because we constructed what the discrete-time equations, and they are linear.
The only difficult one is that we used the Markov assumption along with the Taylor approximation to convert
$\E\left[r_{t+1} \mvert \F_t\right]$ into a linear function of $\sigma^2_t$ (\cref{eqn:market_risk_price}).
The last two equations are not moment conditions because they most exactly; they are a restriction upon the
population values.
They define an population relationship between the parameters implicitly defining the risk prices in terms of the
other parameters. 

First, we define two functions that we use to reparameterize the moment conditions.
We do this because the model creates some cross-equation equations to eliminate two of the redundant parameters.
By doing this we are able to avoid the econometric complications that we would have to handle in the general
case.


\begin{defn}{Equilibrium Conditions}
    \begin{align}
        \E\left[\sigma^2_{t+1} \mvert \sigma^2_t \right]  &= \rho \sigma^2_t  + c \delta\\
    %
        \E\left[\sigma^2_{t+1}\right]  &= \frac{c \delta}{1 - \rho} \\
    %
        \Var\left[\sigma^2_{t+1} \mvert \sigma^2_t \right]  &=  2 c \sigma^2_t  + c^2 \omega \\
    %
        \Var\left[\sigma^2_{t+1} \right]  &=  \frac{c^2}{1 - \rho^2}  \left(\frac{2 \rho \delta}{1 - \rho}  +
        \omega \right)  \\
    %
        \label{eqn:market_risk_price}
        \E\left[r_{t+1} \mvert \sigma^2_t\right] &= \gamma + \beta (\rho \sigma^2_t + c \delta)  \\
    %
        \E\left[r_{t+1} \mvert \sigma^2_t, \sigma^2_{t+1}\right] &= \gamma + (\beta + \psi) \sigma^2_{t+1}  \\
    %
        \gamma (\theta - 1) + c \delta \left(\pi + \psi(\theta - 1) + \phi \frac{(\theta - 1)^2}{2}\right) &=
        \gamma \theta + c \delta \left(\pi + \psi \theta + \phi \frac{\theta^2}{2} \right) \\ 
    %
        \beta (\theta - 1) + \rho \left(\pi + \psi (\theta-1)  + \phi \frac{(\theta -1)^2}{2}\right)
        &= \beta \theta + \rho \left(\pi + \psi \theta  + \phi \frac{\theta^2}{2}\right) 
    \end{align}
\end{defn}

To fully specify the GMM conditions, we need to also specify the instruments we are using.
Doing this correctly is somewhat subtle because we need to only use $\F_{t-1}$-measurable variables in order to
satisfy the necessary exogeneity conditions, while providing instruments for both $\sigma^2_t$ and
$\sigma^2_{t+1}$. 
The natural instrument to use in both cases is $\sigma^2_t$.
This comes from the first-order Markov assumption that we made.
However, we need more than one instrument in order to be identified.
The way around this is to notice that although we assumed first-order Markov, we did not assume that the
transition was linear. 

To see this more fully, note that the volatility risk price is being identified through the part of $\psi$ above
and beyond a Jensen inequality effect.

Furthermore, recall \cref{eqn:change_in_expected_rtn},  $\E\left[r_{t+1} \mvert \sigma^2_{t+1}, \F_{t}\right]  -
\E\left[ r_{t+1} \mvert \F_t\right]$ = $\psi \E\left[\sigma^2_{t+1} \mvert \F_t\right]$.
To the extent that we can predict $\sigma^2_{t+1}$ by using $\sigma^2_t$, the price will already be absorbed into
$\beta$, the coefficient on $\sigma^2_t$.
Hence, to identify $\psi$ correctly, we need the conditioning set here to be more than just $\sigma^2_t$.


As an aside, using more discrete-time information here to predict $\sigma^2_{t+1}$ does not contradict our
derivation above because we conditioned on $\sigma^2(t-)$ in continuous time, and so the investors in the market
would not have had to instrument for $\sigma^2(t)$ using $\F_{t-1}$ information.
To put it another way, the investors in the market have access to a more informative sufficient statistic than we
do, and so we assumed they were using that in the derivation.
Consequently, it is innocuous to assume that they are not using the $\F_{t}$ information that we are using.

Consequently, we use lags of the volatility and powers of those lags as instruments.
In principle, we would only need to use powers of $\sigma^2_t$ as long as we used enough of them.
However, since in practice we are using an estimate of $\sigma^2_t$, using additional lags instead of very high
powers of $\sigma^2_t$ will likely perform better in practice.


\begin{defn}{Instruments}
    \label{defn:instruments}
    \begin{equation}
        Z_t \coloneqq 1, \sigma^2_{t}, \sigma^2_{t-1}, \sigma^2_{t-2}, \ldots, (\sigma^2_{t})^2,
        (\sigma^2_{t-1})^2, (\sigma^2_{t-2})^2, \ldots
    \end{equation}
\end{defn}

Having constructed the moments and the instruments, we can use GMM to estimate the parameters.
We do need to run a constricted optimization though, because only certain values of the parameters are valid. 
In particular, the parameters must be contained in the followed identified set.


\begin{restatable}[Identified Set]{lemma}{identifiedSet}
    \label{lemma:IdentifiedSet}
    Let $\eta_0 \coloneqq \left(\rho_0, c_0, \phi_0, \pi_0, \theta_0\right)$.
    Let the domain $\Eta$ for $\eta$ be defined as follows. 
    \begin{equation}
    \begin{split}
        \Eta &\coloneqq \left\{ \eta \in  [0, \overline{\rho}] \otimes [0, \overline{c}] \otimes \left[-1,
            1\right] \otimes \left[-\underline{\theta}_1, \overline{\theta}_1\right] \otimes
            \left[-\underline{\theta}_2, \overline{\theta}_2\right]\right\} \\
        &0 < \overline{c}, -\underline{\theta}_1, \overline{\theta}_1, -\underline{\theta}_2 < \infty, 0 <
            \overline{\rho} < 1
    \end{split}
    \end{equation}


    \begin{equation}
        \text{Given}\ \eta_0 \in \Eta,\ \text{the moment conditions}\ g_t(x, \eta)\ \text{identify}\
    \begin{cases}
        \eta_0                  &\text{if}\ \phi \neq 0 \\
        \eta_0 \setminus \pi_0  &\text{if}\ \phi = 0
    \end{cases}
    \label{eqn:EtaDefn}
    \end{equation}

    In addition, if $\phi = 0$, the GMM criterion function is independent of $\theta$.
\end{restatable}


If we plug in the estimated values of the parameters from \textcite{khrapov2016affine} into $\frac{\partial
\phi}{\partial \pi}$ and plot it as a function of $\phi$,  we get the following.
The scale is omitted because it is not meaningful. 
As can clearly be seen in \cref{fig:fig:gamma_diff_theta2}, there is a zero when $\phi = 0$.

\begin{figure}[htb]
    \centering
    \caption{Derivative of $\gamma(x)$ with respect to $\theta$}
    \label{fig:fig:gamma_diff_theta2}
    \includegraphics[width=.5\textwidth]{gamma_diff_theta2.pdf}
\end{figure}


\section{Weak Identification Setup}

In this section, take the model described in the previous sections and place it in the setup of
\textcite{andrews2014Gmm} so that we ca analyze the effects of possible lack of identification in the model in a
nice clean way.
The goal here is to perform valid inference for $\pi, \theta$ even when $\phi$ might be zero. 


From the discussion above, we can collect the parameters discussed above into a parameter vector of the following
form,i.e.\@ recall the following: $\eta = \lbrace \rho, c, \delta, \phi, \pi, \theta \rbrace$
To write it in the notation of \textcite{andrews2014Gmm}, we partition $\eta$ into three subsets.

\begin{align}
    \phi &\coloneqq \phi  \in (-1, 1) \\ 
    \zeta &\coloneqq \lbrace \rho, c, \delta, \theta \rbrace \in [0,1) \times \R_{++} \times \R_{++} \times
    \R  \\
    \pi &\coloneqq \pi \in \R 
\end{align}

Let $\Eta$ be the set of possible $\eta$, that as defined above.
It is worth noting that the parameter space has a product form, i.e.\@ the values do not affect the valid values
of the other parameters.

In this environment, $\pi$ is not identified when $\phi = 0$.
Both $\phi$ and $\zeta$ are always identified, and $\zeta$ does not affect the identification of $\pi$.

Let $Q_T(\eta)$ be the GMM criterion function, then the GMM estimator $\hat{\eta}_T$ satisfies the following.


\begin{equation}
    \widehat{\eta}_T \in \Eta\ \text{and}\ Q_T(\hat{\eta}_T) = \inf_{\eta \in \Eta} Q_T(\eta) +
    o\left(T^{-1}\right) 
\end{equation}


Now that we have defined the parameters, we can characterize the set of assumptions necessary for valid inference.
We will work through the assumptions described in \textcite{andrews2014Gmm}.
The set of necessary assumptions is relatively complicated because we have to characterize the asymptotic
distribution under several different estimation strengths simultaneously, and the assumptions required to do that
  differ in the various cases. 
In what follows, we will use 

The first assumption specifies the basic identification
problem. It also provides conditions that are used to determine the
probability limit of the GMM estimator, when it exists, under all categories
of drifting sequences of distributions.
Let $\xi$ index the part of the distribution of the data $r_{t+1}, \sigma^2_{t+1}$ that is not determined by the
moment equations.
In general, it is a (likely infinite-dimensional) nuisance parameter that affects the distribution of the data. 


We collect the parameters that we are estimating $\eta$ and the nuisance parameter $\xi$ into one parameter,
$\gamma$ and associated parameter space $\Gamma$.
In the previous discussion we characterized the parameter spaces in a non-compact fashion, let $\Eta^{*}$ be a
compact subset of $\Eta$, where the true parameter values live.
%TODO Does the set of possible nuisance parameters depend upon the estimated parameters in our model?

\begin{defn}{Complete Parameter Space}
    \begin{equation}
        \Gamma \coloneqq \left\lbrace \gamma = (\eta, \xi) \mvert \eta \in \Eta, \xi \in \Xi \right\rbrace 
    \end{equation}
\end{defn}

We characterize these drifting sequences of distributions by sequences of true parameters $\gamma_T \coloneqq
(\eta_T, \phi_T)$.

\purple{TODO Add discussion of the limiting process.}
\purple{Verify that the assumptions on the parameter space hold.}
\purple{Discuss what happens if we lack identification and hence cannot consistently estimate the parameter.}

\begin{restatable}[Inference for $\eta$ under Weak Identification]{theorem}{InferenceWeakID}
    Let that $\phi_0  \in (\underline{\phi}_0,1)$, for some $\underline{\phi}_0 > -1$. 
    $\rho_0 \in [0,1)$, and $c_0 > 0$. 

    \purple{TODO  Add Conclusion}
\end{restatable}


\section{Simulations}

\section{Data}\label{sec:data}

We also assume that $\left[ \frac{\psi}{\phi} \right]^2 \approx \frac{\E \left[\sigma^2_{t+1} \mvert
\F_t\right]}{\Var\left[r_{t+1} \mvert \F_t\right]}$, which enables our approximation of $\sigma^2_{t+1}$ by the
realized volatility.

\section{Empirical Results}

\section{Conclusion}

\clearpage

\phantomsection
\addcontentsline{toc}{section}{References}
\printbibliography
\clearpage

\begin{appendices}


\section{Identification Proofs}

\priceDGPGivenVol*

\begin{proof}

    We work directly with the martingale part of the price, $\sigma(t) \dif W(t)$, the drift part can just be
    added back to the price when we are finished because it is predictable.
    
    To model the additional information in a tractable way, consider a dense discrete grid of points over $\R$ ---
    the magnitudes of the prices.
    Define a set of times $\lbrace \tau_i\rbrace $ such that $\tau_i$ is the time at which the volatility crosses
    the $i$\textsuperscript{th} point, but not the $(i+1)$\textsuperscript{th} one. 
    Note, since equality at a discrete dense set of points is sufficient to imply equality over any open set,
    equality over $\lbrace \tau_i \rbrace$ is sufficient for equality in probability since the opens sets generate
    the Borel measure.
    
    Now, each $\tau_i$ is an honest time with respect to the $\F_{t=\tau_i-}$ filtration, but not a stopping time
    because it is not contained within the $\F_{\tau_i-}$ filtration.
    Define $\F_{\sigma,t} \coloneqq \F_{\sigma, t+} \cap \sigma\left(\lbrace \tau_i \rbrace\right)$. 
    Clearly, there is only one $\tau_i$ in that filtration, which we will refer to as $\tau$ in the sequel.
    
    We are augmenting the predictable filtration $\F_{t-}$ with an honest time --- $\tau$.
    Since, $p(t)$ is adapted to $\F_{t-}$, we can use formulas for augmenting the price with an honest time. 
    Also, since there are at most countably many jumps and the prices and volatilities are locally square
    integrable, there is at most one jump in $\sigma^2(t)$ that is measurable with respect to $\F_{\sigma, t+}$.
    
    Since $\sigma^2(t)$ has no predictable jumps, $\tau$ avoid all stopping times of $\F_{t-}$.  
    Hence, the optional and predictable projections coincide, and we can choose the following quantities to be
    continuous.
    Let $A^{\tau}(t)$ be the predictable projection of the process $1\lbrace \tau \leq t \rbrace$, define the
    c\'{a}dl\'{a}g martingale $\mu^{\tau}(t) \coloneqq \E\left[A^{\tau}(\infty) \mvert \F_{t-}
    \right]$.
    Then by \textcite[eqn 2.3]{nikeghbali2007nonstopping}, we can write the martingale part of the process stopped
    at $\tau$  as follows, where $M(t)$ is a martingale.

    
    \begin{align}
        \label{eqn:filtration_changed_martingale}
        \int_0^{t \wedge \tau} \sigma(s)  \dif W(s)   
         &= M(t) + \int_0^{t \wedge \tau} \frac{1}{\Pr\left(\tau > s
            \mvert \F_{s-}\right)} \dif \predQV*{\int_0^s \sigma(u) \dif W(u), \mu^{\tau}(s) } 
%
            \intertext{Since drifts do not affect quadratic variation and $\mu^{\tau}(t)$ is continuous. }
%
        &= M(t \wedge \tau) + \int_0^{t \wedge \tau} \frac{1}{\Pr\left(\tau > s \mvert \F_{s-}\right)} \dif
          \predQV*{\int_0^t \sigma(s) \dif W(s), \sigma_{\mu} \dif W^{\mu}(s)} (s)  \\
    %
        &= M(t \wedge \tau) + \int_0^{t \wedge \tau} \frac{\sigma(s) \sigma_{\mu}(s)}{\Pr\left(\tau > s \mvert
           \F_{s-}\right)} \dif s
    \end{align}
    
    We now let $\psi(t) \coloneqq \frac{\sigma_{\mu}(s) }{\Pr\left(\tau > s \mvert \F_{s-}\right)}$. 
    Then we can write the drift as $\E\left[\dif p(t) \mvert \F_{t-}\right] = \psi(t) \sigma(s)$.

    In addition, $M(t)$ has some predictable quadratic variation that we assume is absolutely continuous.
    We will define $\phi(t)$ so that its diffusion part --- $M^D(t)$ -- has the appropriate relationship with the
    prices.
    Define $\phi(t) \coloneqq \frac{\dif \predQV*{M}(t)}{\sigma_{\sigma}(t)}$. 
    
    
    $M(t)$ is still a martingale in $\mathcal{G}(t)$, which is the smallest filtration satisfying the standard
    conditions that is generated by the prices augmented with $\F_{t-}$. 
    In addition, since $\predQV{M}(t)$ is predictable, and hence $\F_{t-}$ measurable, we can choose its
    instantaneous variance to be same in the two different filtrations. 

    Martingales in a larger filtration are still martingales in a smaller filtration.
    Consequently, $M(t)$ is still a martingale in $\mathcal{G}(t)$.
    However, since only predictable processes are measurable with respect to $\mathcal{G}(t)$.
    Consequently, $M(t)$ is a predictable martingale, implying it is a continuous version.
    
    Hence $\predQV*{M}(t) <= \predQV*{p(t) \perp \sigma^2(t)}$ because conditioning in Gaussian environments does
    not decrease the variance.
    The reverse inequality holds because $p(t) \perp \sigma^2(t)$ is still a martingale in $\F_{\sigma,t}$.
    Consequently, the two processes are equivalent.
   
    Since $p(t) \perp \sigma^2(t) = \int^t \phi(s) \dif W^{p}(s)$, the result follows.


\end{proof}

\section{Inference Assumptions}

    In what follows, three sets of drifting sequences $\lbrace \gamma_T \rbrace$ are key. 
    
    \begin{defn}{Drifting Sequence Parameter Spaces}
        \begin{align}
            \Gamma\left(\gamma_0\right) &\coloneqq \left\lbrace \left\lbrace \gamma_T \in \Gamma \right\rbrace
            \mvert \gamma_T \to \gamma_0 \in \Gamma \right\rbrace\\ 
            \Gamma(\gamma_0, 0, b) &\coloneqq \left\lbrace \lbrace \gamma_T \rbrace \in \Gamma(\gamma_0) \mvert
            \phi_0 = 0\ \text{and}\ \sqrt{T} \phi_T \to b \in (\R \cup \lbrace \pm \infty) \right\rbrace \\
            \Gamma(\gamma_0, \infty, b_0) &\coloneqq \left\lbrace \lbrace \gamma_T \rbrace \in \Gamma (\gamma_0)
            \mvert \sqrt{T} \norm{\phi_T} \to \infty\ \text{and}\ \frac{\phi_T}{\norm{\phi_T}} \to b_0
            \right\rbrace 
        \end{align}
    \end{defn}
    
    These are the standard GMM regularity conditions appropriately adjusted for the lack of identification when
    $\phi =0$.
    
    \begin{assump}[GMM 1]\label{ass:GMM1}
    \begin{assumplist}
        \item If $\phi_0=0$, $\sampmom(\eta)$ and $\W_{T}(\eta)$ do not depend on $\pi$ for all $\eta \in \Eta$,
            for all $T \geq 1$, and for all $\gamma^{*}\in \Gamma.$ 
            \label{ass:GMM1a}
        \item If $\lbrace \gamma_{T} \rbrace \in \Gamma\left(\gamma_0\right)$, $\sup_{\eta \in \Eta}
            \norm*{\sampmom(\eta) - \E\left[g\left(\eta \mvert \gamma_0\right)\right]} \pto 0$ and $\sup_{\eta
            \in \Eta} \norm{\W_{T}(\eta)-\E\left[\W\left(\eta \mvert \gamma_0\right)\right]} \pto 0$.
            \label{ass:GMM1b}
        \item When $\phi_0 = 0$,  $g_0\left(\phi, \zeta ,\pi \mvert \gamma_0\right) = 0$ if and only if $\phi
            =\phi_0$ and $\zeta = \zeta_0$ for all $\pi \in \Pi$ and for all $\gamma_0 \in \Gamma.$
            \label{ass:GMM1c}
        \item When $\phi_0 \neq 0$, $g_0\left(\eta \mvert \gamma_0\right)=0$ if and only if $\eta =\eta_0$ for all
            $\gamma_0 \in \Gamma.$
            \label{ass:GMM1d}
        \item  $g_0\left(\eta \mvert \gamma_0\right)$ is continuously differentiable in $\eta $ on $\Eta$ with
            partial derivatives with respect to $\eta$ and $\xi$ denoted by $g_{\eta}\left(\theta \mvert
            \gamma_0\right) \in R^{k\times d_{\eta }}$ and $g_{\xi }\left(\eta \mvert \gamma_0\right)\in R^{k\times
            d_{\xi }}$, respectively.
            \label{ass:GMM1e}
        \item $\W\left(\eta \mvert \gamma_0\right)$ is continuous in $\eta$ on $\Eta$ for all $\gamma_0\in
            \Gamma$.  \label{ass:GMM1f}
        \item $0 < \lambda_{\min}(\W\left(\xi_0, \pi \mvert \gamma_0\right))\leq \lambda_{\max }(\W\left(\xi_0,\pi
            \mvert \gamma_0\right)) < \infty$, $\forall \pi \in \Pi$, for all $\gamma_0 \in \Gamma$.
            \label{ass:GMM1g}
        \item $\lambda_{\min} (g_{\xi}\left(\xi_0,\pi \mvert \gamma_0\right)^{\prime} \W\left(\xi_0,\pi \mvert
            \gamma_0\right)g_{\xi }\left(\xi_0,\pi \mvert \gamma_0\right))>0$, for all $\pi \in \Pi$,  and for all 
            $\gamma_0 \in \Gamma$ with $\phi_0=0.$
            \label{ass:GMM1h}
        \item$\Xi(\pi)$ is compact for all $\pi \in \Pi$, and both $\Pi$ and $\Eta$ are compact.
            \label{ass:GMM1i}
        \item For all $\epsilon > 0$, there exits a $\delta > 0$ such that $d_{H}\left(\Xi \left(\pi_{1}\right),
            \Xi \left( \pi_{2}\right) \right) < \epsilon$ for $\pi_{1}, \pi_{2} \in \Pi$ with
            $\norm*{\pi_{1}-\pi_{2}} < \delta$, where $d_{H}\left( \cdot \right)$ is the Hausdorff metric.
            \label{ass:GMM1j}
    \end{assumplist}
    \end{assump}
    
    
    
    \begin{assump}[GMM 2*]\label{ass:GMM2}
    \begin{assumplist}
        \item $\sampmom(\eta)$ is continuously differentiable in $\eta$ for all $T \geq 1$. 
            \label{ass:GMM2a}
        \item If $\{\gamma_T\} \in \Gamma\left(\gamma_0, 0, b\right)$, $\sup_{\left\lbrace \eta \in \Eta \mvert
            \norm*{(\phi, \zeta')' - (\phi_T, \zeta_0')} \leq \delta_T \right\rbrace}
            \norm*{\frac{\partial}{\partial (\phi, \zeta')'} \sampmom(\eta) - \E\left[\popmom_{(\phi,
            \zeta')'}(\eta) \mvert \gamma_0\right]} = o_p(1)$ for all deterministic sequences  $\delta_T \to 0$.
            \label{ass:GMM2b}
        \item  Let $\Eta_T \coloneqq \left\lbrace \eta \in \Eta \mvert \norm*{(\phi, \zeta_) - (\phi_T, \zeta_T)}
            \leq \delta_T \norm*{\beta_T}\, \text{and}\, \norm*{\pi - \pi_T} \leq \delta_T \right\rbrace$.  Let
            $\delta_T$ be a deterministic sequence that converges to zero.  If $\{\gamma_T \} \in
            \Gamma\left(\gamma_0, \infty, b_0\right)$, then we have the following asymptotic behavior.
            $\sup_{\eta \in \Eta_T} \norm*{\left(\frac{\partial}{\partial \eta'} \overline{g}_T -
            \E\left[g_{\eta}(\eta) \mvert \gamma_0\right]\right) \diag\left(1_{1+d_\zeta}',
            (1/\phi_T)_{d_{\pi}}'\right)}  = o_p(1)$. 
            \label{ass:GMM2c}
    \end{assumplist}
    \end{assump}
    
    Once we have \nameref{ass:GMM1} and \nameref{ass:GMM2}, we use \nameref{ass:GMM3} to derive the asymptotic
    distribution under weak and semi-strong identification.
    These conditions will be characterized using the expected derivative of the population moment conditions. 
    
    \begin{defn}
        \label{defn:moment_derivative_func}
        \begin{equation}
            K_{T,g}\left(\eta \mvert \gamma^{*}\right) \coloneqq  \frac{1}{T} \sum_{i=1}^T \frac{\partial}{\partial
            \phi^{*}} \E \left[ \popmom(W_T, \eta) \mvert \gamma^{*} \right]
        \end{equation}
    \end{defn}
    
    
    \begin{assump}[GMM 3]\label{ass:GMM3}
    \begin{assumplist}
        \item $\sampmom(\eta) = \frac{1}{T} \sum_{i=1}^T \popmom(W_T, \eta)$  for some function $\popmom(W_T,
            \eta) : \R^{k \times k} \times \Eta \to \R^k$.
            \label{ass:GMM3a}
        \item $\E\left[\popmom(W_T, \beta_0, \zeta^{*}, \pi) \mvert \gamma^{*} \right] = 0$ for all $\pi \in \Pi$ and
            for all $i \geq 1$ if $\gamma^{*} = \left(0,\zeta^{*}, \pi^{*}, \xi^{*} \right) \in \Gamma$.
            \label{ass:GMM3b}
        \item If $\{ \gamma_T \} \in \Gamma(\gamma_0, 0, b)$, $\frac{1}{\sqrt{T}} \sum_{i=1}^T \left(g(W_T,
            \zeta_{0,T}, \pi_T) - \E \left[g(W_T, \zeta_{0,T}, \pi_T)\mvert \gamma_T \right]\right)  \dto \N\left(0,
            \aleph(\gamma_0)\right)$, where $\aleph(\gamma_0)$ is a $k \times k$ matrix.
            \label{ass:GMM3c}
        \item 
            \label{ass:GMM3d}
            \begin{enumerate}
                \item  $K_{T,g}\left(\eta \mvert \gamma^{*}\right)$ exists for all $\{\eta, \gamma^{*} \} \in
                    \left(\Eta_{\delta} \times \Gamma_{0}\right)$ and for all $T \geq 1$.
                \item $K_{T,g}\left(\phi_T, \zeta_T, \pi \mvert \widetilde{\gamma}_T\right)$ uniformly converges
                    to some non-stochastic matrix-valued function  $K_{g}\left(0, \zeta_0, \pi \mvert
                    \gamma_0\right)$ over $\pi \in \Pi$ for all deterministic sequences $\{\phi_T, \zeta_T,
                    \widetilde{\gamma}_T \}$ satisfying $\widetilde{\gamma}_T \in \Gamma$, $\widetilde{\gamma}_T
                    \to \gamma_0 \coloneqq (0, \zeta_0, \pi_0, \xi)$, $\{\phi_T, \zeta_T, \pi \} \in \Eta$ and
                    $\{\phi_T, \zeta_T \} \to (0, \zeta_0)$.
                \item $K_g\left(\phi_0, \zeta_0, \pi \mvert \gamma_0\right)$ is continuous on $\Pi$ for all
                    $\gamma_0 \in \Gamma$ with $\phi_0 = 0$.
            \end{enumerate}
            \item $K\left(\phi_0, \zeta_0, \pi \mvert \gamma_0\right) = \popmom_{\phi, \zeta}\left(\phi_0,
                \pi\mvert \gamma_0\right) x$ for some $x \in \R^{1+d_{\zeta}}$ if and only $\pi =
                \pi_0$.\footnote{Since $\dim(\phi) = 1$, we can assume without loss of generality that the
                $\omega_0$ from \textcite{andrews2014Gmm} equals $1$.}
                \label{ass:GMM3e}
            \item If $\{ \gamma_T \} \in \Gamma(\gamma_0, 0, b)$, $\frac{1}{T} \sum_{i=1}^T
                \frac{\partial}{\partial \eta}  \E\left[ \popmom\left(W_T, \eta_T \right ) \mvert \gamma_T \right]
                \to \popmom_{\eta}\left(\eta_0 \mvert \gamma_0\right)$.
            \label{ass:GMM3f}
    \end{assumplist}
    \end{assump}

    \begin{defn}{\popmom*}
        \begin{equation}
            g_{\phi, \zeta}^{*}\left(\phi_0, \zeta_0, \pi_1, \pi_2 \mvert \gamma_0\right)  =
            \left[g_{\phi}\left(\phi_0, \zeta_0, \pi_1 \mvert \gamma_0\right)  , g_{\phi}\left(\phi_0, \zeta_0,
            \pi_2 \mvert \gamma_0\right) , g_{\zeta} \left(\phi_0, \zeta_0 \mvert \gamma_0\right)  \right]  \in
            \R^{k \times (d_{\zeta} + 2)}
        \end{equation}
    \end{defn}


    \begin{assump}[GMM 4]\label{ass:GMM4}
    \begin{assumplist}
        \item $\phi$ is a scalar.
            \label{ass:GMM4a}
        \item $g_{\phi, \zeta}^{*}\left(\phi_0, \zeta_0, \pi_1, \pi_2 \mvert \gamma_0\right)$ has full column
            rank. 
            \label{ass:GMM4b}
        \item $\aleph(\gamma_0)$ is positive definite for all $\gamma_0 \in \gamma $ with $\phi_0 = 0$. 
            \label{ass:GMM4c}
    \end{assumplist}
    \end{assump}

    \section{Inference Proofs}

    \identifiedSet*

    \begin{proof}
    
    Since the exponential function is a strictly positive function, and we are considering a grid of $x$ values, a
    sufficient condition for $\rho, \delta$, \& $c$ to be identified is for the relevant rows of $\nabla a(x)$ and
    $\nabla b(x)$ to equal zero only at $\eta_{0}$ which are satisfied if $\rho, c, \delta > 0$.
    Testing if if $\phi$ and $\theta$ are identified is somewhat trickier. 
    Consider $\nabla \alpha(x)$. 
    Since we are using a grid of $x$'s, and the gradient of $\alpha$ is a nonlinear function of $x$, the first two
    rows of the \cref{eqn:alpha_gradient} imply $\phi$ is identified.
    
    \begin{equation}
        \label{eqn:alpha_gradient}
        \frac{\partial \alpha(x)}{\partial (\phi, \theta, c)'}  = \begin{bmatrix} \phi x^{2} + x \left(- 2 \beta
        \left(\theta_{2} - \frac{1}{2}\right) - \frac{\phi}{2 \sqrt{c} \left(\phi + 1\right)^{\frac{3}{2}}} +
        \frac{1}{\sqrt{c} \sqrt{\phi + 1}}\right) \\ x \left(- \phi^{2} + 1\right) \\ \frac{\phi x}{2
        c^{\frac{3}{2}} \sqrt{\phi + 1}} \end{bmatrix} 
    \end{equation}
    
    The top line of \cref{eqn:alpha_gradient} can be solved for $\theta$, which would create a local lack of
    identification for $\theta$.
    However, this creates a linear relationship between $\theta$ and the other parameters and $x$.
    However, since we are using multiple $x$'s we can avoid this issue.
    Consequently,  $\phi \in (-1,1], c > 0$, are sufficient to identify all of the parameters except for $\pi$,
    the price of volatility risk.
    We can use $a_{\PP}(x)$ to identify $\rho$.
    
    If $\phi = 0$, $\beta(x) = x \left(a (\pi + \alpha(\theta)) - a(\pi + \alpha(\theta))\right)$, and
    $\gamma(x) =  x \left(b (\pi + \alpha(\theta)) - b(\pi + \alpha(\theta))\right)$.
    These both  identically zero, and $\pi$ does not show up in anywhere  else in the criterion function.
    
    \end{proof}


\begin{restatable}[Uniform Convergence under Strong Identification]{lemma}{UllnStrongID}
    \label{lemma:UniformConvergenceStrongID}
    Let $\Eta$ be the identified set defined by \cref{eqn:EtaDefn}.
    Further assume that $\phi_0 \neq 0$. 
    Let $\sampmom$ be the sample moment condition defined above, and $\W_T$ be the associated optimal weight matrix
    estimator.
    Then we have the following convergence.

    \begin{align}
        &\sup_{\eta \in \Eta} \norm*{\sampmom(\eta) - \E\left[g\left(\eta \mvert \gamma_0\right)\right]}_{Fro}
          \pto 0 \\ 
        &\sup_{\eta \in \Eta} \norm{\W_{T}(\eta)-\E\left[\W\left(\eta \mvert \gamma_0\right)\right]} \pto 
    \end{align}

\end{restatable}

\begin{proof}

    In this proof we rely heavily on the continuity of the moment conditions over their domain. 
    This can be seen from simple inspection since we assumed that $\phi_0 \geq \underline{\phi} -1$.
    Furthermore since $\Eta$ is compact, this continuity implies uniform continuity.
    
    For any positive definite weight-matrix by \textcite[Lemma 2.3]{newey1994large} our criterion function has
    a unique optimum.
    The data, $\sigma^2_{t+1}, r_{t+1}$, are ergodic and stationary.
    Since the moment conditions are not redundant the optimal (GMM) weight matrix $\W$ is positive definite. 
    In addition, $\popmom$ is continuous at each $\eta$, given the restrictions above and properties of
    characteristic functions imply that $\popmom$ is uniformly bounded. 
    For convenience, we assume that the space of $\eta$ is compact.
    This should not be an issue here because the parameters  are either a priori bounded, such as $\phi$ or we
    have substantial a priori knowledge on their plausible magnitudes.
    Hence, \textcite[Theroem 2.6]{newey1994large} implies our estimator is consistent.
    
    However, when we allow for weak identification late on, we need this convergence to be uniform. 
    One straightforward way to show this is to show that our criterion function is globally Lipschitz in a set of
    high probability. 
    
    The other issue is that we need the weight matrix to converge uniformly to its expectation.
    Since the moments are continuous functions over their domain as is the square function.
    This convergence is uniform if and only if the matrix inverse is continuous.
    
    Since we have a finite number of non-redundant moments, the minimum eigenvalue, \\
    $\lambda_{min}\left(\W\left(\phi_0, \zeta_0, \pi \mvert \gamma_0\right)\right) > 0$, and so the matrix inverse
    is uniformly continuous in $\gamma_0$ with respect to the Frobenius norm, which is the sum of the eigenvalues.
    (Recall, that the eigenvalues of the inverse are the inverse of the eigenvalues.)


\end{proof}


\begin{assump}[Weak Dependence]
    \label{assumption:weak_dependence}
    $z_t \coloneqq \begin{pmatrix} r_{t+1} \\ \sigma^2_{t+1} \end{pmatrix}$ are $\alpha$-mixing with $\alpha_t =
       O\left(T^{-5}\right)$
\end{assump}


\begin{theorem}[Inference for $\eta$ under Strong Identification]
    Assume that $\phi_0  \in (-1,1) \setminus 0$, $\rho_0 \in [0,1)$, and $c_0 > 0$. 
    Further assume that the data are ergodic, stationary, and satisfy \cref{assumption:weak_dependence}.
    Then the following convergence in distribution holds.

    \begin{equation}
    \sqrt{T} (\widehat{\eta}_T - \eta_0) \dto \N\left(0, \left(G' \E[\W] G\right)^{-1}\right)
    \end{equation}
\end{theorem}

\begin{proof}

    By the above arguments, we have a consistent estimator for $\eta$ and the optimal weight matrix $\W \coloneqq
    (\E\left[g g'\right])^{-1}$, and we will assume that the true value $\eta_{0}$ is in the interior of its
    sample space $\Eta$.\footnote{Throughout we will use subscript \num{0}  to denote true values for parameters.}
    Let $G \coloneqq \E\left[\frac{\partial}{\partial \eta} \popmom \right]$ Clearly, $g$ is continuously
    differentiable, and its derivative $G$ is continuous.
    In addition, by the identification discussion $G' W \nabla G$ is nonsingular.
    
    %TODO Replace this with strong (alpha) mixing of order (1+\epsilon). 
    %Don't you need a lot more than this...?

    Since, $\norm*{g_t}$ is almost surely bounded by $1$ it has all of its moments and $z_t$ being $\alpha$-mixing
    implies $g_t$ is as well by the central limit theorem for strongly mixing process $\sqrt{T} \sampmom(\eta^{*})
    \dto \N\left(0, \E\left[\W\right]^{-1}\right)$ as required. 
    Consequently, by \textcite[theorem 3.2]{newey1994large} we have convergence in distribution as well as
    convergence in probability.
    

\end{proof}


\InferenceWeakID*

\begin{proof}
We prove this result by showing that Assumptions GMM 1-4 are satisfied.

\begin{proofpart}
    \label{part:main_theorem_proof_part1}
    In this part, we show that \nameref{ass:GMM1} is satisfied. 
    To do this, we break \nameref{ass:GMM1}  down into three subsections.
    Assumptions \namedref{ass:GMM1a}, \namedref{ass:GMM1b}, \namedref{ass:GMM1c}, and \namedref{ass:GMM1d} state
    that when $\phi = 0$, the moment conditions contain no information regarding $\pi$, but when $\phi \neq 0$,
    the model is identified.
    This is what we show in \cref{lemma:IdentifiedSet}.
    We further showed the relevant uniform convergence to verity \namedref{ass:GMM1b} in
    \cref{lemma:UniformConvergenceStrongID}.
    
    The next two assumptions (\namedref{ass:GMM1e} and \namedref{ass:GMM1f}) are  technical conditions regarding
    the behavior of the moment conditions and weight matrix. 
    Since our moment conditions are derived from an infinitely-differentiable  characteristic function and the
    weight matrix is the optimal one, they both hold trivially.
    
    The third subsection of Assumption \nameref{ass:GMM1} concerns the weight matrix.
    Since we are using the inverse covariance matrix of valid non-redundant model, assumptions
    \namedref{ass:GMM1g} and \namedref{ass:GMM1h} automatically hold.
    
    The last two assumptions, \namedref{ass:GMM1i} and \namedref{ass:GMM1j} require that the parameter spaces do
    not vary too much with the parameters and are compact.
    Since $\Eta$ is compact, \namedref{ass:GMM1i} holds trivially, and since it has  has a product form,
    \namedref{ass:GMM1j}  holds trivially as well.
    
\end{proofpart}


\begin{proofpart}
    \label{part:mainTheoremProofPart2}

    In this section, we show that the derivatives of the moment conditions have the correct behavior locally to
    the true parameters.
    We have to do this for the different classes of drifting sequences.
    We will do this by verifitying \cref{ass:GMM2}.
    This is valid since \textcite{andrews2014Gmm} show that this is a sufficient condition for their Assumption
    GMM2, which is what we actually need. 

    Our moment conditions are sample averages of the characteristic function, they satisfy \namedref{ass:GMM2a}
    automatically. 
    Since characteristic functions are uniformly bounded, by the dominated convergence theorem we can interchange the 
    expectation and derivative operators. 
    Hence \namedref{ass:GMM2b} and \namedref{ass:GMM2c} are equivalent to the statements in terms of the moment
    conditions themselves mutatis mutandis.  
    In addition, sice the derivate is a linear operator, we can pull it outside of the norm.
    The reason that the uniform law of large numbers in \cref{part:main_theorem_proof_part1} does not trivially
    imply this result is because we are not considering sequences $\phi_T \to \phi_0$. 


    We create a mean value expansions around around $(\phi_0, \zeta_T, \pi_T)$ of the sample moment condition and
    around $(\phi_0, \zeta_0, \pi_0)$ for the population moment condition.
    (This is not the same in both cases, not is it the true parameter for the drifting sequence in the case of the
    sample moment condition.)
    In addition, also since we are considering continuous functions of compact spaces --- the $\delta_T$ ball in
    $\R^{\dim(\eta)}$ --- pointwise convergence implies uniform convergence, and so we only need to show pointwise
    convergence below.

    \begin{alignat}{2}
        & &&\norm*{\sampmom(\phi_T, \zeta_T, \pi_T) -  \E\left[\popmom(\phi_T, \zeta_T, \pi_T) \mvert
          \gamma_0\right] } \\ 
        \intertext{We take a mean value expansion of both functions around $\eta_0$. The point at which the
        derivative in the two locations is taken may not be the same.}
        &= &&\left\lVert \sampmom(\phi_0, \zeta_0, \pi_0) + \frac{\partial}{\partial (\phi, \zeta,
           \pi)}\sampmom(\widetilde{\phi}^s, \widetilde{\zeta}^s, \widetilde{\pi}^s)\left((\phi_0, \zeta_0, \pi_0)
            - (\phi, \zeta, \pi)\right)\right. \\
        &  &&\quad \left. - \E\left[\popmom(\phi_0, \zeta_0, \pi_0) \mvert \gamma_0\right] + 
           \frac{\partial}{\partial (\phi, \zeta, \pi)} \E\left[\popmom(\widetilde{\phi}^p, \widetilde{\zeta}^p,
           \widetilde{\pi}^p)\mvert \gamma_0\right] \left((\phi_0, \zeta_0, \pi_0) - (\phi, \zeta,
           \pi)\right) \right\rVert \\ 
        \intertext{By the triangle inequality.}
        &\leq && \norm*{\sampmom(\phi_0, \zeta_0, \pi_0) - \E\left[\popmom(\phi_0, \zeta_0, \pi_0) \mvert
           \gamma_0\right]}  \\
        &+  && \norm*{\frac{\partial}{\partial (\phi, \zeta)}\sampmom(\widetilde{\phi}^s, \widetilde{\zeta}^s,
          \widetilde{\pi}^s)\left((\phi_0, \zeta_0)  - (\phi, \zeta)\right) -  \frac{\partial}{\partial (\phi,
          \zeta)} \E\left[\popmom(\widetilde{\phi}^s, \widetilde{\zeta}^p, \widetilde{\pi}^p)\mvert \gamma_0\right]
          \left((\phi_0, \zeta_0) - (\phi, \zeta)\right)} \\
        &+  && \norm*{\frac{\partial}{\partial \pi}\sampmom(\widetilde{\phi}^s, \widetilde{\zeta}^s,
          \widetilde{\pi}^s)\left(\pi_0 - \pi\right) -  \frac{\partial}{\partial \pi}
          \E\left[\popmom(\widetilde{\phi}^p, \widetilde{\zeta}^p, \widetilde{\pi}^p)\mvert \gamma_0\right]
          \left(\pi_0 - \pi\right)} 
          \label{eqn:pi_derivative_term}
    \end{alignat}

    By the uniform law of law numbers in \cref{part:main_theorem_proof_part1}, the first equation is $o_p(1)$.
    For $(\phi_0, \zeta_0) - (\phi, \zeta)$ small, the middle term is bounded by the quantity below. 


    \begin{equation}
        \norm*{\frac{\partial}{\partial (\phi, \zeta)}\sampmom(\widetilde{\phi}^s, \widetilde{\zeta}^s,
        \widetilde{\pi}^s) -  \frac{\partial}{\partial (\phi, \zeta)} \E\left[\popmom(\widetilde{\phi}^s,
        \widetilde{\zeta}^p, \widetilde{\pi}^p)\mvert \gamma_0\right]} \norm*{(\phi_0, \zeta_0) - (\phi, \zeta)}
    \end{equation}

    The first term is almost surely bounded, and the second term is less than $\delta_T$ by assumption, and so the
    product is $o_p(1)$.

    The hard part is the third expression.
    Like before we can bound the pull the $\pi_0 - \pi$ term out of the equation.
    However, this term is no longer converges to zero.
    We will consider the two cases, separately.
    Throughout, we will refer to the behavior of the following equation, which bounds
    \cref{eqn:pi_derivative_term}.


    \begin{equation}
        \norm*{\frac{\partial}{\partial \pi}\sampmom(\widetilde{\phi}^s, \widetilde{\zeta}^s, \widetilde{\pi}^s) -
        \frac{\partial}{\partial \pi} \E\left[\popmom(\widetilde{\phi}^p, \widetilde{\zeta}^p,
        \widetilde{\pi}^p)\mvert \gamma_0\right] } \norm*{\pi_0 - \pi} 
        \label{eqn:pi_derivative_norm_bound}
    \end{equation}

    In general, $\tilde{\pi}^s$ and $\tilde{\pi}^p$ can be arbitrarily far apart.
    However, since $\sampmom \pto \E\left[\popmom \mvert \gamma_0\right]$, and the limiting value is independent of
    $\pi$, the derivative does not depend upon $\pi$ asymptotically by the dominated convergence theorem.
    This applies that the difference between the two derivatives evaluated at different $\pi$ converges to zero.
    Consequently, \cref{eqn:pi_derivative_norm_bound} is $o_p(1)$ and we have shown \namedref{ass:GMM2b}.

    \begin{equation}
        \norm*{\abs*{\frac{\partial}{\partial \pi}\sampmom(\widetilde{\phi}^s, \widetilde{\zeta}^s,
        \widetilde{\pi}^s) - \frac{\partial}{\partial \pi} \E\left[\popmom(\widetilde{\phi}^p,
        \widetilde{\zeta}^p, \widetilde{\pi}^p)\mvert \gamma_0\right]} (1, 1, T)} \norm*{\pi_0 - \pi} 
        \label{eqn:pi_derivative_rescaled_bound}
    \end{equation}

    If we consider the setup in \namedref{ass:GMM2c}, $\tilde{\pi}^s$ and $\tilde{\pi}^p$ are now close together.
    However, we need to show that \cref{eqn:pi_derivative_rescaled_bound} is $o_p(1)$.
    Since we are considering the limiting behavior of a function with a continuous derivative, we can assume that
    the derivative is uniformly bounded without loss of generality. 
    (The constant might depend upon the true value, but not $T$.)
    By a Taylor series expansion of $\sampmom$ around the true value, $\frac{\widetilde{\pi}^p}{\sqrt{T}} \to 0$,
    and $\norm{\pi_0 - \pi} \propto \delta_T$ the result follows.

\end{proofpart}

\begin{proofpart}
    \label{part:mainTheoremProofPart3}

    Assumption \namedref{ass:GMM3a} is trivially satisfied,  and we showed that \namedref{ass:GMM3b} is satisfied
    in \cref{sec:GMM}.  
        
    The conceptual idea driving the reulsts this section is that moment conditions for each $T$ minus their
    conditional exceptions converge to a normal random variable.
    In other words, we are almost in a standard triangular C.L.T.\@ setup with weak time-dependence.

    In particular, both $\sigma^2_t$ and $r_t$ are infinitely differentiable functions of the innovations to
    the volatility and return processes and $\popmom$ are infinitely differentiable functions of $r_{t+1}$ and
    $r_t$ and the innovations are i.i.d.\@ across time by assumption.
    (Note, i.i.d.\@ implies strong mixing of any size.)
    Consequently, $\popmom$ is near epoch dependent (NED) of any size as defined in
    \textcite{andrews1991empirical}.  
    (Take $s>2.$) 
    By \textcite[Theorem 3]{andrews1991empirical}, we have the necessary finite-dimensional convergence in
    distribution to a Gaussian random variable. 

    We now show that \namedref{ass:GMM3d} holds.
    Clearly, $K_{t,g}\left(\eta \mvert \gamma^{*}\right)$ always exists.
    It uniformly converges because the derivatives of the moments are continuous functions of the data and the
    parameters, the process is ergodic, and the characteristic function lives on a compact set.
    It is also clearly continuous.
    By the dominated convergence theorem, we can exchange the derivative and expectations.
    In addition, since \popmom\ does not depend upon $\phi_T$, the limiting behavior is independent of the value
    of $\phi_T$.
    Hence \namedref{ass:GMM1d} holds.
    
    \namedref{ass:GMM3e} says the derivative of the moment function does not depend the true parameter $\phi$ if
    and only if  $\pi =  \pi_0$. 
    We showed this in the proof of \cref{part:mainTheoremProofPart2}.
    \namedref{ass:GMM3f} follows directly from the compactness of the parameter space and the continuity of the
    cross derivatives of \popmom\ by the dominated convergence theorem.

\end{proofpart}

\begin{proofpart}
    \label{mainTheoremProofPart4}
   
    \namedref{ass:GMM4a} holds trivially.
    We verified \namedref{ass:GMM4b} when we showed that $\pi$ and $\zeta$ are strongly identified
    (\cref{lemma:IdentifiedSet}).
    \namedref{ass:GMM4c} holds because the identification conditions do not create any singularity in the
    asymptotic covariance matrix. 

\end{proofpart}

\end{proof}





\end{appendices}


\end{document}



\documentclass[11pt, letterpaper, twoside]{article}
\usepackage{risk_price_inference}
\usepackage[autopunct=true, hyperref=true, doi=false, isbn=false, natbib=true,
url=false, eprint=false, style=chicago-authordate]{biblatex} 
\addbibresource{risk_bibliography.bib}

\author{Xu Cheng\thanks{University of Pennsylvania, The Perelman Center for Political Science and Economics, 133 South 36\textsuperscript{th} Street, Philadelphia, PA 19104, \href{mailto:xucheng@upenn.edu}{xucheng@upenn.edu}} \and Eric Renault\thanks{Brown University, Department of Economics -- Box B, 64 Waterman Street, Providence, RI 02912, \href{mailto:eric_renault@brown.edu}{eric\_renault@brown.edu}} \and Paul Sangrey\thanks{University of Pennsylvania, The Perelman Center for Political Science and Economics, 133 South 36\textsuperscript{th} Street, Philadelphia, PA 19104, \href{mailto:paul@sangrey.io}{paul@sangrey.io}}}

\title{Identification Robust Inference for Risk Prices in Structural Stochastic Volatility Models}

\date{\href{http://sangrey.io/risk_price_inference.pdf}{Current Version} \protect\\ This Version: \today}


\begin{document}

\begin{titlepage}


\maketitle
\thispagestyle{empty}
\addtocounter{page}{-1}

\begin{abstract} 

\singlespacing \noindent 
In structural stochastic volatility asset pricing models, changes in volatility affect risk premia through two channels: (1) the investor's willingness to bear high volatility in order to get high expected returns as measured by the market return risk price, and (2) the investor’s direct aversion to changes in future volatility as measured by the volatility risk price. Disentangling these channels is difficult and poses a subtle identification problem that invalidates standard inference. We adopt the discrete-time exponentially affine model of \textcite{han2018leverage}, which links the identification of the volatility risk price to the leverage effect. In particular, we develop a minimum distance criterion that links the market return risk price, the volatility risk price, and the leverage effect to well-behaved reduced-form parameters that govern the return and volatility's joint distribution. The link functions are almost flat if the leverage effect is close to zero, making estimating the volatility risk price difficult. We translate the conditional quasi-likelihood ratio test that \textcite{andrews2016conditional} develop in a nonlinear GMM framework to a minimum distance framework. The resulting conditional quasi-likelihood ratio test is uniformly valid. We invert this test to derive robust confidence sets that provide correct coverage for the risk prices regardless of the leverage effect's magnitude. 
%
\end{abstract} 

\medskip
\jelcodes{C12, C14, C38, C58, G12}

\medskip

\keywords{weak identification, robust inference, stochastic volatility, leverage, market return, risk premium, volatility risk premium, risk price, confidence set, asymptotic size}

\end{titlepage}

\clearpage

\section{Introduction}

A fundamental question in finance is how investors optimally trade off risk and return. Economic theory predicts investors demand a higher return as compensation for bearing more risk. Hence, we should expect a positive relationship between the mean and volatility of returns. Some seminal early papers proposed a static trade-off between risk and expected return, most notably the capital asset pricing model (CAPM) of \textcites{sharpe1964capital,lintner1965security}. In practice, volatility varies over time. Consequently, a significant strand of the recent literature examines the dynamic tradeoff between volatility and returns, including structural stochastic volatility models  such as \textcites{christoffersen2013capturing, bansal2014volatility, dewbecker2017price}.  In nonlinear models like these, investors care not just about how an asset's returns co-move with the volatility but also care how they co-move with changes in volatility. 

In these structural stochastic volatility models, changes in volatility affect risk premia through two channels: (1) the investor's willingness to tolerate high volatility in order to get high expected returns as measured by the market return risk price, and (2) the investor’s direct aversion to changes in future volatility as measured by the volatility risk price. We adopt the discrete-time exponentially affine model of \textcite{han2018leverage}, who represent the market return risk price and the volatility risk price by two structural parameters. In this model,  \textcite{han2018leverage} establish the significant result that the identification of the volatility risk price depends on a substantial leverage effect, which is the negative contemporaneous correlation between returns and volatility. 

Although the leverage effect is theoretically less than zero, it is difficult to quantify empirically, and its estimate usually is small \parencites{aitsahalia2013leverage}. When the leverage effect is small, the data only provide a limited amount of information about the volatility risk, compared to the finite-sample noise in the data. This low signal-to-noise ratio,  as modeled by weak identification, invalidates standard inference based on the generalized method of moments (GMM) estimator; see \textcites{stock2000GMM,andrews2012estimation}.

We provide an identification-robust confidence set for the structural parameters that measure the market return risk price, the volatility risk price, and the leverage effect. 
The robust confidence set provides correct asymptotic coverage, uniformly over a large set of models and allows for any magnitude of the leverage effect. This uniform validity is crucial for the confidence set to have good finite-sample coverage \parencites{mikusheva2007uniform, andrews2010asymptotic}. In contrast, standard confidence sets based on the GMM estimator and its asymptotic normality do not have uniform validity in the presence of a small leverage effect. This issue affects all of the structural parameters because they are estimated simultaneously.

We achieve robust inference in two steps. First, we establish a minimum distance criterion using link functions between the structural parameters and a set of reduced-form parameters that determine the joint distribution of the return and volatility. The structural model implies that the link functions are zero when evaluated at the true values of the structural parameters and the reduced-form parameters. Identification and estimation of these reduced form parameters are standard and are not affected by the presence of a small leverage effect. However, the link functions are almost flat in one of the structural parameters when the leverage effect is small, resulting in weak identification. Second, given this minimum distance criterion, we invert the conditional quasi-likelihood ratio (QLR) test by \textcite{andrews2016conditional} to construct a robust confidence set. The key feature of this test is that it treats the flat link functions as an infinite-dimensional nuisance parameter. The critical value is constructed by conditioning on a sufficient statistic for this nuisance parameter, and it is known to yield a valid test regardless of the nuisance parameter's value. \Textcite{andrews2016conditional} develop this test in a GMM framework. We show it works in minimum distance contexts such as the one considered here and provide conditions for its asymptotic validity. For practitioners, we provide a detailed algorithm for the construction of this simulation-based robust confidence set.


Our empirical results relates to the empirical analysis of the effect of volatility on risk premia. As \textcite{lettau2010measuring} mention,  the evidence here is inconclusive. \textcites{bollerslev1988capital, harvey1989timevarying, ghysels2005there, bali2006there, ludvigson2007empirical} find a positive relationship, while \textcites{campbell1987stock, breen1989economic, pagan1991nonparametric, whitelaw1994time, brandt2004relationship} find a negative relationship. Also, some papers use both a market return risk factor and a variance risk factor to explain the risk premia dynamics, including \textcites{christoffersen2013capturing, feunou2014risk, dewbecker2017price}. In related strand of the literature, \textcite{bollerslev2008risk, drechsler2011whats} document a substantial positive variance risk premium. We contribute to this literature by providing the first method for making valid inference for the market return risk price and the volatility risk price. This new confidence set not only allows for both effects but also takes into account the potential identification issue.

To have a non-linear relationship between changes in volatility and expected returns, we need either volatility of volatility (as used by \textcite{drechsler2011whats}) or jumps (as used by \textcite{drechsler2014uncertainty}).   Since we are working in discrete-time, it is far more natural to work volatility than with jumps. This is because discontinuities are not well-defined in discrete-time; all functions are continuous in the discrete-topology. The most straightforward models that allow for closed-form expressions for the risk prices are exponentially-affine (not affine) models. This is why they are frequently used  in the option-pricing literature.  In order to avoid complicating the analysis, we use such a model. We focus on the time-series behavior of the index because as a complement to, not a subtitle to, estimating the risk prices from cross-sectional or options pricing data. Using market-level variation over time to examine this non-linear relationship is a common approach in the literature, used by both the variance-premium literature and \textcite{han2018leverage}.  

%QUESTION: does the validity of cross-sectional asset pricing risk prices rely upon linearity?
%It is not obvious how to map a non-linear model for risk in the market return into a cross-sectional model of risk prices. In addition, since these risk prices are so fundamental to investors' preferences, there is certainly room for both approaches.


The weak identification issue studied in this paper is relevant in many economic applications, ranging from linear instrumental variable models \parencite{staiger1997instrumental} to nonlinear structural models \parencites{mavroeidis2014empirical, andrews2015maximum}. This paper is the first one to study this issue in structural asset pricing models with stochastic volatility. 

\Textcite{moreira2003conditional} introduces the conditional inference approach to the linear instrumental variable model creating the conditional likelihood-ratio (CLR) test, and \textcite{kleibergen2005testing} applies it to the nonlinear GMM problem.  \Textcites{magnusson2010identification, magnusson2010inference} extend \gentextcite{kleibergen2005testing} results to the minimum-distance case.  The key issue with these papers is that they rely exclusively upon local behavior of the moment-conditions. This is inherently under-powered in some environments. \Textcite{andrews2016conditional} resolve this issue by developing a global approach in the GMM case by proposing conditional inference for nonlinear GMM problems with an infinite-dimensional nuisance parameter. Their method is known to be the most-powerful in some special cases.  We develop a global weak-identification robust inference method for minimum distance estimation by extending \textcite{andrews2016conditional}. We bear the same relationship to  \textcites{magnusson2010identification, magnusson2010inference} that \textcite{andrews2016conditional} bears to \textcite{kleibergen2005testing}.  We also extend the scope of the application of these weak-identification robust methods to a new type of asset pricing model with substantial non-linearity and heteroskedasticity. 

The rest of the paper is organized as follows. \Cref{sec:model} provides the model and its parameterization. \Cref{sec:ilnk functions} provides model-implied restrictions and use them to derive the link function. \Cref{sec:robust inference} provides the asymptotic distribution of the reduced-form parameter and robust confidence sets for the structural parameter. A detailed algorithm to construct the robust confidence set is given in \cref{sec:conditional QLR}. 
\Cref{sec:simulation} show that the method works well in simulation, and \Cref{sec:empirical} applies the methods to data on the S\&P 500 providing estimates of the risk prices. \Cref{sec:risk_conclusion} concludes. 
Proofs are given in the appendix.

\section{Model}\label{sec:model}

This section provides a parametric structural model with stochastic volatility, following \textcite{han2018leverage}. They extend the discrete-time exponentially-affine model of \textcite{darolles2006structural}, and their model is a natural discrete-time analog of the \textcite{heston1993closedform} model. We specify this model using a stochastic discount factor (SDF), also called the pricing kernel, and the physical measure, which gives the joint distribution of the return and volatility dynamics.\footnote{The risk-neutral measure is unobserved due to the lack of option data.} We first define the SDF and parameterize it as an exponential affine function with unknown parameters. We then provide parametric distribution for the physical measure. 
  
Let $P_t$ be the price of the asset under consideration. Let $r_{t+1}=\log(P_{t+1}/P_t)-r_f$ denote the log excess return minus the risk-free rate and $\sigma^2_{t+1}$ denote the associated volatility. The observed data is $W_t=(r_t,\sigma^2_{t})$ for $t=1,\ldots,T$. 
Let $\F_t$ be the representative investor's information set at time $t$ . 

\subsection{Stochastic Discount Factor and Its Parameterization}\label{sec:deriving_sdf_functions}

The prices of all assets satisfy the following asset pricing equation in terms of the SDF:
%
\begin{equation}
  P_t = \E\left[M_{t,t+1} \exp\left(-r_f\right) P_{t+1} \mvert \F_t \right]. 
\end{equation}
%
  Following the definition of $r_{t+1}$, the pricing equation implies that for all assets
%
\begin{equation}
1 = \E\left[M_{t,t+1} \exp\left(r_{t+1}\right) \mvert \F_t \right].
\end{equation}

We start by parameterizing the SDF by the exponential affine model. Let $\pi$ be the price of volatility risk and $\kappa$ be the market return risk price. They are both considered as structural parameters.

\begin{definition}{Parameterizing the Stochastic Discount Factor}
 \label{defn:SDF}
%
 \begin{equation}
    M_{t,t+1}(\pi, \kappa) = \exp\left(m_{0} + m_1 \sigma_t^2 - \pi \sigma^2_{t+1} - \kappa r_{t+1}\right). 
 \end{equation}
\end{definition}

Throughout we assume that the two risks that command nonzero prices are the market return risk price and the volatility risk price. These two risks are closely related to the first two moments of $r_{t+1}$. Consequently, we only use variation in the first two moments of the data to estimate these parameters. 

\subsection{Parameterizing the Volatility and Return Dynamics}

Next, we parameterize the joint distribution of $\left\lbrace W_t:t=1,\ldots, T\right\rbrace $. 
Following \textcite{han2018leverage}, we make the following assumptions. First, the return $r_t$ and volatility $\sigma^2_t$ are first-order Markov. Second, there is no Granger-causality from the return to the volatility. Third, returns are independent across time given the volatility. We do allow $\sigma^2_{t}$ and $r_{t}$ to be contemporaneously correlated, as they are in the data. 

Under these assumptions, the volatility drives all of the dynamics of the process. The only relevant information in the information set $\F_{t}$ for time $t+1$-measurable variables is contained in $\sigma^2_t$. In general, $\sigma^2_t$, $\sigma^2_{t+1}$, and $r_{t+1}$ form a sufficient statistic for $\F_{t+1}$. 

We adopt the conditional autoregressive gamma process as in \textcite{gourieroux2006autoregressive, han2018leverage} for the volatility process. The model is parameterized in terms of the Laplace transform: 
%
\begin{equation}
    \E\left[\exp(-x \sigma^2_{t+1}) \mvert \F_{t}\right] = \exp\left(- A(x) \sigma^2_{t} - B(x)\right)
    \label{eqn:vol_laplace_transform}
\end{equation}
%
for all $x \in \R$. The function $A(x)$ and $B(x)$ are parameterized as follows.

\begin{definition}{Parameterize the Volatility Dynamics}
     \label{defn:physical_vol_dynamics}
     \begin{align}
        \label{defn:a_PP}
        A(x) &\coloneqq \frac{\rho x}{1 + c x}, \\
        \label{defn:b_PP}
        B(x) &\coloneqq \delta \log(1 + c x),
     \end{align}
with $\rho \in [0,1-\epsilon],$ $c > \epsilon$, $\delta > \epsilon$ for some $\epsilon > 0$.
\end{definition}

In this specification, $\rho$ is a persistence parameter, $c$ is a scale parameter, and $\delta$ is a level parameter. We can see this clearly in the following conditional mean and variance formulas for $\sigma^2_{t+1}$.

\begin{remark}[Volatilty Moment Conditions] 
 \label{remark:vol_moment_conditions}
    \begin{align}
        \E\left[\sigma^2_{t+1} \mvert \sigma^2_t \right] &= \rho \sigma^2_t + c \delta,\\
%   
        \Var\left[\sigma^2_{t+1} \mvert \sigma^2_t \right] &= 2 c \rho \sigma^2_t + c^2 \delta.
%   
    \end{align}
\end{remark}

Next, we model the return dynamics. Similar to the volatility dynamics, the distribution of $r_t$ given both $\sigma^2_{t+1}$ and $\sigma^2_{t}$ is specified in terms of the Laplace transform:
%
\begin{equation}
    \label{eqn:return_laplace_transform}
    \E\left[\exp(- x r_{t+1}) \mvert \F_{t}, \sigma^2_{t+1} \right] = \exp\left(- C(x) \sigma^2_{t+1} - D(x) \sigma^2_t - E(x)\right)
\end{equation}
%
for all $x \in \R$. The function $C(x)$, $D(x)$, and $E(x)$ are parameterized as follows such that the return has a conditional Gaussian distribution.

\begin{definition}{Parameterizing the Return Dynamics}
    \label{defn:physical_return_dynamics}
    \begin{align}
        C(x) &\coloneqq \psi x - \frac{1 - \phi^2}{2} x^2,\\
        D(x) &\coloneqq \beta x, \\
        E(x) &\coloneqq \gamma x,
    \end{align}
with $\phi \in [-1+\epsilon, 0]$ for some $\epsilon>0$.
\end{definition}

Under this specification, we have the following representation of the conditional mean and variance for $r_{t+1}$.

\begin{remark}[Return Moment Conditions] 
	\label{remark:return_moment_conditions}
	\begin{align}
		\label{eqn:rtn_cond_mean}
		\E\left[r_{t+1} \mvert \sigma^2_t, \sigma^2_{t+1}\right] = \psi \sigma^2_{t+1} + \beta \sigma^2_t + \gamma, \\
		%   
		\label{eqn:rtn_cond_vol}
		\Var\left[r_{t+1} \mvert \sigma^2_t, \sigma^2_{t+1}\right] = (1 - \phi^2) \sigma^2_{t+1}.
		%   
	\end{align}
\end{remark}


The parameter $\phi$ represents the leverage effect because it measures the reduction in return's volatility caused by conditioning on the volatility path. 

\section{Link Functions}\label{sec:ilnk functions}

So far, we have introduced the following parameters: $(m_{0},m_{1},\kappa ,\pi )$ in SDF, $(\rho ,c,\delta)$ for the volatility dynamics, and $(\psi ,\beta ,\gamma ,\phi )$ for the return dynamics. Next, we explore restrictions among these parameters that are consistent with this model. In other words, not all of these parameters can change freely under the structural model.

We use these restrictions to construct link functions between a set of reduced-form parameters and a set of structural parameters. These link functions play an important role in separating the regularly behaved reduced-form parameters from the structural parameters. They also are used to conduct identification robust inference for the structural parameters based on a minimum distance criterion.
All of these restrictions are also imposed in the GMM estimation in \textcite{han2018leverage}. However, because the volatility risk price is weakly identified, they calibrate it instead of estimating it. Given this calibrated value, they proceed to estimate all other parameters with GMM. 

\subsection{Pricing Equation Restrictions}

We first explore restrictions implied by the pricing equation $\E[ M_{t,t+1}\exp (r_{t+1}) \ivert \F_{t}]=1$. We start with a simple result stating that the constants $m_{0}$ and $m_{1}$ are normalization constants implied by all the other parameters. Thus, $m_{0}$ and $m_{1}$ are not free parameters to be estimated. Instead, they should take the values given below, once other parameters are specified. These restrictions on $m_{0}$ and $m_{1}$ are obtained by applying the restriction $\mathbb{E[} M_{t,t+1}\exp (r_{t+1})|\mathcal{F}_{t}]=1$ to the risk free asset. Applying the same argument to any other asset, we also obtain another set of two restrictions, which can be written in terms of the coefficients $\beta $ and $ \gamma $ under the linear form of $D(x)$ and $E(x)$.

\begin{lemma}
    \label{Lemma m0 and m1}
    Given the parameterization in the model, the pricing equation \newline $\E[M_{t,t+1}\exp (r_{t+1}) \ivert \F_{t}]=1$ implies that\footnote{\Cref{proof:lemma_m0_and_m1}}
%
    \begin{align*}
        m_{0} &= E(\kappa )+B\left( \pi +C\left( \kappa \right) \right) , \\
%
        m_{1} &= D\left( \kappa \right) +A\left( \pi +C\left( \kappa \right) \right) ,
    \end{align*}
    %
    and
%
    \begin{align*}
        \gamma  &= B\left( \pi +C\left( \kappa -1\right) \right) -B\left( \pi +C\left( \kappa \right) \right), \\
        \beta  &= A\left( \pi +C\left( \kappa -1\right) \right) -A\left( \pi +C\left( \kappa \right) \right).
    \end{align*}
\end{lemma}

The two equalities on $\beta $ and $\gamma $ link them to the market return risk price, $\kappa$, and the volatility risk price, $\pi$,  through the functions $A(\cdot),B(\cdot ),C(\cdot ),$ which also involve the parameters $(\rho, c,\delta, \psi, \phi).$ We treat these two equalities as link functions in the minimum distance criterion specified below.

\subsection{Leverage Effect Restrictions}\label{sec:leverage effect restrict}


Following \textcite{han2018leverage}, we parameterize $\psi$ as 
%
\begin{equation}
    \label{eqn:leverage restriction}
    \psi = \frac{\phi}{\sqrt{2c}} - \frac{1 - \phi^2}{2} + (1-\phi^2) \kappa.
\end{equation}
%
The first part $\phi / \sqrt{2 c}$ measures the leverage effect arising from the instantaneous correlation between $r_{t+1}$ and $\sigma^2_{t+1}$. 
The second part is the traditional Jensen effect term that arises from taking expectation of a log-Gaussian random variable.  The third term arises from risk-aversion, which is why it is proportional to $\kappa$.
%By noting that 
% In particular, it measures the reduction in the return
% We first show that both parameter $\psi $ and $\phi $ are linked to the leverage effect. Given the variance of $r_{t+1}$ conditional on $(\sigma _{t+1}^{2},\sigma _{t}^{2}),$ specified in \cref{eqn:rtn_cond_vol}, we have
% %
% \begin{equation*}
%     \phi ^{2}=\sigma _{t+1}^{2}-\Var[r_{t+1}|\sigma _{t+1}^{2},\sigma _{t}^{2}].
% \end{equation*}
% %
% This shows that $\phi $ is linked to the leverage effect because it measures the return volatility reduction after conditioning on the volatility path.  On the other hand, given the mean of $r_{t+1}$ conditional on $(\sigma _{t+1}^{2},\sigma _{t}^{2}),$ specified in \cref{eqn:rtn_cond_mean}, we have\footnote{To see this result, note that the mean of $r_{t+1}-\psi \sigma _{t+1}^{2}$ given $(\sigma _{t+1}^{2},\sigma _{t}^{2})$ does not depend on $\sigma
% _{t+1}^{2}$.}
%
%\begin{equation}
%    \E[r_{t+1}|\sigma _{t+1}^{2},\sigma _{t}^{2}]-E[r_{t+1}|\sigma _{t}^{2}]=\psi \left \{ \sigma _{t+1}^{2}-E[\sigma _{t+1}^{2}|\sigma _{t}^{2}]\right \},
%    \label{eqn:vol_versus_psi}
%\end{equation}
%%
%we can solve for $k$ using our parametric model, assuming it is time-invariant.
% In addition, \textcite{han2018leverage} show that $k$ is the value under which $\mathbb{C}\mathrm{orr}[r_{t+1},\sigma _{t+1}^{2}|\sigma _{t}^{2}]=\phi $ if this correlation is indeed time invariant. Guided by this condition, they show that $k=1/(2c)^{1/2}$ should be used for the volatility dynamic specified in \cref{eqn:vol_versus_psi}  and \cref{eqn:leverage restriction}.


\subsection{Structural and Reduced-Form Parameters}

Because $\phi$ is the leverage effect parameter, we group it together with market return risk price, $\kappa$, and the volatility risk price, $\pi$, and call $ \theta \coloneqq (\kappa ,\pi ,\phi )^{\prime}$ structural parameters. These structural parameters are estimated by restrictions from this structural model. In contrast, the other parameters in the conditional mean and variance of the return and volatility, see \cref{remark:vol_moment_conditions} and \cref{remark:return_moment_conditions}, are simply estimated using these moments, without any model restrictions. As such, we call them the reduced-form parameters. Because $1-\phi ^{2}$ shows up in the conditional variance of $r_{t+1},$ see \cref{eqn:rtn_cond_vol}, we define $\zeta =1-\phi ^{2}$ as a reduced-form parameter and link it to the structural parameter $\phi$ through this relationship. To sum up, the reduced-form parameters are $\omega \coloneqq (\rho, c,\delta, \psi, \beta, \gamma, \zeta )^{\prime }$.

Using $\zeta $ as a reduced-form parameter has the additional benefit of avoiding estimating $\phi$ directly. Estimating $\phi $ when its true value is close to 0 results in an estimator with a non-standard asymptotic distribution due to the boundary constraint. The inference procedure below does not require estimation of $\phi$ and is uniform over $\phi$ even if its true value is on or close to the boundary $0$. It is worth noting that this boundary condition gives us additional information in estimating $\phi$ in some cases. The estimator for $\phi$ may converge quite rapidly; however, it is almost certainly not asymptotically approximately Gaussian. In addition, we cannot recover asymptotic Gaussianity by removing this constraint. Even though, $\phi$ could conceivably be greater than $0$, $\phi^2$ cannot conceivably be  less than $0$. The $\phi$ parameter enters the last link in \cref{eqn:link_function_g} through $\phi^2$.
This is where the non-standard behavior arises. Economically, we saying that the variance of $r_{t+1}$ must reduce when we condition on more information. Although, this is clearly in population, it may not hold for the sample variances.

The link functions between the structural parameter $\theta$ and the reduced-form parameter $\omega $ are collected together in
%
\begin{equation}
    \label{eqn:link_function_g}
   g(\theta, \omega) = 
%
    \begin{pmatrix}
        \gamma - [B\left( \pi +C\left( \kappa -1\right) \right) -B\left( \pi +C\left( \kappa \right) \right)] \\ 
        \beta - [A\left( \pi +C\left( \kappa -1\right) \right) -A\left( \pi +C\left( \kappa \right) \right)] \\ 
        \psi -(1-\phi ^{2})\kappa +\frac{1}{2}(1-\phi ^{2})-1/(2c)^{1/2}\phi  \\ 
        \zeta -\left( 1-\phi ^{2}\right) 
    \end{pmatrix}.
\end{equation}
%
For the inference problem studied below, we know $g(\theta _{0},\omega_{0})=0$ when evaluated at the true value of $\theta $ and $\omega .$

\subsection{Identification}

One of the important contributions of \textcite{han2018leverage} is to establish the relationship between the identification of the volatility risk price and the leverage effect. In particular, they show that when the leverage effect parameter $\phi =0,$ the volatility risk price $\pi $ is not identified. To see this result, note that the only source of identification information on $\pi $ are the first two link functions in $g(\theta _{0},\omega _{0})=0$, which come from \cref{Lemma m0 and m1}. Clearly, these two equations are independent of $\pi$ if $C(\kappa )=C(\kappa -1)$. Using the definition of $C(\cdot)$ and \cref{eqn:leverage restriction}, we have 
%
\begin{equation}
    C(\kappa )-C(\kappa -1)=\psi -(1-\phi ^{2})\left( \kappa -\frac{1}{2}\right) = \frac{\phi}{\sqrt{2 c}}.
\end{equation}
%
Clearly, the strength of identification is governed by the strength of the leverage effect.
In other words, we need $\phi \neq 0$ to identify the volatility risk price $\pi$.

Even if $\phi \neq 0$, we do not know it. In practice, with a finite-sample size and different types of noise in the data, such as measurement errors and omitted variables, a substantial leverage effect is required to obtain a standard identification situation where the noise in the data is negligible compared to the information to identify $\pi$. However, if only a small leverage effect is found, as in \textcites{bandi2012timevarying, aitsahalia2013leverage}, or the magnitude of the leverage effect is completely unknown, an identification robust procedure is needed to conduct inference in this problem. In addition, standard minimum-distance estimators do not provide valid inference when some of the first-stage parameters are either asymptotically non-Gaussian or the link functions are ill-behaved. In our case, we should not expect $\phi$ to be asymptotically Gaussian even though it is well identified. We provide a procedure that is robust to both non-standard issues now.

\section{Robust Inference for Risk Prices}\label{sec:robust inference}

\subsection{Asymptotic Distribution of the Reduced-Form Parameter}

Write $\omega = (\omega _{1},\omega _{2},\omega _{3})^{\prime },$ where $\omega _{1}=(\rho ,c,\delta)' \in O_{1},$ $\omega _{2}=(\gamma ,\beta ,\psi)' \in O_{2}$, and $\omega _{3}=\zeta \in O_{3}.$ The parameter space for $ \omega $ is $O=O_{1}\times O_{2}\times O_{3}\subset R^{d_{\omega }}$. The true value of $\omega $ is assumed to be in the interior of the parameter
space.

Below we describe the estimator $\widehat{\omega } \coloneqq (\widehat{\omega }_{1}, \widehat{\omega }_{2},\widehat{\omega }_{3})^{\prime }$ and provide its asymptotic distribution. We estimate these parameters separately because $\omega _{1}$ only shows up in the conditional mean and variance of $\sigma _{t+1}^{2}$, $\omega_{2}$ only shows up in the conditional mean of $r_{t+1}$, and $\omega _{3}$ only shows up in the conditional variance of $
r_{t+1}.$

We first estimate $\omega _{1}=(\rho,c)'$ based on the conditional mean and variance of $\sigma _{t+1}^{2}$, which can be equivalently written as 
%
\begin{align}
    E[\sigma _{t+1}^{2}|\sigma _{t}^{2}] &= A\text{ and }E[\sigma _{t+1}^{4}|\sigma _{t}^{2}]=B,\text{ where }  \nonumber \\
%
    A &= \rho \sigma _{t}^{2}+c\delta \text{ and }B=A^{2}+\left( 2c\rho \sigma _{t}^{2}+c^{2}\delta \right) .
\end{align}
%
Because the conditional mean of $\sigma _{t+1}^{2}$ and $\sigma _{t+1}^{4}$ are linear and quadratic functions, respectively, of the conditioning variable $\sigma _{t}^{2}$, they can be transformed to the unconditional moments
%
\begin{equation}
    E[h_{t}(\omega _{10})]=0,\text{ where }h_{t}(\omega _{1})=[(1,\sigma _{t}^{2})\otimes (\sigma _{t+1}^{2}-A),(1,\sigma _{t}^{2},\sigma _{t}^{4})\otimes (\sigma _{t+1}^{4}-B)]^{\prime },
\end{equation}
%
and $\omega _{10}$ represents the true value of $\omega _{1}$. The two-step GMM estimator of $\omega _{1}$ is%
%
\begin{equation}
    \label{omega 1 est}
    \widehat{\omega }_{1}=\underset{\omega _{1}\in O_{1}}{\arg \min }\left( T^{-1}\sum_{t=1}^{T}h_{t}(\omega _{1})\right) ^{\prime }\widehat{V}_{1}\left( T^{-1}\sum_{t=1}^{T}h_{t}(\omega _{1})\right) ,
\end{equation}%
%
where $\widehat{V}_{1}$ is a consistent estimator of $V_{1} \coloneqq \sum_{m=-\infty }^{\infty }\Cov[h_{t}(\omega _{10}),h_{t+m}(\omega _{10})].$

We estimate $\omega _{2}$ by the generalized least squares (GLS) estimator because the conditional mean of $r_{t+1}$ is a linear function of the conditioning variable $\sigma _{t}^{2}$ and $\sigma _{t+1}^{2}$ and the conditional variance is proportional to $\sigma _{t+1}^{2}.$ The GLS estimator of $\omega _{2}$ is
%
\begin{align}
    \widehat{\omega }_{2} &= \left( \sum_{t=1}^{T}x_{t}x_{t}^{\prime }\right) ^{-1}\sum_{t=1}^{T}x_{t}y_{t},\text{ where }  \notag \\ 
%
    x_{t} &= \sigma _{t+1}^{-1}(1,\sigma _{t}^{2},\sigma _{t+1}^{2})^{\prime } \text{ and }y_{t}=\sigma _{t+1}^{-1}r_{t+1}.  \label{omega 2 est}
\end{align}
%
We estimate $\omega_{3}$ by the sample variance estimator:
%
\begin{equation}
    \label{omega 3 est}
    \widehat{\omega }_{3}=T^{-1}\sum_{t=1}^{T}\left( y_{t}-\widehat{y}_{t}\right) ^{2},\text{ where }\widehat{y}_{t}=x_{t}^{\prime }\widehat{ \omega }_{2}.  
\end{equation}

Let $P$ denote the distribution of the data $\{W_{t}=(r_{t+1}, \sigma _{t+1}^{2}):t\geq 1\}$ and $\mathcal{P}$ denote the parameter space of $P$. Note that the true values of the structural parameter and the reduced-form parameters are all determined by $P.$ We allow $P$ to change with $T.$ For notational simplicity, the dependence on $P$ and $T$ is suppressed.

Let 
%
\begin{equation}
    f_{t}(\omega) = 
%
    \begin{pmatrix}
        h_{t}(\omega _{1}) \\ 
        x_{t}(y_{t}-x_{t}^{\prime }\omega _{2}) \\ 
        (y_{t}-x_{t}^{\prime }\omega _{2})^{2}%
    \end{pmatrix}
%
     \in R^{d_{f}}\text{ and } 
%
     V =\sum_{m=-\infty }^{\infty }\Cov\left[f_{t}(\omega _{0}),f_{t+m}(\omega _{0})\right].
\end{equation}
%
The estimator $\widehat{\omega }$ defined above is based on the first moment of $f_{t}(\omega ).$ Thus, the limiting distribution of $\widehat{\omega }$ relates to the limiting distribution of $T^{-1/2}\sum_{t=1}^{T}(f_{t}(\omega _{0})-\E[f_{t}(\omega _{0}])$ following from the central limit theorem. Furthermore, because $\omega _{1}$ is the GMM estimator based on some nonlinear moment conditions, we need uniform convergence of the sample moments and their derivatives to show the consistency and asymptotic normality of $\widehat{\omega }_{1}.$ These uniform convergence follows from the uniform law of large numbers. Because $\widehat{\omega }_{2}$ is a simple OLS estimator by regressing $y_{t}$ and $x_{t},$ we need the regressors to not exhibit multicollinearity. We make the necessary assumptions below. All of them are easily verifiable with weakly dependent time series data.

Let $\widehat{V}$ denote a heteroskedasticity and autocorrelation consistent (HAC) estimator of $V$. The estimator $\widehat{V}_{1}$ is a submatrix of $\widehat{V}$ associate with $V_{1}.$ Let $H_{t}(\omega _{1})=\partial h_{t}(\omega _{1})/\partial \omega _{1}^{\prime }.$

\begin{assumpR}
    \label{assump:R}
    The following conditions hold uniformly over $P\in \mathcal{P}$, for some fixed $0 < C < \infty$.
    
    \begin{enumerate}
        \item $T^{-1}\sum_{t=1}^{T}(h_{t}(\omega_{1})-\E[ h_{t}(\omega _{1}))\rightarrow _{p}0$ and $T^{-1}\sum_{t=1}^{T}(H_{t}(\omega _{1})-\E[H_{t}(\omega _{1})])\rightarrow _{p}0,$ $\E[H_{t}(\omega _{1})]$ is continuous in $\omega _{1},$ all uniformly over the parameter space of $\omega _{1}$.
    %
        \item $T^{-1}\sum_{t=1}^{T}(x_{t}x_{t}^{\prime }-\E[ x_{t}x_{t}^{\prime }{])\rightarrow }_{p}0.$
    %
        \item $V^{-1/2}\{T^{-1/2}(\sum_{t=1}^{T}f_{t}(\omega _{0})-\E[f_{t}(\omega _{0}){]\} \rightarrow }_{d}N(0,I)$ and $\widehat{V} -V\rightarrow _{p}0.$
    %
        \item $C^{-1}\leq \lambda_{\min }(A)\leq \lambda_{\max }(A)\leq C$ for $A=V,\E[H_{t}\left( \omega _{1,0}\right) ^{\prime }H_{t}\left( \omega _{1,0}\right) ]),\E[x_{t}x_{t}^{\prime }],\E[ z_{t}z_{t}^{\prime }],$ where $z_{t}=(1,\sigma _{t}^{2},\sigma _{t}^{4})^{\prime}$.\footnote{We use $\lambda(\mathrm{matrix})$ to denote the eigenvalue of the matrix.}
    %
    \end{enumerate}
\end{assumpR}


Let $H(\omega _{1})=\mathbb{E[}H_{t}(\omega _{1})]$ and $\overline{H}(\omega _{1})=T^{-1}\sum_{t=1}^{T}H_{t}(\omega _{1}).$ Define
%
\begin{align}
%
    \mathcal{B} &= \diag\{[H(\omega _{10})V_{1}^{-1}H(\omega _{10})]^{-1}H(\omega _{10})V_{1}^{-1},\mathbb{E[}x_{t}x_{t}^{\prime }]^{-1},1\},  \notag \\
%
    \widehat{\mathcal{B}} &= \diag\{[\overline{H}(\widehat{\omega }_{1})^{\prime } \widehat{V}_{1}^{-1}\overline{H}(\widehat{\omega }_{1})]^{-1}\overline{H}( \widehat{\omega }_{1})^{\prime }\widehat{V}_{1}^{-1},[T^{-1}
\sum_{t=1}^{T}x_{t}x_{t}^{\prime }]^{-1},1\}.  
%
    \label{Fhat}
\end{align}
%
The following lemma provides the asymptotic distribution of the reduced-form parameter and a consistent estimator of its asymptotic covariance. Note, we put the asymptotic covariance on the left side of the convergence to allow the distribution of the data to change with sample size $T$.

\begin{lemma}
\label{Lemma Reduce}
Suppose \Cref{assump:R} holds. The following results hold uniformly over $P\in \mathcal{P}$.\footnote{\Cref{proof:lemma_reduce}}

    \begin{equation*} 
        \xi _{T}:=\Omega ^{-1/2}T^{-1/2}(\widehat{\omega } -\omega _{0})\rightarrow _{d}\xi \sim N(0,I), \text{  where } \Omega =\mathcal{B}V \mathcal{B}^{\prime }, 
    \end{equation*} 
%
    and 
%
    \begin{equation*}
        \widehat{\Omega }-\Omega \rightarrow_{p}0, \text{ where } \widehat{\Omega }=\widehat{\mathcal{B}}\widehat{V}\widehat{\mathcal{B}}^{\prime}.
    \end{equation*}
\end{lemma}

\subsection{Weak Identification}

The true value of the structural parameter $\theta$ and the reduced-form parameter $\omega$ satisfy the link function $g(\theta_{0},\omega _{0})=0$. In a standard problem without any identification issues, we can estimate $\theta _{0}$ by the minimum distance estimator $\widehat{\theta} =(\widehat{\kappa },\widehat{\pi },\widehat{\phi})'$, which minimizes $ Q_{T}(\theta )=g(\theta ,\widehat{\omega })^{\prime }W_{T}g(\theta , \widehat{\omega })$ for some weighting matrix $W_{T}$, and construct tests and confidence sets for $\theta _{0}$ using an asymptotically normal approximation for $T^{1/2}(\widehat{\theta }-\theta _{0})$. However, this standard method does not work in the present problem when $\pi _{0}$ is only weakly identified. In this case, $g(\theta ,\widehat{\omega })$ is almost flat in $\pi $ and the minimum distance estimator of $\widehat{\pi }$ is not even consistent. To make the problem even more complicated, the inconsistency of $\widehat{\pi }$ has a spillover effect on $\widehat{\kappa }$ and $\widehat{\phi}$, making the distribution of $\widehat{\kappa }$ and $\widehat{\phi }$ non-normal even in large samples.

Before presenting the robust confidence set, we first introduce some useful quantities and provide a heuristic discussion of the identification problem and its consequences. Let $G(\theta ,\omega )$ denote the partial derivative of $ g(\theta ,\omega )$ with respect to (w.r.t.)\@ $\omega .$ Let $g_{0}(\theta )=g(\theta ,\omega _{0})$ and $G_{0}(\theta )=G(\theta ,\omega _{0})$ be the link function and its derivative evaluated at $\omega _{0}$ and $\widehat{g}(\theta
)=g(\theta ,\widehat{\omega })$ and $\widehat{G}(\theta )=G(\theta , \widehat{\omega })$ be the same quantities evaluated at the estimator $ \widehat{\omega }.$ The delta method gives 
%
\begin{equation}
    \eta _{T}(\theta ):=T^{1/2}\left[ \widehat{g}(\theta )-g_{0}(\theta ) \right] =G_{0}(\theta )\Omega ^{1/2}\cdot \xi _{T}+o_{p}(1),
    \label{emp pro}
\end{equation}
%
where $\xi _{T}\rightarrow _{d}N(0,I)$ following \cref{Lemma Reduce}.  Thus, $\eta _{T}(\cdot )$ weakly converges to a Gaussian process $\eta (\cdot )$ with covariance function $\Sigma (\theta _{1},\theta _{2})=G_{0}(\theta _{1})\Omega G_{0}(\theta _{2})^{\prime }.$

Following \cref{emp pro}, we can write $T^{1/2}\widehat{g}(\theta )=\eta _{T}(\theta )+T^{1/2}g_{0}(\theta ),$ where $\eta _{T}(\theta )$ is the noise from the reduced-form parameter estimation and $T^{1/2}g_{0}(\theta )$ is the signal from the link function. Under weak identification, $ g_{0}(\theta )$ is almost flat in $\theta ,$ modeled as the signal $ T^{1/2}g_{0}(\theta )$ being finite even for $\theta \neq \theta _{0}$ and $T\rightarrow \infty .$ Thus, the signal and the noise are of the same order of magnitude, yielding an inconsistent minimum distance estimator $ \widehat{\theta }.$ This is in contrast with the strong identification scenario, where $T^{1/2}g_{0}(\theta )\rightarrow \infty $ for $\theta \neq \theta _{0}$ as $T\rightarrow \infty $ and $g_{0}(\theta _{0})=0.$ In this case, the signal is strong enough that the minimum distance estimator is consistent.

The identification strength of $\theta _{0}$ is determined by the function $ T^{1/2}g_{0}(\theta ).$ However, this function is unknown and cannot be consistently estimated (due to $T^{1/2}$). Thus, we take the conditional inference procedure as in \textcite{andrews2016conditional} and view $ T^{1/2}g_{0}(\theta )$ as an infinite dimensional nuisance parameter for the inference of $\theta _{0}$. The goal is to construct robust confidence set for $\theta _{0}$ that has correct size asymptotically regardless of this unknown nuisance parameter.

\subsection{Conditional QLR Test}\label{sec:conditional QLR}

We construct a confidence set for $\theta \in \Theta \coloneqq [0, M_1] \times [-M_2, 0] \times [1 - \epsilon, 0]$ by inverting the test $ H_{0}: \theta =\theta_{0}$ vs $H_{1}: \theta \neq \theta _{0}$, where $M_1$ and $M_2$ are large positive constants and $\epsilon$ is a small positive constant. The test statistic is a QLR statistic that takes the form
%
\begin{equation}
    QLR(\theta _{0}) \coloneqq T\widehat{g}(\theta _{0})^{\prime}\widehat{\Sigma} (\theta _{0},\theta _{0})^{-1}\widehat{g}(\theta _{0})-\underset{\theta \in \Theta }{\min }T\widehat{g}(\theta )^{\prime }\widehat{\Sigma } (\theta ,\theta )^{-1}\widehat{g}(\theta ),  
    \label{QLR stat}
\end{equation}
%
where $\widehat{\Sigma }(\theta _{1},\theta _{2},)=\widehat{G}(\theta _{1})\widehat{\Omega }\widehat{G}(\theta _{2})^{\prime }$ and $\widehat{ \Omega }$ is the consistent estimator of $\Omega $ defined above.

\Textcite{andrews2016conditional} provide the conditional QLR test in a nonlinear GMM problem, where $\widehat{g}(\theta )$ is replaced by a sample moment. The same method can be applied to the present nonlinear minimum distance problem. Following \textcite{andrews2016conditional}, we first project $\widehat{g}(\theta )$ onto $\widehat{g}(\theta _{0})$ and construct a residual process
%
\begin{equation}
    \widehat{r}(\theta )=\widehat{g}(\theta )-\widehat{\Sigma }(\theta ,\theta _{0})\widehat{\Sigma }(\theta _{0},\theta _{0})^{-1}\widehat{g} (\theta _{0}).  
    \label{red process}
\end{equation}
%
The limiting distributions of $\widehat{r}(\theta )$ and $\widehat{g} (\theta _{0})$ are Gaussian and independent. Thus, conditional on $\widehat{ r}(\theta ),$ the asymptotic distribution of $\widehat{g}(\theta )$ no longer depends on the nuisance parameter, $T^{1/2}g_{0}(\theta ),$ making the procedure robust to any identification strength.

Specifically, we obtain the $1-\alpha $ conditional quantile of the QLR statistic, denoted by $c_{1-\alpha }(r,\theta _{0}),$ as follows. For $b=1,\ldots,B$, we take independent draws $\eta _{b}^{\ast }\sim N(0,\widehat{\Sigma }(\theta _{0},\theta _{0}))$ and produce a simulated process, 
%
\begin{equation}
    g_{b}^{\ast }(\theta ) \coloneqq \widehat{r}(\theta ) + \widehat{\Sigma }(\theta ,\theta _{0})\widehat{\Sigma }(\theta _{0},\theta _{0})^{-1}\eta _{b}^{\ast},
\end{equation}
%
and a simulated statistic,
%
\begin{equation}
    QLR_{b}^{\ast }(\theta _{0}) \coloneqq T  {\eta_{b}^{\ast}}^{\prime} \widehat{\Sigma} (\theta _{0},\theta _{0})^{-1} \eta_{b}^{\ast} -   \underset{\theta \in \Pi }{\min }Tg_b^{\ast }(\theta )^{\prime }\widehat{\Sigma } (\theta ,\theta )^{-1}g_{b}^{\ast }(\theta ).
\end{equation}
%
Let $b_{0}=\lceil (1-\alpha )B\rceil ,$ the smallest integer greater than or equal to $(1-\alpha )B$. Then the critical value $c_{1-\alpha }(r,\theta _{0})$ is the $b_{0}^{th}$ smallest value among $\{QLR_{b}^{\ast },b=1,\ldots,B\}$.
We execute the steps reported in \cref{alg:constructing_the_cs} to form a robust confidence set for $\theta$.

\begin{algorithm}
    \caption{Construing the Confidence Set}
    \label{alg:constructing_the_cs}
    
    \begin{enumerate}
        \item Estimate the reduced-form parameter $\widehat{\omega }=( \widehat{\omega }_{1},\widehat{\omega }_{2},\widehat{\omega }_{3})^{\prime }$ following the estimators defined in \cref{omega 1 est}, \cref{omega 2 est}, and  \cref{omega 3 est}.
%
        \item Obtain a consistent estimator of $\widehat{\omega}$'s asymptotic covariance $\widehat{ \Omega }=\widehat{\mathcal{B}}\widehat{V}\widehat{\mathcal{B}}^{\prime },$ where $\widehat{\mathcal{B}}$ is defined in \cref{Fhat} and $\widehat{V}$ is a HAC estimator of $V.$

        \item For $\theta _{0}\in \Theta$,
%
        \begin{enumerate}
            \item Construct the QLR statistic $QLR(\theta _{0})$ in \cref{QLR stat} using $g(\theta ,\omega ),$ $G(\theta ,\omega ),$ $\widehat{\omega },$ and $\widehat{\Omega }.$
%
            \item Compute the residual process $\widehat{r}(\theta )$ in \cref{red process}.

            \item Given $\widehat{r}(\theta ),$ compute the critical value $c_{1-\alpha }(r,\theta _{0})$ described above.
%
        \end{enumerate}
%
        \item Repeat these steps for different values of $\theta _{0}$.  Construct a confidence set by collecting the null values that are not rejected, i.e., the nominal level $1-\alpha $ confidence set for $\theta _{0}$ is
%
            \begin{equation*}
                CS_{T}=\{ \theta _{0}:QLR_{T}(\theta _{0})\leq c_{1-\alpha }(r,\theta_{0})\}.
            \end{equation*}
    \end{enumerate}
\end{algorithm}

To obtain confidence intervals for each element of $\theta _{0},$ one simple solution is to project the confidence set constructed above to each axis. The resulting confidence interval also has correct coverage. An alternative solution is to first concentrate out the nuisance parameters before applying the conditional inference approach above, see \textcite[Section 5]{andrews2016conditional}.  However, this concentration approach only works when the nuisance parameter is strongly identified. In the present set-up, this approach does not work for $\kappa $ and $\phi $ because the nuisance parameter $\pi $ is weakly identified.  

\begin{assumpS}
    \label{assump:S}
The following conditions hold over $P\in \mathcal{P},$ for any $\theta $ in its parameter space, and any $\omega $ in some fixed neighborhood around its true value, for some fixed $0<C<\infty$.
%
\begin{enumerate}
    \item $g(\theta ,\omega )$ is partially differentiable in $\omega ,$ with partial derivative $G(\theta ,\omega )$ that satisfies $||G(\theta _{1},\omega )-G(\theta _{2},\omega )||\leq C||\theta _{1}-\theta _{2}||$ and $||G(\theta ,\omega _{1})-G(\theta ,\omega _{2})||\leq C||\omega _{1}-\omega _{2}||.$
%
    \item $C^{-1}\leq \lambda_{\min }(G(\theta ,\omega )^{\prime }G(\theta ,\omega ))\leq \lambda_{\max }(G(\theta ,\omega )^{\prime }G(\theta ,\omega ))\leq C$.
\end{enumerate}
\end{assumpS}

\begin{theorem}
    \label{Lemma CS}
    Suppose \cref{assump:R} and \cref{assump:S} hold. Then, 
%
    \begin{equation*} 
        \underset{T\rightarrow \infty }{\lim \inf }\underset{P\in \mathcal{P}}{\inf }\Pr \left( \theta _{0}\in CS_{T}\right) \geq 1-\alpha .\footnote{\Cref{proof:lemma_CS}}
    \end{equation*}
\end{theorem}

This theorem states that the confidence set constructed by the conditional QLR\ test has correct asymptotic size. Uniformity is important for this confidence set to cover the true parameter with a probability close to $1-\alpha $ in finite-samples. This uniform result is established over a parameter space $\mathcal{P}$ that allows for weak identification of the structural parameter $\theta$.


\section{Simulations}\label{sec:simulation}

In this section, we investigate the finite-sample performance of the proposed test and show that the asymptotic approximations derived above work well in practice. We also compare it with the standard test that assumes all parameters are strongly identified. The standard test is known to be invalid under weak identification but its degree of distortion is unknown in general. We simulate the data with the parametric model above where the true values of the parameters are given in \cref{tbl:simulationParameters} based on the values used by \textcite{han2018leverage}. To investigate the robustness of the procedure with respect to various identification strengths, we vary both $\phi$ and $T$. Specifically, we consider $\phi \in \lbrace -0.40, -0.10, -0.01 \rbrace$ and $T \in \lbrace 2,000; 10,000 \rbrace$. The number of data points in the empirical section is $3,700$. 

\begin{table}[htb]
 
 \centering
 \caption{Simulation Set-up}
 \label{tbl:simulationParameters}
 
 \begin{tabularx}{.75\textwidth}{X X c X X}

  \toprule
  $\delta$ & $\rho$ & $c$ & $\pi$ & $\kappa$ \\
  \midrule
  \multicolumn{5}{c}{Parameter Values used by \textcite{han2018leverage}} \\
  \midrule
  0.6475  & 0.50  & \num[scientific-notation=true]{.00394128} & -10 & 1.7680 \\
  \bottomrule
%
 \end{tabularx}

\end{table}

 
To avoid boundary issues with respect to the estimate of $c$ and $\delta$ in finite-sample, we reparameterize the moment conditions and link functions in terms of $\log(c)$,  $\log(c) + \log(\delta)$, and $\logit(\rho)$. This reparameterization forces the scale parameters to be positive and $\rho$ to lie in $(0,1)$. We find that the resulting estimates for the transformed reduced-form parameters are better approximated by the Gaussian distribution for a given finite sample. 

To show the effect of various identification strength, we first vary the true value of $\phi$ and plot the distribution of $\widehat{\pi}$ and $\widehat{\theta}$ in \cref{fig:sim_parameter_estimates}. The reported result is based on $10,000$  observations and \num{500} simulation repetitions. The black lines in the middle of the figures are the true parameter values. Clearly, the estimators sometimes pile up at the boundaries of the parameter space. As expected, this simulation shows that the Gaussian distribution is not a good approximation for the finite-sample distribution of either of the estimators.

\begin{figure}[htb]
  
  \caption[Parameter Estimates' \textit{t}-Statistics]{Parameter Estimates' $t$-Statistics}
  \label{fig:sim_parameter_estimates}


  \begin{subfigure}[t]{.48\textwidth}
    \caption[pi with phi = -0.40]{$\pi$ with $\phi = -0.40$}
    \includegraphics[width=\textwidth, height=.7\textwidth]{pi_est_500_minus_0_point_40.pdf}
  \end{subfigure}
%
  \hfill
%
  \begin{subfigure}[t]{.48\textwidth}
    \caption[pi with phi = -0.01]{$\pi$ with $\phi = -0.01$}
    \includegraphics[width=\textwidth, height=.7\textwidth]{pi_est_500_minus_0_point_01.pdf}
  \end{subfigure}
%
  \begin{subfigure}[b]{.48\textwidth}
    \caption[theta with phi = -0.40]{$\theta$ with $\phi = -0.40$}
    \includegraphics[width=\textwidth, height=.7\textwidth]{theta_est_500_minus_0_point_40.pdf}
  \end{subfigure}
%
  \hfill
%
  \begin{subfigure}[b]{.48\textwidth}
    \caption[theta with phi = -0.01]{$\theta$ with $\phi = -0.01$}
    \includegraphics[width=\textwidth, height=.7\textwidth]{theta_est_500_minus_0_point_01.pdf}
  \end{subfigure}
%
\end{figure}

% Increase the number of simulations
Next, we study the finite-sample size of in the standard QLR test and the proposed conditional QLR test for a joint test for the three structural parameters. The nominal level of the test is \SI{5}{\percent}. The critical value of the standard QLR test is the \SI{95}{\percent} quantile of the $\chi^2$-distribution with $3$ degree of freedom. The critical value of the conditional QLR test is obtained by the stimulation-based procedure in \cref{alg:constructing_the_cs}, with \num{250} simulation repetitions to approximate the quantile of the conditional distribution. The finite-sample size is based on \num{250} simulation repetitions.

The standard test is no longer valid under weak identification because the QLR statistic does not have a $\chi^2$-distribution in this case. However, it is not clear whether the standard QLR test under-rejects or over-rejects in finite-sample and how large is the difference from \SI{5}{\percent}. 


\begin{table}[htb]
 
  \centering
  \caption{Finite-Sample Size of the Standard and Proposed Tests}
  \label{tbl:test_performance}

 \sisetup{
  round-mode=places,
  round-precision=2,
 }
 
 \begin{tabularx}{.85\textwidth}{X | S  S | S S }
%
  \toprule
  & {Standard \%} & {Proposed \%} & {Standard \%} & {Proposed \%} \\
  
  \midrule
  $\phi$ & \multicolumn{2}{c}{$T$ = 2,000} & \multicolumn{2}{c}{$T$ = 10,000} \\
  \midrule
  -0.01  & 2.00 & 5.20 & 1.60  &  4.40 \\
  % -0.10  &   &   & &   \\
  -0.40  &  2.40  & 5.60  &  6.00 & 6.40  \\
  \bottomrule

 \end{tabularx}

\end{table}

Simulation results show that the standard QLR test under-rejects in finite-sample. This is most severe when the identification is weak, e.g., for $\phi=-0.01$ and $T = 10,000$, the rejection rate is \SI{1.6}{\percent}. If we have enough data and $\phi$ is large enough in magnitude, the standard test static does okay. However, this is not the empirically relevant case.  The proposed test, however, has approximately uniform  coverage and, hence, is much more trustworthy.

\section{Data and Empirical Results}\label{sec:empirical}

For the empirical application, we use the daily return on the S\&P 500 for $r_{t+1}$ and the associated realized volatility computed with high-frequency data for $\sigma^2_{t+1}$. The data is obtained from SPY (SPDR S\&P 500 ETF Trust), an exchange-traded fund that mimics the S\&P 500. This gives us a market index whose risk is not easily diversifiable and can be used to estimate the prices of risk that investors face in practice. We use the procedure \textcite{sangrey2018jumps} develops to estimate the integrated total volatility, i.e, the instantaneous expectation of the price variance. This measure reduces to the integrated diffusion volatility if prices have continuous paths and it works well in the presence of market microstructure noise.

Since SPY is one of the most liquid assets traded, we can choose the frequency at which we sample the underlying price. To balance market-microstructure noise, computational cost, and efficiency of the resultant estimators, we sample at the \num{1}-second frequency. We annualize the data by multiplying $r_{t+1}$ by \num{252} and $\sigma^2_{t+1}$ by $252^2$. The data starts in 2003 and ends in September 2017. Since the asset is only traded during business hours, this leads to \num{3713} days of data with an average of approximately \num{24000} observations per day. We compute $r_{t+1}$ as the daily return from the open to the close of the market, the interval over which we can estimate the volatility. This avoids specifying the relationship between overnight and intra-day returns. We preprocess the data using the pre-averaging approach as in \textcites{podolskij2009bipower, aitsahalia2012testing}. 

\begin{figure}[htb]

  \centering
  \caption{S\&P 500 Volatility and Log-Return}


  \begin{subfigure}[t]{.54\textwidth}
    \label{risk_fig:spy_dynamics}
    \caption{Time Series}
    \includegraphics[width=\textwidth, height=.81\textwidth]{time_series.pdf}
  \end{subfigure}%
%
  \hfill
%
  \begin{subfigure}[t]{.44\textwidth}
    \label{risk_fig:spy_static}
    \caption{Joint Distribution}
    \includegraphics[width=\textwidth, height=\textwidth]{joint_dist.pdf}
  \end{subfigure}
\end{figure}


To see how the data move over time, we plot their time series in  \cref{risk_fig:spy_dynamics}.  We also plot the joint unconditional distribution in \cref{risk_fig:spy_static} to see the static relationship between the two series. The volatility has a long-right tail, a typical gamma-type distribution. The returns has a bell-shaped distribution. They are slightly negatively correlated, as shown by the regression line in the joint plot. This corroborates the work by \textcites{bandi2012timevarying, aitsahalia2013leverage}. We also report a series of summary statistics.

\begin{table}[htb]

  \centering
  \caption{Summary Statistics}
  \label{tbl:summary_stats}


  \sisetup{
   table-align-text-pre=false,
   table-align-text-post=false,
   round-mode=places,
   round-precision=2,
   table-space-text-pre=\lbrack,
   table-space-text-post=\rbrack,
  }

  \begin{tabularx}{.5\textwidth}{X | S S}
    \toprule
    & {$r_{t+1}$} & {$\sigma^2_{t+1}$} \\
    \midrule
      Mean & 0.023421 & 5.621287 \\
      \rowcolor{gray!20}
      Standard Deviation & 2.350165 & 14.458446\\
      Skewness & -0.312 & 12.209 \\
      \rowcolor{gray!20}
      Kurtosis & 13.066 & 243.401 \\
      Correlation & \multicolumn{2}{c}{\num{-0.024379}} \\
    \bottomrule
  \end{tabularx}

\end{table}

We now report the estimates and confidence intervals for the reduced-form parameters $c, \delta$, and $\rho$. The confidence intervals reported here use the Gaussian limiting theory, i.e., the point estimates $\pm 1.96$ standard errors. We first obtain confidence intervals for $\log(c)$ and $\log(c) + \log(\delta)$, and transform them into confidence intervals for $c$ and $\delta$.  Similarly, we create the confidence interval for $\rho$ by inverting  the interval for $\logit(\rho)$.

\begin{table}[htb]
  \caption{Parameters that Govern the Volatility Process} 
  \label{tbl:reduced_form_parameters}

    \centering
    \sisetup{
        detect-mode,
        tight-spacing           = true,
        group-digits            = false,
        input-symbols           = {(}{)},
        input-open-uncertainty  = ,
        input-close-uncertainty = ,
        round-mode              = places,
        round-precision         = 2,
        table-align-text-pre    = false,
        table-align-text-post   = false,
        table-alignment         = center,
    }

    \begin{tabularx}{.7\textwidth}{X | S >{{(}} S[table-space-text-pre={(}] <{{,\,}}
      S[table-space-text-pre={\hspace{-1.5cm}}] <{{)}}}
%
    \toprule
    & {Point Estimate} & \multicolumn{2}{c}{\SI{95}{\percent} Confidence Interval} \\
    \midrule
    $c$     & 3.0668 & 1.384 2 & 6.7946 \\
    \rowcolor{gray!20}
    $\delta$  & 37.981 & 17.6528 & 81.718 \\
    $\rho$   & 0.77107 & 0.6747 & 0.8454 \\
    \bottomrule 
  \end{tabularx}
\end{table}

For confidence intervals of the three structural parameters, we first compute their joint confidence set based on the conditional QLR test and then project it to each of the components. We also plot a joint confidence sets for the two risk prices, after projecting out $\phi$. We use 500 simulations to compute the quantile for the QLR statistic.

% \footnote{ We plot contour plots for both the conditional QLR quantile and the QLR statistic in the appendix to aid in interpreting \cref{fig:confidence_region}. In particular, the disconnectedness in \cref{fig:confidence_region} is coming from the multi-modality of the statistic. The conditional quantile is actually quite smooth.  We do this by choosing the minimal (maximal) value of the statistic (quantile) for each ($\kappa, \pi$) pair. If the $\phi$ estimate were independent of the other parameters' estimates,  \cref{fig:confidence_region} would simply be the region where the statistic exceeds the quantile. We deal with the multi-modality of the posterior by using both a random initialization, the true value under the null hypothesis, and a closed-form guess computed by using a subset of the moment conditions. We then use the minimum value computed from these three minimizations. Other initialization procedures do not qualitatively affect the results.}

\begin{table}[htb]
  \caption{Structural Parameters} 
  \label{tbl:structural_param_estimates}

  \centering

  \begin{tabularx}{.4\textwidth}{X | c } 
%
    \toprule
    & \SI{95}{\percent} Confidence Interval \\
    \midrule
    $\phi$      & (-0.33, -0.27) \\
    \rowcolor{gray!20}
    $\pi$       & (-30.97,  0.00)  \\
    $\kappa$    & (0.00, 2.00) \\
    \bottomrule 
  \end{tabularx}
\end{table}


The results in \cref{tbl:structural_param_estimates}.
% and \cref{fig:confidence_region}
have a few notable features. 
First, we can reject the null hypothesis $\phi = 0$. We cannot, however, reject the hypothesis that $\pi = 0$ at the \SI{5}{\percent} level. 
% The standard confidence interval, which is likely not valid, has approximately the same area. Because we have both issues with weak identification and boundary issues, there is not a priori reason to expect it to be either larger or smaller than the valid confidence sets. It does allow for much larger magnitudes of $\pi$.
% \begin{figure}[htb]
%  
%   \caption{Confidence Set for Risk Prices}
%
%   \begin{subfigure}[t]{.32\textwidth}
% 	\label{fig:confidence_region}
%         \caption{Proposed}	
% 	\includegraphics[width=\textwidth, height=\textwidth]{qlr_confidence_region_500.pdf}
%   \end{subfigure}
% %
%   \begin{subfigure}[t]{.32\textwidth}
%     \caption{Anderson-Rubin}
%     \includegraphics[width=\textwidth, height=\textwidth]{ar_confidence_region_500.pdf}
%   \end{subfigure}
% %
%   \begin{subfigure}[t]{.32\textwidth}
%     \caption{Standard}
%     \includegraphics[width=\textwidth, height=\textwidth]{standard_confidence_region.pdf}
%   \end{subfigure}
%
% \end{figure}
We cannot reject the null hypotheses that $\kappa = 0$. This should not be particularly surprising given the difficulty in precisely estimating this parameter documented in the previous literature, \parencites{lettau2010measuring}. Although not recorded on the table, we can reject the hypothesis that both $\kappa = \pi = 0$. The procedure can determine that investors demand compensation for risk, just not what combination of risks they demand compensation for.  
%
% Although the projection procedure can lead to conservative sub-vector confidence sets whose coverage could be larger than \SI{95}{\percent}, the confidence set in \cref{fig:confidence_region} appear relatively informative. 
%
The risk price associated with the market return is covered by $(0.00, 2.00)$ with at least \SI{95}{\percent} probability. The volatility risk price is covered by $(-30.97, 0.00))$ with at least \SI{95}{\percent} probability. Our confidence intervals for both parameters are reasonable given other values that previous authors have found. For example, \Textcite{han2018leverage} preferred a value of $\pi = -10$, for example. 


\section{Conclusion}\label{sec:risk_conclusion}


In structural stochastic volatility models as the one considered here, changes in the volatility affect returns through two channels. On the one hand, investors are willing to tolerate high volatility to get high expected returns as measured by the price of market return risk. On the other hand, investors are directly averse to changes in future volatility, as measured by the price of volatility risk. \Textcite{han2018leverage} shows how to disentangle these two channels by exploiting information arising from the leverage effect in an exponentially-affine pricing model. However, standard inference for this structural model is invalid because the volatility risk price is only weakly identified when the leverage effect is mild. This paper propose an identification robust inference procedure that provides reliable confidence sets for the risk prices regardless of the magnitude of the leverage effect. We take this procedure to the data on the S\&P 500. The robust inference procedure provides reliable yet informative confidence intervals for the risk prices associated with the market return and the volatility. 





\printbibliography[heading=bibintoc]

\begin{appendices}
\section{Proofs}\label{sec:proofs}


\subsection{\texorpdfstring{Proof of \cref{Lemma m0 and m1}}{Proof of Lemma 1}}
\label[proof]{proof:lemma_m0_and_m1}

\begin{proof}
For the risk-free asset, the excess return $r_{t+1}=0.$ 
Therefore, we have
%
\begin{align*}
    1 &= E\left[ \exp \left( m_{0}+m_{1}\sigma _{t}^{2}-\pi \sigma _{t+1}^{2}-\theta r_{t+1}\right) \mvert \F_{t}\right]  \\
%
    &= \exp (m_{0}+m_{1}\sigma _{t})E\left[ \exp \left( -\pi \sigma _{t+1}^{2}\right) E\left[ \exp \left( -\theta r_{t+1}\right) \mvert \F _{t},\sigma _{t+1}^{2}\right] \mvert \F_{t}\right]  \\
%
    &= \exp (m_{0}-E\left( \theta \right) +m_{1}\sigma _{t}-D\left( \theta \right) \sigma _{t}^{2})E\left[ \exp \left( -\pi \sigma _{t+1}^{2}-C\left( \theta \right) \sigma _{t+1}^{2}\right) \mvert \F_{t}\right]  \\
%
    &= \exp (m_{0}-E\left( \theta \right) +m_{1}\sigma _{t}-D\left( \theta \right) \sigma _{t}^{2}-A\left( \pi +C\left( \theta \right) \right) \sigma _{t}^{2}-B\left( \pi +C\left( \theta \right) \right) ),
%
\end{align*}
%
where the first equality follows from the pricing equation, the second equality follows from the law of iterated expectations, the third equation uses the Laplace transform for $r_{t+1}$ in \cref{eqn:return_laplace_transform}, and the last equality follows from the Laplace transform for $\sigma _{t+1}^{2}$ in \cref{eqn:vol_laplace_transform}. 
Since $M_{t,t+1}$ must integrate to $1$, the constant term and coefficient for $\sigma_{t}^{2}$ must equal 0, which gives the claimed result for $m_{0}$ and $m_{1}$.

We can apply the same argument above to any asset $r_{t+1}$. This gives the same result, except $\theta$ is replaced by $\theta -1$ throughout. 
This implies that the two equalities for $m_{0}$ and $m_{1}$ also hold with $\theta $ replaced by $\theta -1$. 
Therefore, 
%
\begin{align*}
    E(\theta -1)+B\left( C\left( \theta -1\right) +\pi \right)  
%
    &= E(\theta)+B\left( C\left( \theta \right) +\pi \right) , \\
%
    D\left( \theta -1\right) +A\left( C\left( \theta -1\right) +\pi \right) 
%
    &= D\left( \theta \right) +A\left( C\left( \theta \right) +\pi \right).
\end{align*}
%
The claimed results for $\gamma $ and $\beta $ follow from $\gamma = \E(\theta)-E(\theta -1)$ and $\beta =D(\theta )-D(\theta -1)$ under the linear specification of $E(x)=\gamma x$ and $D(x)=\beta x$.

\end{proof}

\subsection{\texorpdfstring{Proof of \cref{Lemma Reduce}}{Proof of Lemma 2}}
\label[proof]{proof:lemma_reduce}

\begin{proof}

    Under the assumption that (i) $ \mathbb{E(}z_{t}z_{t}^{\prime })$ has the smallest eigenvalue bounded away from 0 and (ii) $c>\varepsilon $ and $\delta >\varepsilon $ for some $ \varepsilon >0,$ we not only have $\omega _{10}$ as an unique minimizer of $||\mathbb{E}[h_{t}(\omega _{1})]||$ but also have a uniform positive lower bound for $\norm{\E[h_{t}(\omega _{1})]}$ for $\norm{\omega _{1}-\omega _{10}} \geq \varepsilon$. Thus, consistency of $\widehat{\omega }_{1}$ follows from standard arguments for the consistency of a GMM estimator under an uniform convergence of the criterion under \cref{assump:R} (1) and (2).

Let $\overline{h}(\omega _{1})=T^{-1}\sum_{t=1}^{T}h_{t}(\omega _{1})$ and $ \overline{H}(\omega )=T^{-1}\sum_{t=1}^{T}H_{t}(\omega _{1}).$ By construction, the estimator satisfies the first order condition
%
\begin{align}
    0 &= 
    \begin{pmatrix} 
        \overline{H}(\widehat{\omega }_{1})^{\prime }\widehat{V}_{1}^{-1}\overline{h} (\widehat{\omega }_{1}) \\ 
%
        T^{-1}\sum_{T=1}^{T}x_{t}(y_{t}-x_{t}^{\prime }\widehat{\omega }_{2}) \\ 
%
        \widehat{\omega }_{3}-T^{-1}\sum_{t=1}^{T}\left( y_{t}-\widehat{y} _{t}\right) ^{2} 
    \end{pmatrix} \nonumber \\ 
%
    &= 
%
    \begin{pmatrix}
%
        \overline{H}(\widehat{\omega }_{1})^{\prime }\widehat{V}_{1}^{-1}\overline{h} (\omega _{10})+\overline{H}(\widehat{\omega }_{1})^{\prime }\widehat{V} _{1}^{-1}\overline{H}(\widetilde{\omega }_{1})(\widehat{\omega }_{1}-\omega
_{10}) \\ 
%
        T^{-1}\sum_{t=1}^{T}x_{t}(y_{t}-x_{t}^{\prime }\omega _{20})-T^{-1}\sum_{t=1}^{T}x_{t}x_{t}^{\prime }\left( \widehat{\omega } _{2}-\omega _{20}\right) \\ 
%
        \left( \widehat{\omega }_{3}-\omega _{3}\right) +\omega _{3}-T^{-1}\sum_{t=1}^{T}\left( y_{t}-x_{t}\widehat{\omega }_{2}\right) ^{2}
    \end{pmatrix},
%
  \label{L-R-1}
\end{align}
%
where the second equality follows from a mean value expansion of $\overline{h }(\widehat{\omega }_{1})$ around $\omega _{10},$ with $\widetilde{\omega } _{1}$ between $\omega _{10}$ and $\widehat{\omega }_{1}$. 
Let
%
\begin{equation}
%
    \widetilde{\mathcal{B}} = \diag\left\lbrace[\overline{H}(\widehat{\omega }_{1})^{\prime } \widehat{V}_{1}^{-1}\overline{H}(\widetilde{\omega }_{1})]^{-1}\overline{H}( \widehat{\omega }_{1})^{\prime }\widehat{V}_{1}^{-1},[T^{-1} \sum_{t=1}^{T}x_{t}x_{t}^{\prime }]^{-1},1\right\rbrace.  
%
\end{equation}
%
Then \cref{L-R-1} implies that 
%
\begin{align}
    T^{1/2}\left( \widehat{\omega }-\omega \right) 
%    
    &= \widetilde{\mathcal{B}} \cdot T^{-1/2}\sum_{t=1}^{T} 
%
    \begin{pmatrix}
        -h_{t}(\omega _{10}) \\ 
%
        x_{t}(y_{t}-x_{t}^{\prime }\omega _{20}) \\ 
%
        \left( y_{t}-x_{t}\widehat{\omega }_{2}\right) ^{2}-\omega _{3}%
    \end{pmatrix}  \nonumber \\
%
    &=
%
    \widetilde{\mathcal{B}}\cdot T^{-1/2}\sum_{t=1}^{T} 
%
    \begin{pmatrix}
        -h_{t}(\omega _{10}) \\ 
%
        x_{t}(y_{t}-x_{t}^{\prime }\omega _{20}) \\ 
%
        \left( y_{t}-x_{t}^{\prime }\omega _{20}\right) ^{2}-\E\left[\left( y_{t}-x_{t}^{\prime }\omega _{20}\right)^{2}\right]%
%
    \end{pmatrix}%
%
    +
%
    \begin{pmatrix}
        0 \\ 
        0 \\ 
        \varepsilon_{T}
    \end{pmatrix},
%
    \label{L-R-2}
%
\end{align}%
%
where the second equality uses $\omega _{3}=\E[\left( y_{t}-x_{t}^{\prime }\omega _{20}\right) ^{2}]$ by definition and 
%
\begin{align}
    \varepsilon _{T} 
%
    &= T^{-1/2}\sum_{t=1}^{T}\left[ \left( y_{t}-x_{t}^{\prime } \widehat{\omega }_{2}\right) ^{2}-\left( y_{t}-x_{t}^{\prime }\omega _{20}\right) ^{2}\right]  \nonumber \\
%
    &= 2T^{-1}\sum_{t=1}^{T}\left( y_{t}-x_{t}^{\prime }\omega _{20}\right) x_{t}^{\prime }\left[ T^{1/2}\left( \widehat{\omega }_{2}-\omega _{20}\right) \right] +o_{p}(1)  \nonumber \\
%
    &= o_{p}(1)  
%
    \label{L-R-3}
\end{align}
%
because $T^{-1}\sum_{t=1}^{T}\left( y_{t}-x_{t}^{\prime }\omega _{20}\right) x_{t}^{\prime }\rightarrow _{p}0$ and $T^{1/2}(\widehat{\omega }_{2}-\omega _{20})=O_{p}(1)$ following \cref{assump:R}. 
In addition, 
%
\begin{equation}
    \widetilde{\mathcal{B}}\rightarrow _{p}\mathcal{B}  
    \label{L-R-4}
\end{equation}%
%
following from the consistency of $\widehat{\omega}_{1}$ and \cref{assump:R}.  Finally, the desirable result follows from \cref{L-R-2}--\cref{L-R-4} and \cref{assump:R}.  The consistency of $\widehat{\Omega }$ follows from the consistency of $\widehat{\mathcal{B}}$ and $\widehat{V}$. 

\end{proof}

\subsection{\texorpdfstring{Proof of \cref{Lemma CS}}{Proof of Theorem 3}}
\label[proof]{proof:lemma_CS}

\begin{proof}

We obtain this result by applying \textcite[Theorem 1]{andrews2016conditional}. 
We now verify Assumptions 1--3 in \textcite{andrews2016conditional}. 
To show weak convergence $\eta _{T}(\cdot )$ to $\eta (\cdot )$ uniformly over $\mathcal{P}$, note that by a second-order Taylor expansion,
%
\begin{align}
    \eta _{T}(\lambda) 
%
    &\coloneqq T^{1/2}\left[ \widehat{g}(\lambda )-g_{0}(\lambda ) \right] = G_{0}(\lambda )\Omega ^{1/2}\xi _{T}+\delta _{T},\text{ where} \nonumber \\
%
    \xi _{T} 
    &= \Omega ^{-1/2}T^{1/2}\left( \widehat{\omega }-\omega _{0}\right), \text{ and } 
%
    \delta _{T} =\left( G(\lambda ,  \widetilde{\omega })-G(\lambda, \omega _{0})\right) T^{1/2}(\widehat{\omega }-\omega _{0})
%
\end{align}%
%
and $\widetilde{\omega }$ is between $\widehat{\omega }$ and $\omega_{0}$.
%
Because $\norm{G(\lambda ,\widetilde{\omega })-G(\lambda ,\omega _{0})} \leq C \norm{\widetilde{\omega }-\omega_{0}}$, $\delta _{T}=o_{p}(1)$ uniformly over $ \mathcal{P}$ following Lemma \cref{Lemma Reduce}. 
To show $G_{0}(\lambda)\Omega ^{1/2}\xi _{T}$ weakly converges to $\eta (\cdot ),$ it is sufficient to show (i) the pointwise convergence%
%
\begin{equation}
% 
    \begin{pmatrix}
        G_{0}(\lambda _{1})\Omega ^{1/2}\xi _{T} \\ 
        G_{0}(\lambda _{2})\Omega ^{1/2}\xi _{T}%
    \end{pmatrix}%
%
    \rightarrow _{d}( 
%
    \begin{pmatrix}
        \eta (\lambda _{1}) \\ 
        \eta (\lambda _{2})%
    \end{pmatrix},
%
\end{equation}%
%
which follows from \cref{Lemma Reduce}, and (ii) the stochastic equicontinuity condition, i.e., for every $\varepsilon >0$ and $\xi >0,$ there exists a $\delta >0$ such that
%
\begin{equation}
    \underset{T\rightarrow \infty }{\lim\sup }\Pr \left( \underset{P\in \mathcal{P}}{\sup }\underset{\norm{\lambda _{1}-\lambda _{2}}\leq \delta }{\sup }\norm*{G_{0}(\lambda _{1})\Omega ^{1/2}\xi _{T}-G_{0}(\lambda
    _{2})\Omega ^{1/2}\xi _{T}} >\varepsilon \right) < \xi.
\end{equation}%
%
For some $C < \infty$, we have $\norm{G_{0}(\lambda _{1})-G(\lambda _{2})} \leq C \norm{\lambda _{1}-\lambda _{2}}$ under a uniform bound for the derivative in \cref{assump:S}, and we have $\norm{\Omega ^{1/2}} \leq C$ under \cref{assump:R} because $F$ and $V$ both have bounded largest eigenvalue. 
Thus,
%
\begin{align}
    &\phantom{=} \underset{T\rightarrow \infty }{\lim \sup }\Pr\left( \underset{P\in \mathcal{P}}{\sup }\underset{\norm{\lambda _{1}-\lambda _{2}}\leq \delta }{\sup }\norm*{ G_{0}(\lambda _{1})\Omega ^{1/2}\xi _{T}-G_{0}(\lambda _{2})\Omega ^{1/2}\xi _{T}} > \varepsilon \right)  \nonumber \\
%
    &\leq \underset{T\rightarrow \infty }{\lim \sup }\Pr \left( C^{2}\underset{ P\in \mathcal{P}}{\sup }\left \Vert \xi _{T}\right \Vert >\frac{\varepsilon }{\delta }\right). 
%
    \label{EC}
\end{align}
%
Because $\xi _{T}=O_{p}(1)$ uniformly over $P\in \mathcal{P},$ there exists $ \delta $ such that $\varepsilon /\delta $ is large enough to make the right hand side of the inequality in \cref{EC} smaller than $\xi$.

Assumptions 2 and 3 of \textcite[Theorem 1]{andrews2016conditional} follow from \cref{assump:R}. 
\end{proof}

% \section{Additional Empirical Results}\label{app:additional_empirical_results}

% In this graph, I display contour plots of both the conditional quantile and the QLR statistic. I truncate the graphs above at \num{10} because we can clearly reject the parameters that lead to that exceed that number. The quantile of the robust QLR statistic if we do not subtract off anything is bounded above by \num{10}.

% \begin{figure}[htb]
  
%   \caption{Contour Plots of the Quantile and Statistic}

%   \begin{subfigure}[t]{.55\textwidth}
%     \caption{QLR Robust Quantile}
%     \includegraphics[width=\textwidth, height=2in]{qlr_quantile_contours.pdf}
%   \end{subfigure}
% %
%   \hfill
% %
%   \begin{subfigure}[t]{.44\textwidth}
%     \caption{QLR Statistic}
%     \includegraphics[width=\textwidth, height=2in]{qlr_stats_contours.pdf}
%   \end{subfigure}

% \end{figure}


\end{appendices}

\end{document}


